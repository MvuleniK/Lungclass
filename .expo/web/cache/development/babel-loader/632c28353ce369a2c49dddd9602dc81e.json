{"ast":null,"code":"/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Softmax, util } from '@tensorflow/tfjs-core';\nimport { exp } from \"./Exp\";\nimport { max } from \"./Max\";\nimport { div } from \"./RealDiv\";\nimport { reshape } from \"./Reshape\";\nimport { sub } from \"./Sub\";\nimport { sum } from \"./Sum\";\nexport function softmax(args) {\n  var inputs = args.inputs,\n      backend = args.backend,\n      attrs = args.attrs;\n  var logits = inputs.logits;\n  var dim = attrs.dim;\n  var logitsRank = logits.shape.length;\n  var $dim = dim;\n\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n\n  if ($dim !== logitsRank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + (\"Logits was rank \" + logitsRank + \" and dim was \" + $dim));\n  }\n\n  var axes = util.parseAxisParam([$dim], logits.shape);\n  var maxLogit = max({\n    inputs: {\n      x: logits\n    },\n    backend: backend,\n    attrs: {\n      reductionIndices: axes,\n      keepDims: false\n    }\n  });\n  var expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n  var maxLogitReshaped = reshape({\n    inputs: {\n      x: maxLogit\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var a = sub({\n    inputs: {\n      a: logits,\n      b: maxLogitReshaped\n    },\n    backend: backend\n  });\n  var b = exp({\n    inputs: {\n      x: a\n    },\n    backend: backend\n  });\n  var sumExp = sum({\n    inputs: {\n      x: b\n    },\n    backend: backend,\n    attrs: {\n      axis: axes,\n      keepDims: false\n    }\n  });\n  var sumReshaped = reshape({\n    inputs: {\n      x: sumExp\n    },\n    backend: backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  var result = div({\n    inputs: {\n      a: b,\n      b: sumReshaped\n    },\n    backend: backend\n  });\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n  return result;\n}\nexport var softmaxConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax\n};","map":{"version":3,"sources":["../../src/kernels/Softmax.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,YAAR,EAAgD,OAAhD,EAAkG,IAAlG,QAA6G,uBAA7G;AAIA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,OAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AAEA,OAAM,SAAU,OAAV,CACF,IADE,EAEuE;EAE3E,IAAO,MAAP,GAAiC,IAAjC,CAAO,MAAP;EAAA,IAAe,OAAf,GAAiC,IAAjC,CAAe,OAAf;EAAA,IAAwB,KAAxB,GAAiC,IAAjC,CAAwB,KAAxB;EACA,IAAO,MAAP,GAAiB,MAAjB,CAAO,MAAP;EACA,IAAO,GAAP,GAAc,KAAd,CAAO,GAAP;EAEA,IAAM,UAAU,GAAG,MAAM,CAAC,KAAP,CAAa,MAAhC;EAEA,IAAI,IAAI,GAAG,GAAX;;EACA,IAAI,IAAI,KAAK,CAAC,CAAd,EAAiB;IACf,IAAI,GAAG,UAAU,GAAG,CAApB;EACD;;EACD,IAAI,IAAI,KAAK,UAAU,GAAG,CAA1B,EAA6B;IAC3B,MAAM,KAAK,CACP,oFACmB,UADnB,qBAC6C,IAD7C,CADO,CAAX;EAGD;;EAED,IAAM,IAAI,GAAG,IAAI,CAAC,cAAL,CAAoB,CAAC,IAAD,CAApB,EAA4B,MAAM,CAAC,KAAnC,CAAb;EACA,IAAM,QAAQ,GAAG,GAAG,CAAC;IACnB,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CADW;IAEnB,OAAO,EAAP,OAFmB;IAGnB,KAAK,EAAE;MAAC,gBAAgB,EAAE,IAAnB;MAAyB,QAAQ,EAAE;IAAnC;EAHY,CAAD,CAApB;EAKA,IAAM,aAAa,GAAG,YAAY,CAAC,oBAAb,CAAkC,QAAQ,CAAC,KAA3C,EAAkD,IAAlD,CAAtB;EAEA,IAAM,gBAAgB,GAClB,OAAO,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAwB,OAAO,EAAP,OAAxB;IAAiC,KAAK,EAAE;MAAC,KAAK,EAAE;IAAR;EAAxC,CAAD,CADX;EAEA,IAAM,CAAC,GACH,GAAG,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE,MAAJ;MAAY,CAAC,EAAE;IAAf,CAAT;IAA2C,OAAO,EAAP;EAA3C,CAAD,CADP;EAEA,IAAM,CAAC,GAAG,GAAG,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAiB,OAAO,EAAP;EAAjB,CAAD,CAAb;EACA,IAAM,MAAM,GACR,GAAG,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAiB,OAAO,EAAP,OAAjB;IAA0B,KAAK,EAAE;MAAC,IAAI,EAAE,IAAP;MAAa,QAAQ,EAAE;IAAvB;EAAjC,CAAD,CADP;EAEA,IAAM,WAAW,GACb,OAAO,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE;IAAJ,CAAT;IAAsB,OAAO,EAAP,OAAtB;IAA+B,KAAK,EAAE;MAAC,KAAK,EAAE;IAAR;EAAtC,CAAD,CADX;EAGA,IAAM,MAAM,GAAG,GAAG,CAAC;IAAC,MAAM,EAAE;MAAC,CAAC,EAAE,CAAJ;MAAO,CAAC,EAAE;IAAV,CAAT;IAAiC,OAAO,EAAP;EAAjC,CAAD,CAAlB;EAEA,OAAO,CAAC,6BAAR,CAAsC,QAAtC;EACA,OAAO,CAAC,6BAAR,CAAsC,gBAAtC;EACA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;EACA,OAAO,CAAC,6BAAR,CAAsC,CAAtC;EACA,OAAO,CAAC,6BAAR,CAAsC,MAAtC;EACA,OAAO,CAAC,6BAAR,CAAsC,WAAtC;EAEA,OAAO,MAAP;AACD;AAED,OAAO,IAAM,aAAa,GAAiB;EACzC,UAAU,EAAE,OAD6B;EAEzC,WAAW,EAAE,KAF4B;EAGzC,UAAU,EAAE;AAH6B,CAApC","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2020 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { backend_util, Softmax, util } from '@tensorflow/tfjs-core';\nimport { exp } from './Exp';\nimport { max } from './Max';\nimport { div } from './RealDiv';\nimport { reshape } from './Reshape';\nimport { sub } from './Sub';\nimport { sum } from './Sum';\nexport function softmax(args) {\n    const { inputs, backend, attrs } = args;\n    const { logits } = inputs;\n    const { dim } = attrs;\n    const logitsRank = logits.shape.length;\n    let $dim = dim;\n    if ($dim === -1) {\n        $dim = logitsRank - 1;\n    }\n    if ($dim !== logitsRank - 1) {\n        throw Error('Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${logitsRank} and dim was ${$dim}`);\n    }\n    const axes = util.parseAxisParam([$dim], logits.shape);\n    const maxLogit = max({\n        inputs: { x: logits },\n        backend,\n        attrs: { reductionIndices: axes, keepDims: false }\n    });\n    const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n    const maxLogitReshaped = reshape({ inputs: { x: maxLogit }, backend, attrs: { shape: expandedShape } });\n    const a = sub({ inputs: { a: logits, b: maxLogitReshaped }, backend });\n    const b = exp({ inputs: { x: a }, backend });\n    const sumExp = sum({ inputs: { x: b }, backend, attrs: { axis: axes, keepDims: false } });\n    const sumReshaped = reshape({ inputs: { x: sumExp }, backend, attrs: { shape: expandedShape } });\n    const result = div({ inputs: { a: b, b: sumReshaped }, backend });\n    backend.disposeIntermediateTensorInfo(maxLogit);\n    backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n    backend.disposeIntermediateTensorInfo(a);\n    backend.disposeIntermediateTensorInfo(b);\n    backend.disposeIntermediateTensorInfo(sumExp);\n    backend.disposeIntermediateTensorInfo(sumReshaped);\n    return result;\n}\nexport const softmaxConfig = {\n    kernelName: Softmax,\n    backendName: 'cpu',\n    kernelFunc: softmax\n};\n//# sourceMappingURL=Softmax.js.map"]},"metadata":{},"sourceType":"module"}