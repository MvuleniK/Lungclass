{"ast":null,"code":"import _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AdadeltaOptimizer } from \"./adadelta_optimizer\";\nimport { AdagradOptimizer } from \"./adagrad_optimizer\";\nimport { AdamOptimizer } from \"./adam_optimizer\";\nimport { AdamaxOptimizer } from \"./adamax_optimizer\";\nimport { MomentumOptimizer } from \"./momentum_optimizer\";\nimport { RMSPropOptimizer } from \"./rmsprop_optimizer\";\nimport { SGDOptimizer } from \"./sgd_optimizer\";\nexport var OptimizerConstructors = function () {\n  function OptimizerConstructors() {\n    _classCallCheck(this, OptimizerConstructors);\n  }\n\n  _createClass(OptimizerConstructors, null, [{\n    key: \"sgd\",\n    value: function sgd(learningRate) {\n      return new SGDOptimizer(learningRate);\n    }\n  }, {\n    key: \"momentum\",\n    value: function momentum(learningRate, _momentum) {\n      var useNesterov = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n      return new MomentumOptimizer(learningRate, _momentum, useNesterov);\n    }\n  }, {\n    key: \"rmsprop\",\n    value: function rmsprop(learningRate) {\n      var decay = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .9;\n      var momentum = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.0;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      var centered = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : false;\n      return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    }\n  }, {\n    key: \"adam\",\n    value: function adam() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0.001;\n      var beta1 = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n      var beta2 = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.999;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    }\n  }, {\n    key: \"adadelta\",\n    value: function adadelta() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : .001;\n      var rho = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : .95;\n      var epsilon = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : null;\n      return new AdadeltaOptimizer(learningRate, rho, epsilon);\n    }\n  }, {\n    key: \"adamax\",\n    value: function adamax() {\n      var learningRate = arguments.length > 0 && arguments[0] !== undefined ? arguments[0] : 0.002;\n      var beta1 = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.9;\n      var beta2 = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 0.999;\n      var epsilon = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : null;\n      var decay = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 0.0;\n      return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    }\n  }, {\n    key: \"adagrad\",\n    value: function adagrad(learningRate) {\n      var initialAccumulatorValue = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.1;\n      return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n    }\n  }]);\n\n  return OptimizerConstructors;\n}();","map":{"version":3,"sources":["../../src/optimizers/optimizer_constructors.ts"],"names":[],"mappings":";;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,iBAAR;AACA,SAAQ,gBAAR;AACA,SAAQ,aAAR;AACA,SAAQ,eAAR;AACA,SAAQ,iBAAR;AACA,SAAQ,gBAAR;AACA,SAAQ,YAAR;AAEA,WAAa,qBAAb;EAAA;IAAA;EAAA;;EAAA;IAAA;IAAA,OAsCE,aAAW,YAAX,EAA+B;MAC7B,OAAO,IAAI,YAAJ,CAAiB,YAAjB,CAAP;IACD;EAxCH;IAAA;IAAA,OAyDE,kBAAgB,YAAhB,EAAsC,SAAtC,EAA2E;MAAA,IAAnB,WAAmB,uEAAL,KAAK;MAEzE,OAAO,IAAI,iBAAJ,CAAsB,YAAtB,EAAoC,SAApC,EAA8C,WAA9C,CAAP;IACD;EA5DH;IAAA;IAAA,OAkFE,iBACI,YADJ,EAEoB;MAAA,IADM,KACN,uEADc,EACd;MAAA,IADkB,QAClB,uEAD6B,GAC7B;MAAA,IADkC,OAClC,uEADoD,IACpD;MAAA,IAAhB,QAAgB,uEAAL,KAAK;MAClB,OAAO,IAAI,gBAAJ,CACH,YADG,EACW,KADX,EACkB,QADlB,EAC4B,OAD5B,EACqC,QADrC,CAAP;IAED;EAvFH;IAAA;IAAA,OAqGE,gBAE0B;MAAA,IADtB,YACsB,uEADP,KACO;MAAA,IADA,KACA,uEADQ,GACR;MAAA,IADa,KACb,uEADqB,KACrB;MAAA,IAAtB,OAAsB,uEAAJ,IAAI;MACxB,OAAO,IAAI,aAAJ,CAAkB,YAAlB,EAAgC,KAAhC,EAAuC,KAAvC,EAA8C,OAA9C,CAAP;IACD;EAzGH;IAAA;IAAA,OAuHE,oBAAsE;MAAA,IAAtD,YAAsD,uEAAvC,IAAuC;MAAA,IAAjC,GAAiC,uEAA3B,GAA2B;MAAA,IAAtB,OAAsB,uEAAJ,IAAI;MAEpE,OAAO,IAAI,iBAAJ,CAAsB,YAAtB,EAAoC,GAApC,EAAyC,OAAzC,CAAP;IACD;EA1HH;IAAA;IAAA,OAyIE,kBAEe;MAAA,IADX,YACW,uEADI,KACJ;MAAA,IADW,KACX,uEADmB,GACnB;MAAA,IADwB,KACxB,uEADgC,KAChC;MAAA,IADuC,OACvC,uEADyD,IACzD;MAAA,IAAX,KAAW,uEAAH,GAAG;MACb,OAAO,IAAI,eAAJ,CAAoB,YAApB,EAAkC,KAAlC,EAAyC,KAAzC,EAAgD,OAAhD,EAAyD,KAAzD,CAAP;IACD;EA7IH;IAAA;IAAA,OA+JE,iBAAe,YAAf,EAAkE;MAAA,IAA7B,uBAA6B,uEAAH,GAAG;MAEhE,OAAO,IAAI,gBAAJ,CAAqB,YAArB,EAAmC,uBAAnC,CAAP;IACD;EAlKH;;EAAA;AAAA","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { AdadeltaOptimizer } from './adadelta_optimizer';\nimport { AdagradOptimizer } from './adagrad_optimizer';\nimport { AdamOptimizer } from './adam_optimizer';\nimport { AdamaxOptimizer } from './adamax_optimizer';\nimport { MomentumOptimizer } from './momentum_optimizer';\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\nimport { SGDOptimizer } from './sgd_optimizer';\nexport class OptimizerConstructors {\n    /**\n     * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\n     *\n     * ```js\n     * // Fit a quadratic function by learning the coefficients a, b, c.\n     * const xs = tf.tensor1d([0, 1, 2, 3]);\n     * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\n     *\n     * const a = tf.scalar(Math.random()).variable();\n     * const b = tf.scalar(Math.random()).variable();\n     * const c = tf.scalar(Math.random()).variable();\n     *\n     * // y = a * x^2 + b * x + c.\n     * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\n     * const loss = (pred, label) => pred.sub(label).square().mean();\n     *\n     * const learningRate = 0.01;\n     * const optimizer = tf.train.sgd(learningRate);\n     *\n     * // Train the model.\n     * for (let i = 0; i < 10; i++) {\n     *   optimizer.minimize(() => loss(f(xs), ys));\n     * }\n     *\n     * // Make predictions.\n     * console.log(\n     *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\n     * const preds = f(xs).dataSync();\n     * preds.forEach((pred, i) => {\n     *   console.log(`x: ${i}, pred: ${pred}`);\n     * });\n     * ```\n     *\n     * @param learningRate The learning rate to use for the SGD algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static sgd(learningRate) {\n        return new SGDOptimizer(learningRate);\n    }\n    /**\n     * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\n     * descent.\n     *\n     * See\n     * [http://proceedings.mlr.press/v28/sutskever13.pdf](\n     * http://proceedings.mlr.press/v28/sutskever13.pdf)\n     *\n     * @param learningRate The learning rate to use for the Momentum gradient\n     * descent algorithm.\n     * @param momentum The momentum to use for the momentum gradient descent\n     * algorithm.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static momentum(learningRate, momentum, useNesterov = false) {\n        return new MomentumOptimizer(learningRate, momentum, useNesterov);\n    }\n    /**\n     * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\n     * descent. This implementation uses plain momentum and is not centered\n     * version of RMSProp.\n     *\n     * See\n     * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\n     * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n     *\n     * @param learningRate The learning rate to use for the RMSProp gradient\n     * descent algorithm.\n     * @param decay The discounting factor for the history/coming gradient.\n     * @param momentum The momentum to use for the RMSProp gradient descent\n     * algorithm.\n     * @param epsilon Small value to avoid zero denominator.\n     * @param centered If true, gradients are normalized by the estimated\n     * variance of the gradient.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n        return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n    }\n    /**\n     * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adam gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n        return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\n     * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\n     *\n     * @param learningRate The learning rate to use for the Adadelta gradient\n     * descent algorithm.\n     * @param rho The learning rate decay over each update.\n     * @param epsilon A constant epsilon used to better condition the grad\n     * update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n        return new AdadeltaOptimizer(learningRate, rho, epsilon);\n    }\n    /**\n     * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\n     * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\n     *\n     * @param learningRate The learning rate to use for the Adamax gradient\n     * descent algorithm.\n     * @param beta1 The exponential decay rate for the 1st moment estimates.\n     * @param beta2 The exponential decay rate for the 2nd moment estimates.\n     * @param epsilon A small constant for numerical stability.\n     * @param decay The learning rate decay over each update.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n        return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n    }\n    /**\n     * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\n     * See\n     * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\n     * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n     * or\n     * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\n     * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\n     *\n     * @param learningRate The learning rate to use for the Adagrad gradient\n     * descent algorithm.\n     * @param initialAccumulatorValue Starting value for the accumulators, must be\n     * positive.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\n     */\n    static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n        return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n    }\n}\n//# sourceMappingURL=optimizer_constructors.js.map"]},"metadata":{},"sourceType":"module"}