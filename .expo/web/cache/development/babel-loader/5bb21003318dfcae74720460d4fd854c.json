{"ast":null,"code":"import _regeneratorRuntime from \"@babel/runtime/regenerator\";\n\nfunction _createForOfIteratorHelperLoose(o, allowArrayLike) { var it = typeof Symbol !== \"undefined\" && o[Symbol.iterator] || o[\"@@iterator\"]; if (it) return (it = it.call(o)).next.bind(it); if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; return function () { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from \"../base_callbacks\";\nimport { NotImplementedError, ValueError } from \"../errors\";\nimport { disposeTensorsInLogs } from \"../logs\";\nimport { singletonOrArray, toList } from \"../utils/generic_utils\";\nimport { standardizeClassWeights, standardizeWeights } from \"./training_utils\";\nvar DEFAULT_VALIDATION_BATCH_SIZE = 32;\n\nfunction standardizeDataIteratorOutput(model, iteratorOut) {\n  var xs;\n  var ys;\n  var iteratorOutObj = iteratorOut;\n  xs = iteratorOutObj['xs'];\n  ys = iteratorOutObj['ys'];\n  tfc.util.assert(xs != null && ys != null, function () {\n    return 'A Dataset iterator for fitDataset() is expected to generate ' + 'objects of the form `{xs: xVal, ys: yVal}`, where the two ' + 'values may be `tf.Tensor`, an array of Tensors, or a map of ' + 'string to Tensor.  The provided Dataset instead generates ' + (\"\" + iteratorOut);\n  });\n  var flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n  var flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n  var batchSize = flattenedXs[0].shape[0];\n  tfc.util.assert(flattenedXs.length === model.inputs.length, function () {\n    return \"LayersModel has \" + model.inputs.length + \" inputs, but the dataset \" + (\"provides \" + flattenedXs.length + \" inputs.  (Expected input keys: \") + (JSON.stringify(model.inputNames) + \")\");\n  });\n  tfc.util.assert(flattenedYs.length === model.outputs.length, function () {\n    return \"LayersModel has \" + model.outputs.length + \" outputs, but the dataset \" + (\"provides \" + flattenedYs.length + \" outputs.  (Expected output keys: \") + (JSON.stringify(model.outputNames) + \")\");\n  });\n\n  var _loop = function _loop(xIndex) {\n    tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: input \" + (model.inputNames[xIndex] + \" has \" + flattenedXs[xIndex].shape[0] + \"; \") + (\"expected  \" + batchSize + \" based on input \" + model.inputNames[0] + \".\");\n    });\n  };\n\n  for (var xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n    _loop(xIndex);\n  }\n\n  var _loop2 = function _loop2(yIndex) {\n    tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, function () {\n      return \"Batch size mismatch: output \" + (model.outputNames[yIndex] + \" has \" + flattenedYs[yIndex].shape[0] + \"; \") + (\"expected  \" + batchSize + \" based on input \" + model.inputNames[0] + \".\");\n    });\n  };\n\n  for (var yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n    _loop2(yIndex);\n  }\n\n  return {\n    xs: flattenedXs,\n    ys: flattenedYs\n  };\n}\n\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n  if (values instanceof tfc.Tensor) {\n    return [values];\n  } else if (Array.isArray(values)) {\n    tfc.util.assert(values.length === names.length, function () {\n      return \"Received an array of \" + values.length + \" Tensors, but expected \" + names.length + \" to match the \" + inputOrOutput + \" keys \" + names + \".\";\n    });\n    return values;\n  } else {\n    var result = [];\n\n    for (var _iterator = _createForOfIteratorHelperLoose(names), _step; !(_step = _iterator()).done;) {\n      var name = _step.value;\n\n      if (values[name] == null) {\n        throw new ValueError(\"The feature data generated by the dataset lacks the required \" + (inputOrOutput + \" key '\" + name + \"'.\"));\n      }\n\n      result.push(values[name]);\n    }\n\n    return result;\n  }\n}\n\nfunction standardizeTensorValidationData(data) {\n  if (data.length === 3) {\n    throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n  }\n\n  return {\n    xs: data[0],\n    ys: data[1]\n  };\n}\n\nexport function fitDataset(model, dataset, args) {\n  var hasBatchesPerEpoch, doValidation, valXs, valYs, validationData, trainFunction, outLabels, callbackMetrics, callbacks, verbose, _configureCallbacks, callbackList, history, epoch, dataIterator, epochLogs, stepsDone, batchIndex, iteratorOut, _standardizeDataItera, xs, ys, batchLogs, sampleWeights, standardClassWeights, i, ins, outs, _i, label, out, valOuts, _i2;\n\n  return _regeneratorRuntime.async(function fitDataset$(_context) {\n    while (1) {\n      switch (_context.prev = _context.next) {\n        case 0:\n          hasBatchesPerEpoch = args.batchesPerEpoch != null;\n          tfc.util.assert(model.optimizer != null, function () {\n            return 'You must compile a model before training/testing. Use ' + 'LayersModel.compile(modelCompileConfig).';\n          });\n          tfc.util.assert(args != null, function () {\n            return \"For fitDataset(), the 2nd argument (config) is required, \" + \"but it is not provided in this call.\";\n          });\n          tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), function () {\n            return \"For fitDataset(), config.epochs is expected to be a positive \" + (\"integer, but got \" + args.epochs);\n          });\n          tfc.util.assert(!hasBatchesPerEpoch || args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch), function () {\n            return \"For fitDataset(), config.batchesPerEpoch is expected to be a \" + (\"positive integer if specified, but got \" + args.batchesPerEpoch);\n          });\n          tfc.util.assert(args['validationSplit'] == null, function () {\n            return '`validationSplit` is not supported by `fitDataset()`. ' + 'Use validationData instead.';\n          });\n\n          if (!model.isTraining) {\n            _context.next = 8;\n            break;\n          }\n\n          throw new Error('Cannot start training because another fit() call is ongoing.');\n\n        case 8:\n          model.isTraining = true;\n          _context.prev = 9;\n          doValidation = args.validationData != null;\n\n          if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n              tfc.util.assert(args.validationBatches == null || args.validationBatches > 0 && Number.isInteger(args.validationBatches), function () {\n                return \"For fitDataset() with dataset-based validation, \" + \"config.validationBatches is expected not to be provided, \" + \"or to be a positive integer, \" + (\"but got \" + args.validationBatches);\n              });\n            } else {\n              validationData = standardizeTensorValidationData(args.validationData);\n              valXs = validationData.xs;\n              valYs = validationData.ys;\n            }\n          }\n\n          trainFunction = model.makeTrainFunction();\n          outLabels = model.getDedupedMetricsNames();\n\n          if (doValidation) {\n            callbackMetrics = outLabels.slice().concat(outLabels.map(function (n) {\n              return 'val_' + n;\n            }));\n          } else {\n            callbackMetrics = outLabels.slice();\n          }\n\n          callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n          verbose = args.verbose == null ? 1 : args.verbose;\n          _configureCallbacks = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, doValidation, callbackMetrics), callbackList = _configureCallbacks.callbackList, history = _configureCallbacks.history;\n          callbackList.setModel(model);\n          model.history = history;\n          _context.next = 22;\n          return _regeneratorRuntime.awrap(callbackList.onTrainBegin());\n\n        case 22:\n          model.stopTraining_ = false;\n          epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n          _context.next = 26;\n          return _regeneratorRuntime.awrap(dataset.iterator());\n\n        case 26:\n          dataIterator = _context.sent;\n\n        case 27:\n          if (!(epoch < args.epochs)) {\n            _context.next = 98;\n            break;\n          }\n\n          epochLogs = {};\n          _context.next = 31;\n          return _regeneratorRuntime.awrap(callbackList.onEpochBegin(epoch));\n\n        case 31:\n          stepsDone = 0;\n          batchIndex = 0;\n\n          if (hasBatchesPerEpoch) {\n            _context.next = 37;\n            break;\n          }\n\n          _context.next = 36;\n          return _regeneratorRuntime.awrap(dataset.iterator());\n\n        case 36:\n          dataIterator = _context.sent;\n\n        case 37:\n          if (!(hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true)) {\n            _context.next = 91;\n            break;\n          }\n\n          _context.next = 40;\n          return _regeneratorRuntime.awrap(dataIterator.next());\n\n        case 40:\n          iteratorOut = _context.sent;\n\n          if (!(hasBatchesPerEpoch && iteratorOut.done)) {\n            _context.next = 44;\n            break;\n          }\n\n          console.warn('You provided `batchesPerEpoch` as ' + (args.batchesPerEpoch + \", \") + 'but your dataset iterator ran out of data after ' + (stepsDone + \" batches; \") + 'interrupting training. Make sure that your ' + 'dataset can generate at least `batchesPerEpoch * epochs` ' + 'batches (in this case, ' + (args.batchesPerEpoch * args.epochs + \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n          return _context.abrupt(\"break\", 91);\n\n        case 44:\n          if (!(iteratorOut.value != null)) {\n            _context.next = 73;\n            break;\n          }\n\n          _standardizeDataItera = standardizeDataIteratorOutput(model, iteratorOut.value), xs = _standardizeDataItera.xs, ys = _standardizeDataItera.ys;\n          batchLogs = {};\n          batchLogs['batch'] = batchIndex;\n          batchLogs['size'] = xs[0].shape[0];\n          _context.next = 51;\n          return _regeneratorRuntime.awrap(callbackList.onBatchBegin(batchIndex, batchLogs));\n\n        case 51:\n          sampleWeights = [];\n\n          if (!(args.classWeight != null)) {\n            _context.next = 64;\n            break;\n          }\n\n          standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n          i = 0;\n\n        case 55:\n          if (!(i < standardClassWeights.length)) {\n            _context.next = 64;\n            break;\n          }\n\n          _context.t0 = sampleWeights;\n          _context.next = 59;\n          return _regeneratorRuntime.awrap(standardizeWeights(ys[i], null, standardClassWeights[i]));\n\n        case 59:\n          _context.t1 = _context.sent;\n\n          _context.t0.push.call(_context.t0, _context.t1);\n\n        case 61:\n          ++i;\n          _context.next = 55;\n          break;\n\n        case 64:\n          ins = xs.concat(ys).concat(sampleWeights);\n          outs = trainFunction(ins);\n          tfc.dispose(ins);\n\n          for (_i = 0; _i < outLabels.length; ++_i) {\n            label = outLabels[_i];\n            out = outs[_i];\n            batchLogs[label] = out;\n            tfc.keep(out);\n          }\n\n          _context.next = 70;\n          return _regeneratorRuntime.awrap(callbackList.onBatchEnd(batchIndex, batchLogs));\n\n        case 70:\n          disposeTensorsInLogs(batchLogs);\n          batchIndex++;\n          stepsDone++;\n\n        case 73:\n          if (!(hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch : iteratorOut.done)) {\n            _context.next = 87;\n            break;\n          }\n\n          if (!doValidation) {\n            _context.next = 86;\n            break;\n          }\n\n          valOuts = void 0;\n\n          if (!isDatasetObject(args.validationData)) {\n            _context.next = 84;\n            break;\n          }\n\n          _context.t2 = toList;\n          _context.next = 80;\n          return _regeneratorRuntime.awrap(model.evaluateDataset(args.validationData, {\n            batches: args.validationBatches\n          }));\n\n        case 80:\n          _context.t3 = _context.sent;\n          valOuts = (0, _context.t2)(_context.t3);\n          _context.next = 85;\n          break;\n\n        case 84:\n          valOuts = toList(model.evaluate(valXs, valYs, {\n            batchSize: args.validationBatchSize == null ? DEFAULT_VALIDATION_BATCH_SIZE : args.validationBatchSize,\n            verbose: 0\n          }));\n\n        case 85:\n          for (_i2 = 0; _i2 < model.metricsNames.length; ++_i2) {\n            epochLogs[\"val_\" + model.metricsNames[_i2]] = valOuts[_i2];\n          }\n\n        case 86:\n          return _context.abrupt(\"break\", 91);\n\n        case 87:\n          if (!model.stopTraining_) {\n            _context.next = 89;\n            break;\n          }\n\n          return _context.abrupt(\"break\", 91);\n\n        case 89:\n          _context.next = 37;\n          break;\n\n        case 91:\n          _context.next = 93;\n          return _regeneratorRuntime.awrap(callbackList.onEpochEnd(epoch, epochLogs));\n\n        case 93:\n          epoch++;\n\n          if (!model.stopTraining_) {\n            _context.next = 96;\n            break;\n          }\n\n          return _context.abrupt(\"break\", 98);\n\n        case 96:\n          _context.next = 27;\n          break;\n\n        case 98:\n          _context.next = 100;\n          return _regeneratorRuntime.awrap(callbackList.onTrainEnd());\n\n        case 100:\n          _context.next = 102;\n          return _regeneratorRuntime.awrap(model.history.syncData());\n\n        case 102:\n          return _context.abrupt(\"return\", model.history);\n\n        case 103:\n          _context.prev = 103;\n          model.isTraining = false;\n          return _context.finish(103);\n\n        case 106:\n        case \"end\":\n          return _context.stop();\n      }\n    }\n  }, null, null, [[9,, 103, 106]], Promise);\n}\n\nfunction getStepsPerEpoch(dataset, args) {\n  var stepsPerEpoch = null;\n\n  if (args.batchesPerEpoch != null) {\n    stepsPerEpoch = args.batchesPerEpoch;\n  } else if (Number.isFinite(dataset.size)) {\n    stepsPerEpoch = dataset.size;\n  }\n\n  return stepsPerEpoch;\n}\n\nfunction isDatasetObject(dataset) {\n  return typeof dataset.iterator === 'function';\n}\n\nfunction isLazyIteratorObject(iterator) {\n  return typeof iterator.next === 'function';\n}\n\nexport function evaluateDataset(model, dataset, args) {\n  var hasBatches, f, outs, dataIterator, numExamples, batch, _loop3, _ret, i, oldScalar;\n\n  return _regeneratorRuntime.async(function evaluateDataset$(_context3) {\n    while (1) {\n      switch (_context3.prev = _context3.next) {\n        case 0:\n          args = args || {};\n          hasBatches = args.batches != null;\n          f = model.testFunction;\n          outs = [];\n\n          if (!(args.verbose > 0)) {\n            _context3.next = 6;\n            break;\n          }\n\n          throw new NotImplementedError('Verbose mode is not implemented yet.');\n\n        case 6:\n          tfc.util.assert(!hasBatches || args.batches > 0 && Number.isInteger(args.batches), function () {\n            return 'Test loop expects `batches` to be a positive integer, but ' + (\"received \" + JSON.stringify(args.batches));\n          });\n\n          if (!isLazyIteratorObject(dataset)) {\n            _context3.next = 11;\n            break;\n          }\n\n          _context3.t0 = dataset;\n          _context3.next = 14;\n          break;\n\n        case 11:\n          _context3.next = 13;\n          return _regeneratorRuntime.awrap(dataset.iterator());\n\n        case 13:\n          _context3.t0 = _context3.sent;\n\n        case 14:\n          dataIterator = _context3.t0;\n          numExamples = 0;\n          batch = 0;\n\n          _loop3 = function _loop3() {\n            var iteratorOut;\n            return _regeneratorRuntime.async(function _loop3$(_context2) {\n              while (1) {\n                switch (_context2.prev = _context2.next) {\n                  case 0:\n                    _context2.next = 2;\n                    return _regeneratorRuntime.awrap(dataIterator.next());\n\n                  case 2:\n                    iteratorOut = _context2.sent;\n                    outs = tfc.tidy(function () {\n                      if (iteratorOut.value) {\n                        (function () {\n                          var _standardizeDataItera2 = standardizeDataIteratorOutput(model, iteratorOut.value),\n                              xs = _standardizeDataItera2.xs,\n                              ys = _standardizeDataItera2.ys;\n\n                          var xsAndYs = xs.concat(ys);\n                          var batchOuts = tfc.tidy(function () {\n                            return f(xsAndYs);\n                          });\n                          tfc.dispose(xsAndYs);\n\n                          if (batch === 0) {\n                            for (var _i3 = 0; _i3 < batchOuts.length; ++_i3) {\n                              outs.push(scalar(0));\n                            }\n                          }\n\n                          var batchSize = xsAndYs[0].shape[0];\n\n                          var _loop4 = function _loop4(_i4) {\n                            var batchOut = batchOuts[_i4];\n                            var oldScalar = outs[_i4];\n                            outs[_i4] = tfc.tidy(function () {\n                              return tfc.add(outs[_i4], tfc.mul(batchSize, batchOut));\n                            });\n\n                            if (batch > 0) {\n                              tfc.dispose(oldScalar);\n                            }\n                          };\n\n                          for (var _i4 = 0; _i4 < batchOuts.length; ++_i4) {\n                            _loop4(_i4);\n                          }\n\n                          tfc.dispose(batchOuts);\n                          numExamples += batchSize;\n                          ++batch;\n                        })();\n                      }\n\n                      return outs;\n                    });\n\n                    if (!iteratorOut.done) {\n                      _context2.next = 7;\n                      break;\n                    }\n\n                    if (hasBatches) {\n                      console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' + 'Interrupting evalution. Make sure that your ' + 'dataset can generate at least `batches` ' + (\"batches (in this case, \" + args.batches + \" batches). \") + 'You may need to use the repeat() function when building ' + 'your dataset.');\n                    }\n\n                    return _context2.abrupt(\"return\", \"break\");\n\n                  case 7:\n                  case \"end\":\n                    return _context2.stop();\n                }\n              }\n            }, null, null, null, Promise);\n          };\n\n        case 18:\n          if (!(hasBatches ? batch < args.batches : true)) {\n            _context3.next = 26;\n            break;\n          }\n\n          _context3.next = 21;\n          return _regeneratorRuntime.awrap(_loop3());\n\n        case 21:\n          _ret = _context3.sent;\n\n          if (!(_ret === \"break\")) {\n            _context3.next = 24;\n            break;\n          }\n\n          return _context3.abrupt(\"break\", 26);\n\n        case 24:\n          _context3.next = 18;\n          break;\n\n        case 26:\n          for (i = 0; i < outs.length; ++i) {\n            oldScalar = outs[i];\n            outs[i] = tfc.div(outs[i], numExamples);\n            tfc.dispose(oldScalar);\n          }\n\n          return _context3.abrupt(\"return\", singletonOrArray(outs));\n\n        case 28:\n        case \"end\":\n          return _context3.stop();\n      }\n    }\n  }, null, null, null, Promise);\n}","map":{"version":3,"sources":["../../src/engine/training_dataset.ts"],"names":[],"mappings":";;;;;;;;AAAA;;;;;;;;AAQG;AAMH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,MAAR,QAAqB,uBAArB;AACA,SAAsB,kBAAtB,EAA8F,oBAA9F;AACA,SAAQ,mBAAR,EAA6B,UAA7B;AACA,SAAQ,oBAAR;AAEA,SAAQ,gBAAR,EAA0B,MAA1B;AAGA,SAAqC,uBAArC,EAA8D,kBAA9D;AAkKA,IAAM,6BAA6B,GAAG,EAAtC;;AAgBA,SAAS,6BAAT,CAII,KAJJ,EAIgB,WAJhB,EAI+B;EAC7B,IAAI,EAAJ;EACA,IAAI,EAAJ;EAEA,IAAM,cAAc,GAAG,WAAvB;EACA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;EACA,EAAE,GAAG,cAAc,CAAC,IAAD,CAAnB;EACA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,EAAE,IAAI,IAAN,IAAc,EAAE,IAAI,IADxB,EAEI;IAAA,OAAM,iEACF,4DADE,GAEF,8DAFE,GAGF,4DAHE,SAIC,WAJD,CAAN;EAAA,CAFJ;EAQA,IAAM,WAAW,GACb,yBAAyB,CAAC,OAAD,EAAU,KAAK,CAAC,UAAhB,EAA4B,EAA5B,CAD7B;EAEA,IAAM,WAAW,GACb,yBAAyB,CAAC,QAAD,EAAW,KAAK,CAAC,WAAjB,EAA8B,EAA9B,CAD7B;EAGA,IAAM,SAAS,GAAW,WAAW,CAAC,CAAD,CAAX,CAAe,KAAf,CAAqB,CAArB,CAA1B;EAEA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,MAAN,CAAa,MADxC,EAEI;IAAA,OAAM,qBAAmB,KAAK,CAAC,MAAN,CAAa,MAAhC,gDACU,WAAW,CAAC,MADtB,0CAEC,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,UAArB,CAFD,OAAN;EAAA,CAFJ;EAMA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAZ,KAAuB,KAAK,CAAC,OAAN,CAAc,MADzC,EAEI;IAAA,OACI,qBAAmB,KAAK,CAAC,OAAN,CAAc,MAAjC,iDACY,WAAW,CAAC,MADxB,4CAEG,IAAI,CAAC,SAAL,CAAe,KAAK,CAAC,WAArB,CAFH,OADJ;EAAA,CAFJ;;EA5B6B,2BAmCpB,MAnCoB;IAoC3B,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI;MAAA,OAAM,iCACC,KAAK,CAAC,UAAN,CAAiB,MAAjB,CADD,aAEI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAFJ,2BAGW,SAHX,wBAGuC,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAHvC,OAAN;IAAA,CAFJ;EApC2B;;EAmC7B,KAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;IAAA,MAAnD,MAAmD;EAO3D;;EA1C4B,6BA4CpB,MA5CoB;IA6C3B,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,MAAiC,SADrC,EAEI;MAAA,OAAM,kCACC,KAAK,CAAC,WAAN,CAAkB,MAAlB,CADD,aAEI,WAAW,CAAC,MAAD,CAAX,CAAoB,KAApB,CAA0B,CAA1B,CAFJ,2BAGW,SAHX,wBAGuC,KAAK,CAAC,UAAN,CAAiB,CAAjB,CAHvC,OAAN;IAAA,CAFJ;EA7C2B;;EA4C7B,KAAK,IAAI,MAAM,GAAG,CAAlB,EAAqB,MAAM,GAAG,WAAW,CAAC,MAA1C,EAAkD,MAAM,EAAxD,EAA4D;IAAA,OAAnD,MAAmD;EAO3D;;EAED,OAAO;IAAC,EAAE,EAAE,WAAL;IAAkB,EAAE,EAAE;EAAtB,CAAP;AACD;;AAED,SAAS,yBAAT,CACI,aADJ,EAC2B,KAD3B,EAC4C,MAD5C,EACsE;EACpE,IAAI,MAAM,YAAY,GAAG,CAAC,MAA1B,EAAkC;IAChC,OAAO,CAAC,MAAD,CAAP;EACD,CAFD,MAEO,IAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;IAChC,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,MAAM,CAAC,MAAP,KAAkB,KAAK,CAAC,MAD5B,EAEI;MAAA,iCAA8B,MAAM,CAAC,MAArC,+BACI,KAAK,CAAC,MADV,sBACiC,aADjC,cACuD,KADvD;IAAA,CAFJ;IAIA,OAAO,MAAP;EACD,CANM,MAMA;IACL,IAAM,MAAM,GAAiB,EAA7B;;IAEA,qDAAmB,KAAnB,wCAA0B;MAAA,IAAf,IAAe;;MACxB,IAAI,MAAM,CAAC,IAAD,CAAN,IAAgB,IAApB,EAA0B;QACxB,MAAM,IAAI,UAAJ,CACF,mEACG,aADH,cACyB,IADzB,QADE,CAAN;MAGD;;MACD,MAAM,CAAC,IAAP,CAAY,MAAM,CAAC,IAAD,CAAlB;IACD;;IACD,OAAO,MAAP;EACD;AACF;;AAED,SAAS,+BAAT,CACI,IADJ,EAKqC;EAEnC,IAAI,IAAI,CAAC,MAAL,KAAgB,CAApB,EAAuB;IACrB,MAAM,IAAI,mBAAJ,CACF,wDADE,CAAN;EAED;;EACD,OAAO;IAAC,EAAE,EAAE,IAAI,CAAC,CAAD,CAAT;IAAc,EAAE,EAAE,IAAI,CAAC,CAAD;EAAtB,CAAP;AACD;;AAED,OAAO,SAAe,UAAf,CAIH,KAJG,EAIS,OAJT,EAKH,IALG;EAAA;;EAAA;IAAA;MAAA;QAAA;UAMC,kBAND,GAMsB,IAAI,CAAC,eAAL,IAAwB,IAN9C;UAOL,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,KAAK,CAAC,SAAN,IAAmB,IADvB,EAEI;YAAA,OAAM,2DACF,0CADJ;UAAA,CAFJ;UAKA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,IAAI,IADZ,EAEI;YAAA,OAAM,oGAAN;UAAA,CAFJ;UAIA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,MAAL,IAAe,IAAf,IAAuB,IAAI,CAAC,MAAL,GAAc,CAArC,IAA0C,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,MAAtB,CAD9C,EAEI;YAAA,OAAM,yFACkB,IAAI,CAAC,MADvB,CAAN;UAAA,CAFJ;UAIA,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,kBAAD,IACK,IAAI,CAAC,eAAL,GAAuB,CAAvB,IAA4B,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,eAAtB,CAFrC,EAGI;YAAA,OAAM,+GACwC,IAAI,CAAC,eAD7C,CAAN;UAAA,CAHJ;UAKA,GAAG,CAAC,IAAJ,CAAS,MAAT,CAEK,IAAY,CAAC,iBAAD,CAAZ,IAAmC,IAFxC,EAGI;YAAA,OAAM,2DACF,6BADJ;UAAA,CAHJ;;UAzBK,KA+BD,KAAK,CAAC,UA/BL;YAAA;YAAA;UAAA;;UAAA,MAgCG,IAAI,KAAJ,CACF,8DADE,CAhCH;;QAAA;UAmCL,KAAK,CAAC,UAAN,GAAmB,IAAnB;UAnCK;UAsCG,YAtCH,GAsCkB,IAAI,CAAC,cAAL,IAAuB,IAtCzC;;UAyCH,IAAI,YAAJ,EAAkB;YAChB,IAAI,eAAe,CAAC,IAAI,CAAC,cAAN,CAAnB,EAA0C;cACxC,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,IAAI,CAAC,iBAAL,IAA0B,IAA1B,IACK,IAAI,CAAC,iBAAL,GAAyB,CAAzB,IACA,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,iBAAtB,CAHT,EAII;gBAAA,OAAM,mKAGS,IAAI,CAAC,iBAHd,CAAN;cAAA,CAJJ;YAQD,CATD,MASO;cACC,cADD,GACkB,+BAA+B,CAClD,IAAI,CAAC,cAD6C,CADjD;cAQL,KAAK,GAAG,cAAc,CAAC,EAAvB;cACA,KAAK,GAAG,cAAc,CAAC,EAAvB;YACD;UACF;;UAEK,aAhEH,GAgEmB,KAAK,CAAC,iBAAN,EAhEnB;UAiEG,SAjEH,GAiEe,KAAK,CAAC,sBAAN,EAjEf;;UAoEH,IAAI,YAAJ,EAAkB;YAChB,eAAe,GACX,SAAS,CAAC,KAAV,GAAkB,MAAlB,CAAyB,SAAS,CAAC,GAAV,CAAc,UAAA,CAAC;cAAA,OAAI,SAAS,CAAb;YAAA,CAAf,CAAzB,CADJ;UAED,CAHD,MAGO;YACL,eAAe,GAAG,SAAS,CAAC,KAAV,EAAlB;UACD;;UAEK,SA3EH,GA2Ee,oBAAoB,CAAC,IAAI,CAAC,SAAN,EAAiB,IAAI,CAAC,UAAtB,CA3EnC;UA4EG,OA5EH,GA4Ea,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,CAAvB,GAA2B,IAAI,CAAC,OA5E7C;UAAA,sBA6E6B,kBAAkB,CAC9C,SAD8C,EACnC,OADmC,EAC1B,IAAI,CAAC,MADqB,EACb,IADa,EACP,IADO,EAE9C,gBAAgB,CAAC,OAAD,EAAU,IAAV,CAF8B,EAG9C,IAH8C,EAI9C,YAJ8C,EAIhC,eAJgC,CA7E/C,EA6EI,YA7EJ,uBA6EI,YA7EJ,EA6EkB,OA7ElB,uBA6EkB,OA7ElB;UAkFH,YAAY,CAAC,QAAb,CAAsB,KAAtB;UACA,KAAK,CAAC,OAAN,GAAgB,OAAhB;UAnFG;UAAA,iCAqFG,YAAY,CAAC,YAAb,EArFH;;QAAA;UAsFH,KAAK,CAAC,aAAN,GAAsB,KAAtB;UACI,KAvFD,GAuFS,IAAI,CAAC,YAAL,IAAqB,IAArB,GAA4B,CAA5B,GAAgC,IAAI,CAAC,YAvF9C;UAAA;UAAA,iCAyFsB,OAAO,CAAC,QAAR,EAzFtB;;QAAA;UAyFC,YAzFD;;QAAA;UAAA,MA0FI,KAAK,GAAG,IAAI,CAAC,MA1FjB;YAAA;YAAA;UAAA;;UA2FK,SA3FL,GA2FiC,EA3FjC;UAAA;UAAA,iCA4FK,YAAY,CAAC,YAAb,CAA0B,KAA1B,CA5FL;;QAAA;UA6FG,SA7FH,GA6Fe,CA7Ff;UA8FG,UA9FH,GA8FgB,CA9FhB;;UAAA,IA+FI,kBA/FJ;YAAA;YAAA;UAAA;;UAAA;UAAA,iCAgGsB,OAAO,CAAC,QAAR,EAhGtB;;QAAA;UAgGC,YAhGD;;QAAA;UAAA,MAkGM,kBAAkB,GAAG,SAAS,GAAG,IAAI,CAAC,eAApB,GAAsC,IAlG9D;YAAA;YAAA;UAAA;;UAAA;UAAA,iCAmG2B,YAAY,CAAC,IAAb,EAnG3B;;QAAA;UAmGO,WAnGP;;UAAA,MAuGK,kBAAkB,IAAI,WAAW,CAAC,IAvGvC;YAAA;YAAA;UAAA;;UAwGG,OAAO,CAAC,IAAR,CACI,wCACG,IAAI,CAAC,eADR,WAEA,kDAFA,IAGG,SAHH,mBAIA,6CAJA,GAKA,2DALA,GAMA,yBANA,IAOG,IAAI,CAAC,eAAL,GAAuB,IAAI,CAAC,MAP/B,oBAQA,0DARA,GASA,eAVJ;UAxGH;;QAAA;UAAA,MAsHK,WAAW,CAAC,KAAZ,IAAqB,IAtH1B;YAAA;YAAA;UAAA;;UAAA,wBAwHO,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CAxHpC,EAuHU,EAvHV,yBAuHU,EAvHV,EAuHc,EAvHd,yBAuHc,EAvHd;UAyHS,SAzHT,GAyHqC,EAzHrC;UA0HG,SAAS,CAAC,OAAD,CAAT,GAAqB,UAArB;UACA,SAAS,CAAC,MAAD,CAAT,GAAoB,EAAE,CAAC,CAAD,CAAF,CAAM,KAAN,CAAY,CAAZ,CAApB;UA3HH;UAAA,iCA6HS,YAAY,CAAC,YAAb,CAA0B,UAA1B,EAAsC,SAAtC,CA7HT;;QAAA;UA+HS,aA/HT,GA+HuC,EA/HvC;;UAAA,MAgIO,IAAI,CAAC,WAAL,IAAoB,IAhI3B;YAAA;YAAA;UAAA;;UAiIW,oBAjIX,GAkIS,uBAAuB,CAAC,IAAI,CAAC,WAAN,EAAmB,KAAK,CAAC,WAAzB,CAlIhC;UAmIc,CAnId,GAmIkB,CAnIlB;;QAAA;UAAA,MAmIqB,CAAC,GAAG,oBAAoB,CAAC,MAnI9C;YAAA;YAAA;UAAA;;UAAA,cAoIO,aApIP;UAAA;UAAA,iCAoIgC,kBAAkB,CACvC,EAAE,CAAC,CAAD,CADqC,EAChC,IADgC,EAC1B,oBAAoB,CAAC,CAAD,CADM,CApIlD;;QAAA;UAAA;;UAAA,YAoIqB,IApIrB;;QAAA;UAmIsD,EAAE,CAnIxD;UAAA;UAAA;;QAAA;UA0IS,GA1IT,GA0Ie,EAAE,CAAC,MAAH,CAAU,EAAV,EAAc,MAAd,CAAqB,aAArB,CA1If;UA2IS,IA3IT,GA2IgB,aAAa,CAAC,GAAD,CA3I7B;UA4IG,GAAG,CAAC,OAAJ,CAAY,GAAZ;;UACA,KAAS,EAAT,GAAa,CAAb,EAAgB,EAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,EAAxC,EAA2C;YACnC,KADmC,GAC3B,SAAS,CAAC,EAAD,CADkB;YAEnC,GAFmC,GAE7B,IAAI,CAAC,EAAD,CAFyB;YAGzC,SAAS,CAAC,KAAD,CAAT,GAAmB,GAAnB;YACA,GAAG,CAAC,IAAJ,CAAS,GAAT;UACD;;UAlJJ;UAAA,iCAoJS,YAAY,CAAC,UAAb,CAAwB,UAAxB,EAAoC,SAApC,CApJT;;QAAA;UAqJG,oBAAoB,CAAC,SAAD,CAApB;UAEA,UAAU;UACV,SAAS;;QAxJZ;UAAA,MA2JK,kBAAkB,GAAG,SAAS,IAAI,IAAI,CAAC,eAArB,GACG,WAAW,CAAC,IA5JtC;YAAA;YAAA;UAAA;;UAAA,KA8JO,YA9JP;YAAA;YAAA;UAAA;;UA+JS,OA/JT;;UAAA,KAgKS,eAAe,CAAC,IAAI,CAAC,cAAN,CAhKxB;YAAA;YAAA;UAAA;;UAAA,cAiKiB,MAjKjB;UAAA;UAAA,iCAiK8B,KAAK,CAAC,eAAN,CACnB,IAAI,CAAC,cADc,EACE;YAAC,OAAO,EAAE,IAAI,CAAC;UAAf,CADF,CAjK9B;;QAAA;UAAA;UAiKO,OAjKP;UAAA;UAAA;;QAAA;UAoKO,OAAO,GAAG,MAAM,CAAC,KAAK,CAAC,QAAN,CAAe,KAAf,EAAsB,KAAtB,EAA6B;YAC5C,SAAS,EAAE,IAAI,CAAC,mBAAL,IAA4B,IAA5B,GACP,6BADO,GAEP,IAAI,CAAC,mBAHmC;YAI5C,OAAO,EAAE;UAJmC,CAA7B,CAAD,CAAhB;;QApKP;UA2KK,KAAS,GAAT,GAAa,CAAb,EAAgB,GAAC,GAAG,KAAK,CAAC,YAAN,CAAmB,MAAvC,EAA+C,EAAE,GAAjD,EAAoD;YAClD,SAAS,UAAQ,KAAK,CAAC,YAAN,CAAmB,GAAnB,CAAR,CAAT,GAA4C,OAAO,CAAC,GAAD,CAAnD;UACD;;QA7KN;UAAA;;QAAA;UAAA,KAuLK,KAAK,CAAC,aAvLX;YAAA;YAAA;UAAA;;UAAA;;QAAA;UAAA;UAAA;;QAAA;UAAA;UAAA,iCA2LK,YAAY,CAAC,UAAb,CAAwB,KAAxB,EAA+B,SAA/B,CA3LL;;QAAA;UA4LD,KAAK;;UA5LJ,KA6LG,KAAK,CAAC,aA7LT;YAAA;YAAA;UAAA;;UAAA;;QAAA;UAAA;UAAA;;QAAA;UAAA;UAAA,iCAiMG,YAAY,CAAC,UAAb,EAjMH;;QAAA;UAAA;UAAA,iCAkMG,KAAK,CAAC,OAAN,CAAc,QAAd,EAlMH;;QAAA;UAAA,iCAmMI,KAAK,CAAC,OAnMV;;QAAA;UAAA;UAqMH,KAAK,CAAC,UAAN,GAAmB,KAAnB;UArMG;;QAAA;QAAA;UAAA;MAAA;IAAA;EAAA;AAAA;;AA0MP,SAAS,gBAAT,CACI,OADJ,EACyB,IADzB,EACqD;EAEnD,IAAI,aAAa,GAAW,IAA5B;;EACA,IAAI,IAAI,CAAC,eAAL,IAAwB,IAA5B,EAAkC;IAChC,aAAa,GAAG,IAAI,CAAC,eAArB;EACD,CAFD,MAEO,IAAI,MAAM,CAAC,QAAP,CAAgB,OAAO,CAAC,IAAxB,CAAJ,EAAmC;IACxC,aAAa,GAAG,OAAO,CAAC,IAAxB;EACD;;EACD,OAAO,aAAP;AACD;;AAID,SAAS,eAAT,CACI,OADJ,EAKc;EACZ,OAAQ,OAAQ,OAAsB,CAAC,QAA/B,KAA4C,UAApD;AACD;;AAID,SAAS,oBAAT,CAAiC,QAAjC,EACgD;EAC9C,OAAQ,OAAQ,QAA4B,CAAC,IAArC,KAA8C,UAAtD;AACD;;AAED,OAAO,SAAe,eAAf,CAIH,KAJG,EAIS,OAJT,EAKH,IALG;EAAA;;EAAA;IAAA;MAAA;QAAA;UAML,IAAI,GAAG,IAAI,IAAI,EAAf;UACM,UAPD,GAOc,IAAI,CAAC,OAAL,IAAgB,IAP9B;UAQC,CARD,GAQK,KAAK,CAAC,YARX;UASD,IATC,GASoB,EATpB;;UAAA,MAUD,IAAI,CAAC,OAAL,GAAe,CAVd;YAAA;YAAA;UAAA;;UAAA,MAWG,IAAI,mBAAJ,CAAwB,sCAAxB,CAXH;;QAAA;UAcL,GAAG,CAAC,IAAJ,CAAS,MAAT,CACI,CAAC,UAAD,IAAgB,IAAI,CAAC,OAAL,GAAe,CAAf,IAAoB,MAAM,CAAC,SAAP,CAAiB,IAAI,CAAC,OAAtB,CADxC,EAEI;YAAA,OAAM,8EACU,IAAI,CAAC,SAAL,CAAe,IAAI,CAAC,OAApB,CADV,CAAN;UAAA,CAFJ;;UAdK,KAkBgB,oBAAoB,CAAC,OAAD,CAlBpC;YAAA;YAAA;UAAA;;UAAA,eAmBD,OAnBC;UAAA;UAAA;;QAAA;UAAA;UAAA,iCAoBM,OAAsB,CAAC,QAAvB,EApBN;;QAAA;UAAA;;QAAA;UAkBC,YAlBD;UAsBD,WAtBC,GAsBa,CAtBb;UAuBD,KAvBC,GAuBO,CAvBP;;UAAA;YAAA;YAAA;cAAA;gBAAA;kBAAA;oBAAA;oBAAA,iCA0BuB,YAAY,CAAC,IAAb,EA1BvB;;kBAAA;oBA0BG,WA1BH;oBA2BH,IAAI,GAAG,GAAG,CAAC,IAAJ,CAAS,YAAK;sBACnB,IAAI,WAAW,CAAC,KAAhB,EAAuB;wBAAA;0BAGrB,6BACI,6BAA6B,CAAC,KAAD,EAAQ,WAAW,CAAC,KAApB,CADjC;0BAAA,IAAO,EAAP,0BAAO,EAAP;0BAAA,IAAW,EAAX,0BAAW,EAAX;;0BAEA,IAAM,OAAO,GAAG,EAAE,CAAC,MAAH,CAAU,EAAV,CAAhB;0BACA,IAAM,SAAS,GAAG,GAAG,CAAC,IAAJ,CAAS;4BAAA,OAAM,CAAC,CAAC,OAAD,CAAP;0BAAA,CAAT,CAAlB;0BACA,GAAG,CAAC,OAAJ,CAAY,OAAZ;;0BAEA,IAAI,KAAK,KAAK,CAAd,EAAiB;4BACf,KAAK,IAAI,GAAC,GAAG,CAAb,EAAgB,GAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,GAAxC,EAA2C;8BACzC,IAAI,CAAC,IAAL,CAAU,MAAM,CAAC,CAAD,CAAhB;4BACD;0BACF;;0BAED,IAAM,SAAS,GAAG,OAAO,CAAC,CAAD,CAAP,CAAW,KAAX,CAAiB,CAAjB,CAAlB;;0BAfqB,6BAgBZ,GAhBY;4BAiBnB,IAAM,QAAQ,GAAG,SAAS,CAAC,GAAD,CAA1B;4BACA,IAAM,SAAS,GAAG,IAAI,CAAC,GAAD,CAAtB;4BACA,IAAI,CAAC,GAAD,CAAJ,GACI,GAAG,CAAC,IAAJ,CAAS;8BAAA,OAAM,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,GAAD,CAAZ,EAAiB,GAAG,CAAC,GAAJ,CAAQ,SAAR,EAAmB,QAAnB,CAAjB,CAAN;4BAAA,CAAT,CADJ;;4BAEA,IAAI,KAAK,GAAG,CAAZ,EAAe;8BACb,GAAG,CAAC,OAAJ,CAAY,SAAZ;4BACD;0BAvBkB;;0BAgBrB,KAAK,IAAI,GAAC,GAAG,CAAb,EAAgB,GAAC,GAAG,SAAS,CAAC,MAA9B,EAAsC,EAAE,GAAxC,EAA2C;4BAAA,OAAlC,GAAkC;0BAQ1C;;0BACD,GAAG,CAAC,OAAJ,CAAY,SAAZ;0BACA,WAAW,IAAI,SAAf;0BAEA,EAAE,KAAF;wBA5BqB;sBA6BtB;;sBACD,OAAO,IAAP;oBACD,CAhCM,CAAP;;oBA3BG,KA6DC,WAAW,CAAC,IA7Db;sBAAA;sBAAA;oBAAA;;oBA8DD,IAAI,UAAJ,EAAgB;sBACd,OAAO,CAAC,IAAR,CACI,qEACA,8CADA,GAEA,0CAFA,gCAG0B,IAAI,CAAC,OAH/B,oBAIA,0DAJA,GAKA,eANJ;oBAOD;;oBAtEA;;kBAAA;kBAAA;oBAAA;gBAAA;cAAA;YAAA;UAAA;;QAAA;UAAA,MAyBE,UAAU,GAAG,KAAK,GAAG,IAAI,CAAC,OAAhB,GAA0B,IAzBtC;YAAA;YAAA;UAAA;;UAAA;UAAA;;QAAA;UAAA;;UAAA;YAAA;YAAA;UAAA;;UAAA;;QAAA;UAAA;UAAA;;QAAA;UA2EL,KAAS,CAAT,GAAa,CAAb,EAAgB,CAAC,GAAG,IAAI,CAAC,MAAzB,EAAiC,EAAE,CAAnC,EAAsC;YAC9B,SAD8B,GAClB,IAAI,CAAC,CAAD,CADc;YAEpC,IAAI,CAAC,CAAD,CAAJ,GAAU,GAAG,CAAC,GAAJ,CAAQ,IAAI,CAAC,CAAD,CAAZ,EAAiB,WAAjB,CAAV;YACA,GAAG,CAAC,OAAJ,CAAY,SAAZ;UACD;;UA/EI,kCAiFE,gBAAgB,CAAC,IAAD,CAjFlB;;QAAA;QAAA;UAAA;MAAA;IAAA;EAAA;AAAA","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Interfaces and methods for training models using TensorFlow.js datasets.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { scalar } from '@tensorflow/tfjs-core';\nimport { configureCallbacks, standardizeCallbacks } from '../base_callbacks';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { disposeTensorsInLogs } from '../logs';\nimport { singletonOrArray, toList } from '../utils/generic_utils';\nimport { standardizeClassWeights, standardizeWeights } from './training_utils';\n// Default batch size used during tensor-based validation.\nconst DEFAULT_VALIDATION_BATCH_SIZE = 32;\n/**\n * Standardize the output of a dataset iterator for use by\n * LayersModel.fitDataset().\n *\n * @param model: A `tf.LayersModel` object.\n * @param iteratorOut The output of a dataset iterator. It is required to be\n *   an object of the form `{xs: TensorOrArrayOrMap, ys:\n * TensorOrArrayOrMap}`, where `TensorOrArrayOrMap` is a single `tf.Tensor`,\n * a `tf.Tensor[]`, or a flat map from string names to `tf.Tensor`s.\n * @returns A flat array of `tf.Tensor` objects: the input `tf.Tensor`s\n *   followed by the target `tf.Tensor`s.  When `tf.Tensor`s are provided\n *   as a map, the order in the resulting array is taken from the `inputNames`\n *   and `outputNames` of the model.\n */\nfunction standardizeDataIteratorOutput(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, iteratorOut) {\n    let xs;\n    let ys;\n    const iteratorOutObj = iteratorOut;\n    xs = iteratorOutObj['xs'];\n    ys = iteratorOutObj['ys'];\n    tfc.util.assert(xs != null && ys != null, () => 'A Dataset iterator for fitDataset() is expected to generate ' +\n        'objects of the form `{xs: xVal, ys: yVal}`, where the two ' +\n        'values may be `tf.Tensor`, an array of Tensors, or a map of ' +\n        'string to Tensor.  The provided Dataset instead generates ' +\n        `${iteratorOut}`);\n    const flattenedXs = flattenTensorOrArrayOrMap('input', model.inputNames, xs);\n    const flattenedYs = flattenTensorOrArrayOrMap('output', model.outputNames, ys);\n    const batchSize = flattenedXs[0].shape[0];\n    tfc.util.assert(flattenedXs.length === model.inputs.length, () => `LayersModel has ${model.inputs.length} inputs, but the dataset ` +\n        `provides ${flattenedXs.length} inputs.  (Expected input keys: ` +\n        `${JSON.stringify(model.inputNames)})`);\n    tfc.util.assert(flattenedYs.length === model.outputs.length, () => `LayersModel has ${model.outputs.length} outputs, but the dataset ` +\n        `provides ${flattenedYs.length} outputs.  (Expected output keys: ` +\n        `${JSON.stringify(model.outputNames)})`);\n    for (let xIndex = 0; xIndex < flattenedXs.length; xIndex++) {\n        tfc.util.assert(flattenedXs[xIndex].shape[0] === batchSize, () => `Batch size mismatch: input ` +\n            `${model.inputNames[xIndex]} has ${flattenedXs[xIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    for (let yIndex = 0; yIndex < flattenedYs.length; yIndex++) {\n        tfc.util.assert(flattenedYs[yIndex].shape[0] === batchSize, () => `Batch size mismatch: output ` +\n            `${model.outputNames[yIndex]} has ${flattenedYs[yIndex].shape[0]}; ` +\n            `expected  ${batchSize} based on input ${model.inputNames[0]}.`);\n    }\n    return { xs: flattenedXs, ys: flattenedYs };\n}\nfunction flattenTensorOrArrayOrMap(inputOrOutput, names, values) {\n    if (values instanceof tfc.Tensor) {\n        return [values];\n    }\n    else if (Array.isArray(values)) {\n        tfc.util.assert(values.length === names.length, () => `Received an array of ${values.length} Tensors, but expected ${names.length} to match the ${inputOrOutput} keys ${names}.`);\n        return values;\n    }\n    else {\n        const result = [];\n        // Check that all the required keys are available.\n        for (const name of names) {\n            if (values[name] == null) {\n                throw new ValueError(`The feature data generated by the dataset lacks the required ` +\n                    `${inputOrOutput} key '${name}'.`);\n            }\n            result.push(values[name]);\n        }\n        return result;\n    }\n}\nfunction standardizeTensorValidationData(data) {\n    if (data.length === 3) {\n        throw new NotImplementedError('Validation with sample weights is not implemented yet.');\n    }\n    return { xs: data[0], ys: data[1] };\n}\nexport async function fitDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    const hasBatchesPerEpoch = args.batchesPerEpoch != null;\n    tfc.util.assert(model.optimizer != null, () => 'You must compile a model before training/testing. Use ' +\n        'LayersModel.compile(modelCompileConfig).');\n    tfc.util.assert(args != null, () => `For fitDataset(), the 2nd argument (config) is required, ` +\n        `but it is not provided in this call.`);\n    tfc.util.assert(args.epochs != null && args.epochs > 0 && Number.isInteger(args.epochs), () => `For fitDataset(), config.epochs is expected to be a positive ` +\n        `integer, but got ${args.epochs}`);\n    tfc.util.assert(!hasBatchesPerEpoch ||\n        (args.batchesPerEpoch > 0 && Number.isInteger(args.batchesPerEpoch)), () => `For fitDataset(), config.batchesPerEpoch is expected to be a ` +\n        `positive integer if specified, but got ${args.batchesPerEpoch}`);\n    tfc.util.assert(\n    // tslint:disable-next-line:no-any\n    args['validationSplit'] == null, () => '`validationSplit` is not supported by `fitDataset()`. ' +\n        'Use validationData instead.');\n    if (model.isTraining) {\n        throw new Error('Cannot start training because another fit() call is ongoing.');\n    }\n    model.isTraining = true;\n    try {\n        const doValidation = args.validationData != null;\n        let valXs;\n        let valYs;\n        if (doValidation) {\n            if (isDatasetObject(args.validationData)) {\n                tfc.util.assert(args.validationBatches == null ||\n                    (args.validationBatches > 0 &&\n                        Number.isInteger(args.validationBatches)), () => `For fitDataset() with dataset-based validation, ` +\n                    `config.validationBatches is expected not to be provided, ` +\n                    `or to be a positive integer, ` +\n                    `but got ${args.validationBatches}`);\n            }\n            else {\n                const validationData = standardizeTensorValidationData(args.validationData);\n                valXs = validationData.xs;\n                valYs = validationData.ys;\n            }\n        }\n        const trainFunction = model.makeTrainFunction();\n        const outLabels = model.getDedupedMetricsNames();\n        let callbackMetrics;\n        if (doValidation) {\n            callbackMetrics =\n                outLabels.slice().concat(outLabels.map(n => 'val_' + n));\n        }\n        else {\n            callbackMetrics = outLabels.slice();\n        }\n        const callbacks = standardizeCallbacks(args.callbacks, args.yieldEvery);\n        const verbose = args.verbose == null ? 1 : args.verbose;\n        const { callbackList, history } = configureCallbacks(callbacks, verbose, args.epochs, null, null, getStepsPerEpoch(dataset, args), null, // Batch size determined by the dataset itself.\n        doValidation, callbackMetrics);\n        callbackList.setModel(model);\n        model.history = history;\n        await callbackList.onTrainBegin();\n        model.stopTraining_ = false;\n        let epoch = args.initialEpoch == null ? 0 : args.initialEpoch;\n        let dataIterator = await dataset.iterator();\n        while (epoch < args.epochs) {\n            const epochLogs = {};\n            await callbackList.onEpochBegin(epoch);\n            let stepsDone = 0;\n            let batchIndex = 0;\n            if (!hasBatchesPerEpoch) {\n                dataIterator = await dataset.iterator();\n            }\n            while (hasBatchesPerEpoch ? stepsDone < args.batchesPerEpoch : true) {\n                const iteratorOut = await dataIterator.next();\n                // If `batchesPerEpoch` is specified, the dataset should not be\n                // exhausted until all epoches are done.\n                if (hasBatchesPerEpoch && iteratorOut.done) {\n                    console.warn('You provided `batchesPerEpoch` as ' +\n                        `${args.batchesPerEpoch}, ` +\n                        'but your dataset iterator ran out of data after ' +\n                        `${stepsDone} batches; ` +\n                        'interrupting training. Make sure that your ' +\n                        'dataset can generate at least `batchesPerEpoch * epochs` ' +\n                        'batches (in this case, ' +\n                        `${args.batchesPerEpoch * args.epochs} batches). ` +\n                        'You may need to use the repeat() function when building ' +\n                        'your dataset.');\n                    break;\n                }\n                if (iteratorOut.value != null) {\n                    const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                    const batchLogs = {};\n                    batchLogs['batch'] = batchIndex;\n                    batchLogs['size'] = xs[0].shape[0];\n                    await callbackList.onBatchBegin(batchIndex, batchLogs);\n                    const sampleWeights = [];\n                    if (args.classWeight != null) {\n                        const standardClassWeights = standardizeClassWeights(args.classWeight, model.outputNames);\n                        for (let i = 0; i < standardClassWeights.length; ++i) {\n                            sampleWeights.push(await standardizeWeights(ys[i], null, standardClassWeights[i]));\n                        }\n                    }\n                    // Train on batch.\n                    const ins = xs.concat(ys).concat(sampleWeights);\n                    const outs = trainFunction(ins);\n                    tfc.dispose(ins);\n                    for (let i = 0; i < outLabels.length; ++i) {\n                        const label = outLabels[i];\n                        const out = outs[i];\n                        batchLogs[label] = out;\n                        tfc.keep(out);\n                    }\n                    await callbackList.onBatchEnd(batchIndex, batchLogs);\n                    disposeTensorsInLogs(batchLogs);\n                    batchIndex++;\n                    stepsDone++;\n                }\n                if (hasBatchesPerEpoch ? stepsDone >= args.batchesPerEpoch :\n                    iteratorOut.done) {\n                    // Epoch finished. Perform validation.\n                    if (doValidation) {\n                        let valOuts;\n                        if (isDatasetObject(args.validationData)) {\n                            valOuts = toList(await model.evaluateDataset(args.validationData, { batches: args.validationBatches }));\n                        }\n                        else {\n                            valOuts = toList(model.evaluate(valXs, valYs, {\n                                batchSize: args.validationBatchSize == null ?\n                                    DEFAULT_VALIDATION_BATCH_SIZE :\n                                    args.validationBatchSize,\n                                verbose: 0\n                            }));\n                        }\n                        for (let i = 0; i < model.metricsNames.length; ++i) {\n                            epochLogs[`val_${model.metricsNames[i]}`] = valOuts[i];\n                        }\n                    }\n                    // Call `break` to exit one epoch lopp after validation is done. If\n                    // config.batchesPerEpoch is specified, an epoch while loop will\n                    // stop when `stepsDone >= config.batchesPerEpoch`. When\n                    // config.batchesPerEpoch is not provided, the following `break` is\n                    // required to exit the while lopp after dataset is exhausted.\n                    break;\n                }\n                if (model.stopTraining_) {\n                    break;\n                }\n            }\n            await callbackList.onEpochEnd(epoch, epochLogs);\n            epoch++;\n            if (model.stopTraining_) {\n                break;\n            }\n        }\n        await callbackList.onTrainEnd();\n        await model.history.syncData();\n        return model.history;\n    }\n    finally {\n        model.isTraining = false;\n    }\n}\n/** Helper function that determines number of steps (batches) per epoch. */\nfunction getStepsPerEpoch(dataset, args) {\n    // Attempt to determine # of batches in an epoch.\n    let stepsPerEpoch = null;\n    if (args.batchesPerEpoch != null) {\n        stepsPerEpoch = args.batchesPerEpoch;\n    }\n    else if (Number.isFinite(dataset.size)) {\n        stepsPerEpoch = dataset.size;\n    }\n    return stepsPerEpoch;\n}\n// Check if provided object is a Dataset object by checking its .iterator\n// element.\nfunction isDatasetObject(dataset) {\n    return (typeof dataset.iterator === 'function');\n}\n// Check if provided object is a LazyIterator object by checking it's .next\n// element.\nfunction isLazyIteratorObject(iterator) {\n    return (typeof iterator.next === 'function');\n}\nexport async function evaluateDataset(\n// Type `model` as `any` here to avoid circular dependency w/\n// training.ts.\n// tslint:disable-next-line:no-any\nmodel, dataset, args) {\n    args = args || {};\n    const hasBatches = args.batches != null;\n    const f = model.testFunction;\n    let outs = [];\n    if (args.verbose > 0) {\n        throw new NotImplementedError('Verbose mode is not implemented yet.');\n    }\n    tfc.util.assert(!hasBatches || (args.batches > 0 && Number.isInteger(args.batches)), () => 'Test loop expects `batches` to be a positive integer, but ' +\n        `received ${JSON.stringify(args.batches)}`);\n    const dataIterator = isLazyIteratorObject(dataset) ?\n        dataset :\n        await dataset.iterator();\n    // Keeps track of number of examples used in this evaluation.\n    let numExamples = 0;\n    let batch = 0;\n    while (hasBatches ? batch < args.batches : true) {\n        const iteratorOut = await dataIterator.next();\n        outs = tfc.tidy(() => {\n            if (iteratorOut.value) {\n                // TODO(cais): Once real dataset is available, use\n                //   `map(x => standardizeDataIteratorOutput(model, x).map(f)`.\n                const { xs, ys } = standardizeDataIteratorOutput(model, iteratorOut.value);\n                const xsAndYs = xs.concat(ys);\n                const batchOuts = tfc.tidy(() => f(xsAndYs));\n                tfc.dispose(xsAndYs);\n                if (batch === 0) {\n                    for (let i = 0; i < batchOuts.length; ++i) {\n                        outs.push(scalar(0));\n                    }\n                }\n                const batchSize = xsAndYs[0].shape[0];\n                for (let i = 0; i < batchOuts.length; ++i) {\n                    const batchOut = batchOuts[i];\n                    const oldScalar = outs[i];\n                    outs[i] =\n                        tfc.tidy(() => tfc.add(outs[i], tfc.mul(batchSize, batchOut)));\n                    if (batch > 0) {\n                        tfc.dispose(oldScalar);\n                    }\n                }\n                tfc.dispose(batchOuts);\n                numExamples += batchSize;\n                ++batch;\n            }\n            return outs;\n        });\n        if (iteratorOut.done) {\n            if (hasBatches) {\n                console.warn('Your dataset iterator ran out of data during evaluateDataset(). ' +\n                    'Interrupting evalution. Make sure that your ' +\n                    'dataset can generate at least `batches` ' +\n                    `batches (in this case, ${args.batches} batches). ` +\n                    'You may need to use the repeat() function when building ' +\n                    'your dataset.');\n            }\n            break;\n        }\n    }\n    for (let i = 0; i < outs.length; ++i) {\n        const oldScalar = outs[i];\n        outs[i] = tfc.div(outs[i], numExamples);\n        tfc.dispose(oldScalar);\n    }\n    return singletonOrArray(outs);\n}\n//# sourceMappingURL=training_dataset.js.map"]},"metadata":{},"sourceType":"module"}