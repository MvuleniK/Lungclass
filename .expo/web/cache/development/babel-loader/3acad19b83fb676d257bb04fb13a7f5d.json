{"ast":null,"code":"import _extends from \"@babel/runtime/helpers/extends\";\n\n/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from \"./util\";\nexport function getFilteredNodesXToY(tape, xs, y) {\n  var tensorsFromX = {};\n  var nodesFromX = {};\n\n  for (var i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n\n  for (var _i = 0; _i < tape.length; _i++) {\n    var node = tape[_i];\n    var nodeInputs = node.inputs;\n\n    for (var inputName in nodeInputs) {\n      var input = nodeInputs[inputName];\n      var anyInputFromX = false;\n\n      for (var j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(function (output) {\n            return tensorsFromX[output.id] = true;\n          });\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n\n  var tensorsLeadToY = {};\n  tensorsLeadToY[y.id] = true;\n  var nodesToY = {};\n\n  for (var _i2 = tape.length - 1; _i2 >= 0; _i2--) {\n    var _node = tape[_i2];\n    var _nodeInputs = _node.inputs;\n\n    for (var _j = 0; _j < _node.outputs.length; _j++) {\n      if (tensorsLeadToY[_node.outputs[_j].id]) {\n        for (var _inputName in _nodeInputs) {\n          tensorsLeadToY[_nodeInputs[_inputName].id] = true;\n          nodesToY[_node.id] = true;\n        }\n\n        break;\n      }\n    }\n  }\n\n  var filteredTape = [];\n\n  for (var _i3 = 0; _i3 < tape.length; _i3++) {\n    var _node2 = tape[_i3];\n\n    if (nodesFromX[_node2.id] && nodesToY[_node2.id]) {\n      var prunedInputs = {};\n\n      for (var _inputName2 in _node2.inputs) {\n        var nodeInput = _node2.inputs[_inputName2];\n\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[_inputName2] = nodeInput;\n        }\n      }\n\n      var prunedNode = _extends({}, _node2);\n\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = _node2.outputs;\n      filteredTape.push(prunedNode);\n    }\n  }\n\n  return filteredTape;\n}\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n  var _loop = function _loop(i) {\n    var node = filteredTape[i];\n    var dys = [];\n    node.outputs.forEach(function (o) {\n      var gradTensor = tensorAccumulatedGradientMap[o.id];\n\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        dys.push(null);\n      }\n    });\n\n    if (node.gradient == null) {\n      throw new Error(\"Cannot compute gradient: gradient function not found \" + (\"for \" + node.kernelName + \".\"));\n    }\n\n    var inputGradients = node.gradient(dys);\n\n    var _loop2 = function _loop2(inputName) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(\"Cannot backprop through input \" + inputName + \". \" + (\"Available gradients found: \" + Object.keys(inputGradients) + \".\"));\n      }\n\n      var dx = tidy(function () {\n        return inputGradients[inputName]();\n      });\n\n      if (dx.dtype !== 'float32') {\n        throw new Error(\"Error in gradient for op \" + node.kernelName + \". The gradient of input \" + (inputName + \" must have 'float32' dtype, but has '\" + dx.dtype + \"'\"));\n      }\n\n      var x = node.inputs[inputName];\n\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(\"Error in gradient for op \" + node.kernelName + \". The gradient of input \" + (\"'\" + inputName + \"' has shape '\" + dx.shape + \"', which does not match \") + (\"the shape of the input '\" + x.shape + \"'\"));\n      }\n\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        var curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    };\n\n    for (var inputName in node.inputs) {\n      _loop2(inputName);\n    }\n  };\n\n  for (var i = filteredTape.length - 1; i >= 0; i--) {\n    _loop(i);\n  }\n}","map":{"version":3,"sources":["../src/tape.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAIH,OAAO,KAAK,IAAZ;AAwBA,OAAM,SAAU,oBAAV,CACF,IADE,EACgB,EADhB,EAC8B,CAD9B,EACuC;EAG3C,IAAM,YAAY,GAAkC,EAApD;EACA,IAAM,UAAU,GAAgC,EAAhD;;EACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,EAAE,CAAC,MAAvB,EAA+B,CAAC,EAAhC,EAAoC;IAClC,YAAY,CAAC,EAAE,CAAC,CAAD,CAAF,CAAM,EAAP,CAAZ,GAAyB,IAAzB;EACD;;EAED,KAAK,IAAI,EAAC,GAAG,CAAb,EAAgB,EAAC,GAAG,IAAI,CAAC,MAAzB,EAAiC,EAAC,EAAlC,EAAsC;IACpC,IAAM,IAAI,GAAG,IAAI,CAAC,EAAD,CAAjB;IACA,IAAM,UAAU,GAAG,IAAI,CAAC,MAAxB;;IACA,KAAK,IAAM,SAAX,IAAwB,UAAxB,EAAoC;MAClC,IAAM,KAAK,GAAG,UAAU,CAAC,SAAD,CAAxB;MAEA,IAAI,aAAa,GAAG,KAApB;;MACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,EAAE,CAAC,MAAvB,EAA+B,CAAC,EAAhC,EAAoC;QAClC,IAAI,YAAY,CAAC,KAAK,CAAC,EAAP,CAAhB,EAA4B;UAC1B,IAAI,CAAC,OAAL,CAAa,OAAb,CAAqB,UAAA,MAAM;YAAA,OAAI,YAAY,CAAC,MAAM,CAAC,EAAR,CAAZ,GAA0B,IAA9B;UAAA,CAA3B;UACA,aAAa,GAAG,IAAhB;UACA,UAAU,CAAC,IAAI,CAAC,EAAN,CAAV,GAAsB,IAAtB;UACA;QACD;MACF;;MAED,IAAI,aAAJ,EAAmB;QACjB;MACD;IACF;EACF;;EAGD,IAAM,cAAc,GAAkC,EAAtD;EACA,cAAc,CAAC,CAAC,CAAC,EAAH,CAAd,GAAuB,IAAvB;EACA,IAAM,QAAQ,GAAgC,EAA9C;;EAEA,KAAK,IAAI,GAAC,GAAG,IAAI,CAAC,MAAL,GAAc,CAA3B,EAA8B,GAAC,IAAI,CAAnC,EAAsC,GAAC,EAAvC,EAA2C;IACzC,IAAM,KAAI,GAAG,IAAI,CAAC,GAAD,CAAjB;IACA,IAAM,WAAU,GAAG,KAAI,CAAC,MAAxB;;IAGA,KAAK,IAAI,EAAC,GAAG,CAAb,EAAgB,EAAC,GAAG,KAAI,CAAC,OAAL,CAAa,MAAjC,EAAyC,EAAC,EAA1C,EAA8C;MAC5C,IAAI,cAAc,CAAC,KAAI,CAAC,OAAL,CAAa,EAAb,EAAgB,EAAjB,CAAlB,EAAwC;QACtC,KAAK,IAAM,UAAX,IAAwB,WAAxB,EAAoC;UAClC,cAAc,CAAC,WAAU,CAAC,UAAD,CAAV,CAAsB,EAAvB,CAAd,GAA2C,IAA3C;UACA,QAAQ,CAAC,KAAI,CAAC,EAAN,CAAR,GAAoB,IAApB;QACD;;QACD;MACD;IACF;EACF;;EAGD,IAAM,YAAY,GAAe,EAAjC;;EACA,KAAK,IAAI,GAAC,GAAG,CAAb,EAAgB,GAAC,GAAG,IAAI,CAAC,MAAzB,EAAiC,GAAC,EAAlC,EAAsC;IACpC,IAAM,MAAI,GAAG,IAAI,CAAC,GAAD,CAAjB;;IAEA,IAAI,UAAU,CAAC,MAAI,CAAC,EAAN,CAAV,IAAuB,QAAQ,CAAC,MAAI,CAAC,EAAN,CAAnC,EAA8C;MAE5C,IAAM,YAAY,GAAkC,EAApD;;MACA,KAAK,IAAM,WAAX,IAAwB,MAAI,CAAC,MAA7B,EAAqC;QACnC,IAAM,SAAS,GAAG,MAAI,CAAC,MAAL,CAAY,WAAZ,CAAlB;;QACA,IAAI,YAAY,CAAC,SAAS,CAAC,EAAX,CAAhB,EAAgC;UAC9B,YAAY,CAAC,WAAD,CAAZ,GAA0B,SAA1B;QACD;MACF;;MAGD,IAAM,UAAU,GAAG,SAAc,EAAd,EAAkB,MAAlB,CAAnB;;MACA,UAAU,CAAC,MAAX,GAAoB,YAApB;MACA,UAAU,CAAC,OAAX,GAAqB,MAAI,CAAC,OAA1B;MAEA,YAAY,CAAC,IAAb,CAAkB,UAAlB;IACD;EACF;;EAED,OAAO,YAAP;AACD;AASD,OAAM,SAAU,sBAAV,CACF,4BADE,EAEF,YAFE,EAEwB,IAFxB,EAGF,GAHE,EAGmC;EAAA,2BAE9B,CAF8B;IAGrC,IAAM,IAAI,GAAG,YAAY,CAAC,CAAD,CAAzB;IAEA,IAAM,GAAG,GAAa,EAAtB;IACA,IAAI,CAAC,OAAL,CAAa,OAAb,CAAqB,UAAA,CAAC,EAAG;MACvB,IAAM,UAAU,GAAG,4BAA4B,CAAC,CAAC,CAAC,EAAH,CAA/C;;MACA,IAAI,UAAU,IAAI,IAAlB,EAAwB;QACtB,GAAG,CAAC,IAAJ,CAAS,UAAT;MACD,CAFD,MAEO;QAGL,GAAG,CAAC,IAAJ,CAAS,IAAT;MACD;IACF,CATD;;IAWA,IAAI,IAAI,CAAC,QAAL,IAAiB,IAArB,EAA2B;MACzB,MAAM,IAAI,KAAJ,CACF,oEACO,IAAI,CAAC,UADZ,OADE,CAAN;IAGD;;IAGD,IAAM,cAAc,GAAG,IAAI,CAAC,QAAL,CAAc,GAAd,CAAvB;;IAxBqC,6BA0B1B,SA1B0B;MA2BnC,IAAI,EAAE,SAAS,IAAI,cAAf,CAAJ,EAAoC;QAClC,MAAM,IAAI,KAAJ,CACF,mCAAiC,SAAjC,2CAC8B,MAAM,CAAC,IAAP,CAAY,cAAZ,CAD9B,OADE,CAAN;MAGD;;MAGD,IAAM,EAAE,GAAG,IAAI,CAAC;QAAA,OAAM,cAAc,CAAC,SAAD,CAAd,EAAN;MAAA,CAAD,CAAf;;MACA,IAAI,EAAE,CAAC,KAAH,KAAa,SAAjB,EAA4B;QAC1B,MAAM,IAAI,KAAJ,CACF,8BACI,IAAI,CAAC,UADT,iCAEG,SAFH,6CAEoD,EAAE,CAAC,KAFvD,OADE,CAAN;MAID;;MACD,IAAM,CAAC,GAAG,IAAI,CAAC,MAAL,CAAY,SAAZ,CAAV;;MACA,IAAI,CAAC,IAAI,CAAC,WAAL,CAAiB,EAAE,CAAC,KAApB,EAA2B,CAAC,CAAC,KAA7B,CAAL,EAA0C;QACxC,MAAM,IAAI,KAAJ,CACF,8BACI,IAAI,CAAC,UADT,uCAEI,SAFJ,qBAE6B,EAAE,CAAC,KAFhC,+DAG2B,CAAC,CAAC,KAH7B,OADE,CAAN;MAKD;;MAED,IAAI,4BAA4B,CAAC,CAAC,CAAC,EAAH,CAA5B,IAAsC,IAA1C,EAAgD;QAC9C,4BAA4B,CAAC,CAAC,CAAC,EAAH,CAA5B,GAAqC,EAArC;MACD,CAFD,MAEO;QACL,IAAM,WAAW,GAAG,4BAA4B,CAAC,CAAC,CAAC,EAAH,CAAhD;QACA,4BAA4B,CAAC,CAAC,CAAC,EAAH,CAA5B,GAAqC,GAAG,CAAC,WAAD,EAAc,EAAd,CAAxC;QACA,WAAW,CAAC,OAAZ;MACD;IAxDkC;;IA0BrC,KAAK,IAAM,SAAX,IAAwB,IAAI,CAAC,MAA7B,EAAqC;MAAA,OAA1B,SAA0B;IA+BpC;EAzDoC;;EAEvC,KAAK,IAAI,CAAC,GAAG,YAAY,CAAC,MAAb,GAAsB,CAAnC,EAAsC,CAAC,IAAI,CAA3C,EAA8C,CAAC,EAA/C,EAAmD;IAAA,MAA1C,CAA0C;EAwDlD;AACF","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2017 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as util from './util';\n/**\n * Computes a list of TapeNodes that connect x to y, filtering everything else\n * out and preserving the order of the original tape elements.\n *\n * @param tape The tape elements to filter.\n * @param xs The input Tensors.\n * @param y The output Tensor.\n */\nexport function getFilteredNodesXToY(tape, xs, y) {\n    // Forward pass to compute all the nodes and Tensors that are transitively a\n    // function of x.\n    const tensorsFromX = {};\n    const nodesFromX = {};\n    for (let i = 0; i < xs.length; i++) {\n        tensorsFromX[xs[i].id] = true;\n    }\n    for (let i = 0; i < tape.length; i++) {\n        const node = tape[i];\n        const nodeInputs = node.inputs;\n        for (const inputName in nodeInputs) {\n            const input = nodeInputs[inputName];\n            let anyInputFromX = false;\n            for (let j = 0; j < xs.length; j++) {\n                if (tensorsFromX[input.id]) {\n                    node.outputs.forEach(output => tensorsFromX[output.id] = true);\n                    anyInputFromX = true;\n                    nodesFromX[node.id] = true;\n                    break;\n                }\n            }\n            if (anyInputFromX) {\n                break;\n            }\n        }\n    }\n    // Backward pass to find all of the nodes and Tensors that lead to y.\n    const tensorsLeadToY = {};\n    tensorsLeadToY[y.id] = true;\n    const nodesToY = {};\n    for (let i = tape.length - 1; i >= 0; i--) {\n        const node = tape[i];\n        const nodeInputs = node.inputs;\n        // If any of the outputs lead to y, mark all of the inputs as leading to y.\n        for (let j = 0; j < node.outputs.length; j++) {\n            if (tensorsLeadToY[node.outputs[j].id]) {\n                for (const inputName in nodeInputs) {\n                    tensorsLeadToY[nodeInputs[inputName].id] = true;\n                    nodesToY[node.id] = true;\n                }\n                break;\n            }\n        }\n    }\n    // Return the paths that come from x and lead to y.\n    const filteredTape = [];\n    for (let i = 0; i < tape.length; i++) {\n        const node = tape[i];\n        if (nodesFromX[node.id] && nodesToY[node.id]) {\n            // Prune the inputs from the node that aren't a function of x.\n            const prunedInputs = {};\n            for (const inputName in node.inputs) {\n                const nodeInput = node.inputs[inputName];\n                if (tensorsFromX[nodeInput.id]) {\n                    prunedInputs[inputName] = nodeInput;\n                }\n            }\n            // Copy the node and overwrite inputsAndArgs to the pruned version.\n            const prunedNode = Object.assign({}, node);\n            prunedNode.inputs = prunedInputs;\n            prunedNode.outputs = node.outputs;\n            filteredTape.push(prunedNode);\n        }\n    }\n    return filteredTape;\n}\n/**\n * Backpropagate gradients through the filtered TapeNodes.\n *\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\n * is mutated by this method.\n * @param filteredTape The filtered TapeNodes to backprop through.\n */\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n    // Walk the tape backward and keep a map of Tensor to its gradient.\n    for (let i = filteredTape.length - 1; i >= 0; i--) {\n        const node = filteredTape[i];\n        const dys = [];\n        node.outputs.forEach(o => {\n            const gradTensor = tensorAccumulatedGradientMap[o.id];\n            if (gradTensor != null) {\n                dys.push(gradTensor);\n            }\n            else {\n                // This particular output is not in the back-propagation subgraph, so it\n                // does not affect the final output, thus we put null for its dy.\n                dys.push(null);\n            }\n        });\n        if (node.gradient == null) {\n            throw new Error(`Cannot compute gradient: gradient function not found ` +\n                `for ${node.kernelName}.`);\n        }\n        // Backprop dy through this node and accumulate gradients over the inputs.\n        const inputGradients = node.gradient(dys);\n        for (const inputName in node.inputs) {\n            if (!(inputName in inputGradients)) {\n                throw new Error(`Cannot backprop through input ${inputName}. ` +\n                    `Available gradients found: ${Object.keys(inputGradients)}.`);\n            }\n            // Call the gradient function.\n            const dx = tidy(() => inputGradients[inputName]());\n            if (dx.dtype !== 'float32') {\n                throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +\n                    `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n            }\n            const x = node.inputs[inputName];\n            if (!util.arraysEqual(dx.shape, x.shape)) {\n                throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` +\n                    `'${inputName}' has shape '${dx.shape}', which does not match ` +\n                    `the shape of the input '${x.shape}'`);\n            }\n            if (tensorAccumulatedGradientMap[x.id] == null) {\n                tensorAccumulatedGradientMap[x.id] = dx;\n            }\n            else {\n                const curGradient = tensorAccumulatedGradientMap[x.id];\n                tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n                curGradient.dispose();\n            }\n        }\n    }\n}\n//# sourceMappingURL=tape.js.map"]},"metadata":{},"sourceType":"module"}