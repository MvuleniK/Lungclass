{"ast":null,"code":"import _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\n\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from \"../../engine\";\nimport { customGrad } from \"../../gradients\";\nimport { FusedConv2D } from \"../../kernel_names\";\nimport { makeTypesMatch } from \"../../tensor_util\";\nimport { convertToTensor } from \"../../tensor_util_env\";\nimport * as util from \"../../util\";\nimport { add } from \"../add\";\nimport * as broadcast_util from \"../broadcast_util\";\nimport { conv2d as unfusedConv2d } from \"../conv2d\";\nimport { conv2DBackpropFilter } from \"../conv2d_backprop_filter\";\nimport { conv2DBackpropInput } from \"../conv2d_backprop_input\";\nimport * as conv_util from \"../conv_util\";\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from \"../fused_util\";\nimport { op } from \"../operation\";\nimport { reshape } from \"../reshape\";\n\nfunction fusedConv2d_(_ref) {\n  var x = _ref.x,\n      filter = _ref.filter,\n      strides = _ref.strides,\n      pad = _ref.pad,\n      _ref$dataFormat = _ref.dataFormat,\n      dataFormat = _ref$dataFormat === void 0 ? 'NHWC' : _ref$dataFormat,\n      _ref$dilations = _ref.dilations,\n      dilations = _ref$dilations === void 0 ? [1, 1] : _ref$dilations,\n      dimRoundingMode = _ref.dimRoundingMode,\n      bias = _ref.bias,\n      _ref$activation = _ref.activation,\n      activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n      preluActivationWeights = _ref.preluActivationWeights,\n      leakyreluAlpha = _ref.leakyreluAlpha;\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    var result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  var $x = convertToTensor(x, 'x', 'conv2d');\n  var $filter = convertToTensor(filter, 'filter', 'conv2d');\n  var x4D = $x;\n  var reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, function () {\n    return \"Error in fused conv2d: input must be rank 4, but got rank \" + (x4D.rank + \".\");\n  });\n  util.assert($filter.rank === 4, function () {\n    return \"Error in fused conv2d: filter must be rank 4, but got rank \" + ($filter.rank + \".\");\n  });\n\n  if (dimRoundingMode != null) {\n    util.assert(util.isInt(pad), function () {\n      return \"Error in fused conv2d: pad must be an integer when using, \" + (\"dimRoundingMode \" + dimRoundingMode + \" but got pad \" + pad + \".\");\n    });\n  }\n\n  util.assert(x4D.shape[3] === $filter.shape[2], function () {\n    return \"Error in conv2d: depth of input (\" + x4D.shape[3] + \") must match \" + (\"input depth for filter \" + $filter.shape[2] + \".\");\n  });\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), function () {\n    return 'Error in conv2D: Either strides or dilations must be 1. ' + (\"Got strides \" + strides + \" and dilations '\" + dilations + \"'\");\n  });\n  util.assert(dataFormat === 'NHWC', function () {\n    return \"Error in conv2d: got dataFormat of \" + dataFormat + \" but only NHWC is currently supported.\";\n  });\n  var convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  var $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n\n    var _makeTypesMatch = makeTypesMatch($bias, $x);\n\n    var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 1);\n\n    $bias = _makeTypesMatch2[0];\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  var $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  var grad = function grad(dy, saved) {\n    var _saved = _slicedToArray(saved, 4),\n        $filter = _saved[0],\n        x4D = _saved[1],\n        y = _saved[2],\n        $bias = _saved[3];\n\n    var dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), function () {\n      return 'Error in gradient of fused conv2D: ' + \"dilation rates greater than 1 \" + (\"are not yet supported in gradients. Got dilations '\" + dilations + \"'\");\n    });\n    var xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    var filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    var der = [xDer, filterDer];\n\n    if ($bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n\n    return der;\n  };\n\n  var inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    strides: strides,\n    pad: pad,\n    dataFormat: dataFormat,\n    dilations: dilations,\n    dimRoundingMode: dimRoundingMode,\n    activation: activation,\n    leakyreluAlpha: leakyreluAlpha\n  };\n\n  if (bias == null) {\n    var customOp = customGrad(function (x4D, filter, save) {\n      var res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    var customOpWithBias = customGrad(function (x4D, filter, bias, save) {\n      var res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport var conv2d = op({\n  fusedConv2d_: fusedConv2d_\n});","map":{"version":3,"sources":["../../../src/ops/fused/conv2d.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR;AACA,SAAQ,UAAR;AACA,SAAQ,WAAR;AAIA,SAAQ,cAAR;AACA,SAAQ,eAAR;AAEA,OAAO,KAAK,IAAZ;AACA,SAAQ,GAAR;AACA,OAAO,KAAK,cAAZ;AACA,SAAQ,MAAM,IAAI,aAAlB;AACA,SAAQ,oBAAR;AACA,SAAQ,mBAAR;AACA,OAAO,KAAK,SAAZ;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,EAA+C,oBAA/C,EAAqE,UAArE;AACA,SAAQ,EAAR;AACA,SAAQ,OAAR;;AA2DA,SAAS,YAAT,OAwBC;EAAA,IAvBC,CAuBD,QAvBC,CAuBD;EAAA,IAtBC,MAsBD,QAtBC,MAsBD;EAAA,IArBC,OAqBD,QArBC,OAqBD;EAAA,IApBC,GAoBD,QApBC,GAoBD;EAAA,2BAnBC,UAmBD;EAAA,IAnBC,UAmBD,gCAnBc,MAmBd;EAAA,0BAlBC,SAkBD;EAAA,IAlBC,SAkBD,+BAlBa,CAAC,CAAD,EAAI,CAAJ,CAkBb;EAAA,IAjBC,eAiBD,QAjBC,eAiBD;EAAA,IAhBC,IAgBD,QAhBC,IAgBD;EAAA,2BAfC,UAeD;EAAA,IAfC,UAeD,gCAfc,QAed;EAAA,IAdC,sBAcD,QAdC,sBAcD;EAAA,IAbC,cAaD,QAbC,cAaD;EACC,UAAU,GAAG,UAAU,IAAI,QAA3B;;EAEA,IAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAChE,IAAI,MAAM,GAAG,aAAa,CACtB,CADsB,EACnB,MADmB,EACX,OADW,EACF,GADE,EACG,UADH,EACe,SADf,EAC0B,eAD1B,CAA1B;;IAEA,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;IACD;;IAED,OAAO,eAAe,CACX,MADW,EACH,UADG,EACS,sBADT,EACiC,cADjC,CAAtB;EAED;;EAED,IAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,QAAT,CAA1B;EACA,IAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,QAAnB,CAA/B;EAEA,IAAI,GAAG,GAAG,EAAV;EACA,IAAI,YAAY,GAAG,KAAnB;;EAEA,IAAI,EAAE,CAAC,IAAH,KAAY,CAAhB,EAAmB;IACjB,YAAY,GAAG,IAAf;IACA,GAAG,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,EAAE,CAAC,KAAH,CAAS,CAAT,CAAJ,EAAiB,EAAE,CAAC,KAAH,CAAS,CAAT,CAAjB,EAA8B,EAAE,CAAC,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;EACD;;EACD,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,IAAJ,KAAa,CADjB,EAEI;IAAA,OAAM,gEACC,GAAG,CAAC,IADL,OAAN;EAAA,CAFJ;EAIA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,IAAR,KAAiB,CADrB,EAEI;IAAA,OAAM,iEACC,OAAO,CAAC,IADT,OAAN;EAAA,CAFJ;;EAIA,IAAI,eAAe,IAAI,IAAvB,EAA6B;IAC3B,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,KAAL,CAAW,GAAX,CADJ,EAEI;MAAA,OAAM,qFACiB,eADjB,qBACgD,GADhD,OAAN;IAAA,CAFJ;EAID;;EAED,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,CAAU,CAAV,MAAiB,OAAO,CAAC,KAAR,CAAc,CAAd,CADrB,EAEI;IAAA,OAAM,sCAAoC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAApC,kDACwB,OAAO,CAAC,KAAR,CAAc,CAAd,CADxB,OAAN;EAAA,CAFJ;EAIA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,8BAAV,CAAyC,OAAzC,EAAkD,SAAlD,CADJ,EAEI;IAAA,OAAM,+EACa,OADb,wBACuC,SADvC,OAAN;EAAA,CAFJ;EAIA,IAAI,CAAC,MAAL,CACI,UAAU,KAAK,MADnB,EAEI;IAAA,+CACI,UADJ;EAAA,CAFJ;EAKA,IAAM,QAAQ,GAAG,SAAS,CAAC,iBAAV,CACb,GAAG,CAAC,KADS,EACF,OAAO,CAAC,KADN,EACa,OADb,EACsB,SADtB,EACiC,GADjC,EACsC,eADtC,CAAjB;EAGA,IAAI,KAAJ;;EACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;;IADgB,sBAEN,cAAc,CAAC,KAAD,EAAQ,EAAR,CAFR;;IAAA;;IAEf,KAFe;IAIhB,cAAc,CAAC,0BAAf,CAA0C,QAAQ,CAAC,QAAnD,EAA6D,KAAK,CAAC,KAAnE;EACD;;EAED,IAAI,uBAAJ;;EACA,IAAI,sBAAsB,IAAI,IAA9B,EAAoC;IAClC,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;EAED;;EAED,IAAM,IAAI,GAAG,SAAP,IAAO,CAAC,EAAD,EAAe,KAAf,EAAkC;IAC7C,4BACI,KADJ;IAAA,IAAO,OAAP;IAAA,IAAgB,GAAhB;IAAA,IAAqB,CAArB;IAAA,IAAwB,KAAxB;;IAGA,IAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;IAEA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,iBAAV,CAA4B,SAA5B,CADJ,EAEI;MAAA,OAAM,oIAEoD,SAFpD,OAAN;IAAA,CAFJ;IAMA,IAAM,IAAI,GACN,mBAAmB,CAAC,GAAG,CAAC,KAAL,EAAY,YAAZ,EAA0B,OAA1B,EAAmC,OAAnC,EAA4C,GAA5C,CADvB;IAEA,IAAM,SAAS,GACX,oBAAoB,CAAC,GAAD,EAAM,YAAN,EAAoB,OAAO,CAAC,KAA5B,EAAmC,OAAnC,EAA4C,GAA5C,CADxB;IAEA,IAAM,GAAG,GAAa,CAAC,IAAD,EAAO,SAAP,CAAtB;;IAEA,IAAI,KAAK,IAAI,IAAb,EAAmB;MACjB,IAAM,OAAO,GAAG,oBAAoB,CAAC,KAAD,EAAQ,YAAR,CAApC;MACA,GAAG,CAAC,IAAJ,CAAS,OAAT;IACD;;IACD,OAAO,GAAP;EACD,CAvBD;;EAyBA,IAAM,MAAM,GAAsB;IAChC,CAAC,EAAE,GAD6B;IAEhC,MAAM,EAAE,OAFwB;IAGhC,IAAI,EAAE,KAH0B;IAIhC,sBAAsB,EAAE;EAJQ,CAAlC;EAOA,IAAM,KAAK,GAAqB;IAC9B,OAAO,EAAP,OAD8B;IAE9B,GAAG,EAAH,GAF8B;IAG9B,UAAU,EAAV,UAH8B;IAI9B,SAAS,EAAT,SAJ8B;IAK9B,eAAe,EAAf,eAL8B;IAM9B,UAAU,EAAV,UAN8B;IAO9B,cAAc,EAAd;EAP8B,CAAhC;;EAYA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,IAAM,QAAQ,GACV,UAAU,CAAC,UAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,EAAwD;MACjE,IAAI,GAAG,GAEH,MAAM,CAAC,SAAP,CACI,WADJ,EACiB,MADjB,EAEI,KAFJ,CAFJ;MAMA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,CAAD,CAAJ;;MAEA,IAAI,YAAJ,EAAkB;QAEhB,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MAED;;MAED,OAAO;QAAC,KAAK,EAAE,GAAR;QAAa,QAAQ,EAAE;MAAvB,CAAP;IACD,CAhBS,CADd;IAkBA,OAAO,QAAQ,CAAC,GAAD,EAAM,OAAN,CAAf;EACD,CApBD,MAoBO;IACL,IAAM,gBAAgB,GAAG,UAAU,CAC/B,UAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,EAAgD,IAAhD,EAAsE;MACpE,IAAI,GAAG,GAAsB,MAAM,CAAC,SAAP,CACzB,WADyB,EACZ,MADY,EAEzB,KAFyB,CAA7B;MAIA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,EAAmB,IAAnB,CAAD,CAAJ;;MAEA,IAAI,YAAJ,EAAkB;QAEhB,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;MAED;;MAED,OAAO;QAAC,KAAK,EAAE,GAAR;QAAa,QAAQ,EAAE;MAAvB,CAAP;IACD,CAf8B,CAAnC;IAiBA,OAAO,gBAAgB,CAAC,GAAD,EAAM,OAAN,EAAe,KAAf,CAAvB;EACD;AACF;;AACD,OAAO,IAAM,MAAM,GAAG,EAAE,CAAC;EAAC,YAAY,EAAZ;AAAD,CAAD,CAAjB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes a 2D convolution over the input x, optionally fused with adding a\n * bias and applying an activation.\n *\n * ```js\n * const inputDepth = 2;\n * const inShape = [2, 2, 2, inputDepth];\n * const outputDepth = 2;\n * const fSize = 1;\n * const pad = 0;\n * const strides = 1;\n *\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\n * 16], inShape);\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\n * outputDepth]);\n *\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\n * ```\n *\n * @param obj An object with the following properties:\n * @param x The input tensor, of rank 4 or rank 3, of shape\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\n * assumed.\n * @param filter The filter, rank 4, of shape\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\n * @param strides The strides of the convolution: `[strideHeight,\n * strideWidth]`.\n * @param pad The type of padding algorithm.\n *   - `same` and stride 1: output will be of same size as input,\n *       regardless of filter size.\n *   - `valid` output will be smaller than input if filter is larger\n *       than 1x1.\n *   - For more info, see this guide:\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\n *     \"NHWC\". Specify the data format of the input and output data. With the\n *     default format \"NHWC\", the data is stored in the order of: [batch,\n *     height, width, channels]. Only \"NHWC\" is currently supported.\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\n *     in which we sample input values across the height and width dimensions\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\n *     1, then all values of `strides` must be 1.\n * @param dimRoundingMode A string from: 'ceil', 'round', 'floor'. If none is\n *     provided, it will default to truncate.\n * @param bias Tensor to be added to the result.\n * @param activation Name of activation kernel (defaults to `linear`) to be\n *     applied\n *      after biasAdd.\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\n *     of a `prelu` activation, typically the same shape as `x`.\n * @param leakyreluAlpha Optional. Alpha to be applied as part of a `leakyrelu`\n *     activation.\n */\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha }) {\n    activation = activation || 'linear';\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    const $x = convertToTensor(x, 'x', 'conv2d');\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\n    let x4D = $x;\n    let reshapedTo4D = false;\n    if ($x.rank === 3) {\n        reshapedTo4D = true;\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n    }\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\n        `${x4D.rank}.`);\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\n        `${$filter.rank}.`);\n    if (dimRoundingMode != null) {\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n    }\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\n        `input depth for filter ${$filter.shape[2]}.`);\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\n        `Got strides ${strides} and dilations '${dilations}'`);\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n        [$bias] = makeTypesMatch($bias, $x);\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n    }\n    const grad = (dy, saved) => {\n        const [$filter, x4D, y, $bias] = saved;\n        const dyActivation = getFusedDyActivation(dy, y, activation);\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\n            `dilation rates greater than 1 ` +\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n        const der = [xDer, filterDer];\n        if ($bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            der.push(biasDer);\n        }\n        return der;\n    };\n    const inputs = {\n        x: x4D,\n        filter: $filter,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = {\n        strides,\n        pad,\n        dataFormat,\n        dilations,\n        dimRoundingMode,\n        activation,\n        leakyreluAlpha\n    };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((x4D, filter, save) => {\n            let res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOp(x4D, $filter);\n    }\n    else {\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n            let res = ENGINE.runKernel(FusedConv2D, inputs, attrs);\n            save([filter, x4D, res, bias]);\n            if (reshapedTo4D) {\n                // tslint:disable-next-line: no-unnecessary-type-assertion\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n            }\n            return { value: res, gradFunc: grad };\n        });\n        return customOpWithBias(x4D, $filter, $bias);\n    }\n}\nexport const conv2d = op({ fusedConv2d_ });\n//# sourceMappingURL=conv2d.js.map"]},"metadata":{},"sourceType":"module"}