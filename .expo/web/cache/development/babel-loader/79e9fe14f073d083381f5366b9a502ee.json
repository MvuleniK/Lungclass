{"ast":null,"code":"import _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\nimport _regeneratorRuntime from \"@babel/runtime/regenerator\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from \"../engine\";\nimport { keep, tidy } from \"../globals\";\nimport { add } from \"../ops/add\";\nimport { mul } from \"../ops/mul\";\nimport { scalar } from \"../ops/scalar\";\nimport { registerClass } from \"../serialization\";\nimport { Optimizer } from \"./optimizer\";\nexport var SGDOptimizer = function (_Optimizer) {\n  _inherits(SGDOptimizer, _Optimizer);\n\n  var _super = _createSuper(SGDOptimizer);\n\n  function SGDOptimizer(learningRate) {\n    var _this;\n\n    _classCallCheck(this, SGDOptimizer);\n\n    _this = _super.call(this);\n    _this.learningRate = learningRate;\n\n    _this.setLearningRate(learningRate);\n\n    return _this;\n  }\n\n  _createClass(SGDOptimizer, [{\n    key: \"applyGradients\",\n    value: function applyGradients(variableGradients) {\n      var _this2 = this;\n\n      var varNames = Array.isArray(variableGradients) ? variableGradients.map(function (v) {\n        return v.name;\n      }) : Object.keys(variableGradients);\n      varNames.forEach(function (name, i) {\n        var gradient = Array.isArray(variableGradients) ? variableGradients[i].tensor : variableGradients[name];\n\n        if (gradient == null) {\n          return;\n        }\n\n        var value = ENGINE.registeredVariables[name];\n        tidy(function () {\n          var newValue = add(mul(_this2.c, gradient), value);\n          value.assign(newValue);\n        });\n      });\n      this.incrementIterations();\n    }\n  }, {\n    key: \"setLearningRate\",\n    value: function setLearningRate(learningRate) {\n      this.learningRate = learningRate;\n\n      if (this.c != null) {\n        this.c.dispose();\n      }\n\n      this.c = keep(scalar(-learningRate));\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.c.dispose();\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return _regeneratorRuntime.async(function getWeights$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              _context.next = 2;\n              return _regeneratorRuntime.awrap(this.saveIterations());\n\n            case 2:\n              _context.t0 = _context.sent;\n              return _context.abrupt(\"return\", [_context.t0]);\n\n            case 4:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weightValues) {\n      return _regeneratorRuntime.async(function setWeights$(_context2) {\n        while (1) {\n          switch (_context2.prev = _context2.next) {\n            case 0:\n              _context2.next = 2;\n              return _regeneratorRuntime.awrap(this.extractIterations(weightValues));\n\n            case 2:\n              weightValues = _context2.sent;\n\n              if (!(weightValues.length !== 0)) {\n                _context2.next = 5;\n                break;\n              }\n\n              throw new Error('SGD optimizer does not have settable weights.');\n\n            case 5:\n            case \"end\":\n              return _context2.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      return {\n        'learningRate': this.learningRate\n      };\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      return new cls(config['learningRate']);\n    }\n  }]);\n\n  return SGDOptimizer;\n}(Optimizer);\nSGDOptimizer.className = 'SGD';\nregisterClass(SGDOptimizer);","map":{"version":3,"sources":["../../src/optimizers/sgd_optimizer.ts"],"names":[],"mappings":";;;;;;;;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR;AACA,SAAQ,IAAR,EAAc,IAAd;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,MAAR;AACA,SAAoB,aAApB;AAIA,SAAQ,SAAR;AAGA,WAAa,YAAb;EAAA;;EAAA;;EAKE,sBAAsB,YAAtB,EAA0C;IAAA;;IAAA;;IACxC;IADoB,MAAA,YAAA,GAAA,YAAA;;IAEpB,MAAK,eAAL,CAAqB,YAArB;;IAFwC;EAGzC;;EARH;IAAA;IAAA,OAUE,wBAAe,iBAAf,EAA8D;MAAA;;MAC5D,IAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,GAAlB,CAAsB,UAAA,CAAC;QAAA,OAAI,CAAC,CAAC,IAAN;MAAA,CAAvB,CADa,GAEb,MAAM,CAAC,IAAP,CAAY,iBAAZ,CAFJ;MAGA,QAAQ,CAAC,OAAT,CAAiB,UAAC,IAAD,EAAO,CAAP,EAAY;QAC3B,IAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,iBAAd,IACb,iBAAiB,CAAC,CAAD,CAAjB,CAAqB,MADR,GAEb,iBAAiB,CAAC,IAAD,CAFrB;;QAGA,IAAI,QAAQ,IAAI,IAAhB,EAAsB;UACpB;QACD;;QACD,IAAM,KAAK,GAAG,MAAM,CAAC,mBAAP,CAA2B,IAA3B,CAAd;QACA,IAAI,CAAC,YAAK;UACR,IAAM,QAAQ,GAAG,GAAG,CAAC,GAAG,CAAC,MAAI,CAAC,CAAN,EAAS,QAAT,CAAJ,EAAwB,KAAxB,CAApB;UACA,KAAK,CAAC,MAAN,CAAa,QAAb;QACD,CAHG,CAAJ;MAID,CAZD;MAaA,KAAK,mBAAL;IACD;EA5BH;IAAA;IAAA,OAiCE,yBAAgB,YAAhB,EAAoC;MAClC,KAAK,YAAL,GAAoB,YAApB;;MACA,IAAI,KAAK,CAAL,IAAU,IAAd,EAAoB;QAClB,KAAK,CAAL,CAAO,OAAP;MACD;;MACD,KAAK,CAAL,GAAS,IAAI,CAAC,MAAM,CAAC,CAAC,YAAF,CAAP,CAAb;IACD;EAvCH;IAAA;IAAA,OAyCE,mBAAO;MACL,KAAK,CAAL,CAAO,OAAP;IACD;EA3CH;IAAA;IAAA,OA6CE;MAAA;QAAA;UAAA;YAAA;cAAA;cAAA,iCACgB,KAAK,cAAL,EADhB;;YAAA;cAAA;cAAA;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA7CF;IAAA;IAAA,OAiDE,oBAAiB,YAAjB;MAAA;QAAA;UAAA;YAAA;cAAA;cAAA,iCACuB,KAAK,iBAAL,CAAuB,YAAvB,CADvB;;YAAA;cACE,YADF;;cAAA,MAEM,YAAY,CAAC,MAAb,KAAwB,CAF9B;gBAAA;gBAAA;cAAA;;cAAA,MAGU,IAAI,KAAJ,CAAU,+CAAV,CAHV;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAjDF;IAAA;IAAA,OAwDE,qBAAS;MACP,OAAO;QAAC,gBAAgB,KAAK;MAAtB,CAAP;IACD;EA1DH;IAAA;IAAA,OA6DE,oBACI,GADJ,EACqC,MADrC,EACuD;MACrD,OAAO,IAAI,GAAJ,CAAQ,MAAM,CAAC,cAAD,CAAd,CAAP;IACD;EAhEH;;EAAA;AAAA,EAAkC,SAAlC;AAES,YAAA,CAAA,SAAA,GAAY,KAAZ;AAgET,aAAa,CAAC,YAAD,CAAb","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../engine';\nimport { keep, tidy } from '../globals';\nimport { add } from '../ops/add';\nimport { mul } from '../ops/mul';\nimport { scalar } from '../ops/scalar';\nimport { registerClass } from '../serialization';\nimport { Optimizer } from './optimizer';\n/** @doclink Optimizer */\nexport class SGDOptimizer extends Optimizer {\n    constructor(learningRate) {\n        super();\n        this.learningRate = learningRate;\n        this.setLearningRate(learningRate);\n    }\n    applyGradients(variableGradients) {\n        const varNames = Array.isArray(variableGradients) ?\n            variableGradients.map(v => v.name) :\n            Object.keys(variableGradients);\n        varNames.forEach((name, i) => {\n            const gradient = Array.isArray(variableGradients) ?\n                variableGradients[i].tensor :\n                variableGradients[name];\n            if (gradient == null) {\n                return;\n            }\n            const value = ENGINE.registeredVariables[name];\n            tidy(() => {\n                const newValue = add(mul(this.c, gradient), value);\n                value.assign(newValue);\n            });\n        });\n        this.incrementIterations();\n    }\n    /**\n     * Sets the learning rate of the optimizer.\n     */\n    setLearningRate(learningRate) {\n        this.learningRate = learningRate;\n        if (this.c != null) {\n            this.c.dispose();\n        }\n        this.c = keep(scalar(-learningRate));\n    }\n    dispose() {\n        this.c.dispose();\n    }\n    async getWeights() {\n        return [await this.saveIterations()];\n    }\n    async setWeights(weightValues) {\n        weightValues = await this.extractIterations(weightValues);\n        if (weightValues.length !== 0) {\n            throw new Error('SGD optimizer does not have settable weights.');\n        }\n    }\n    getConfig() {\n        return { 'learningRate': this.learningRate };\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        return new cls(config['learningRate']);\n    }\n}\n/** @nocollapse */\nSGDOptimizer.className = 'SGD'; // Note: Name matters for Python compatibility.\nregisterClass(SGDOptimizer);\n//# sourceMappingURL=sgd_optimizer.js.map"]},"metadata":{},"sourceType":"module"}