{"ast":null,"code":"import _extends from \"@babel/runtime/helpers/extends\";\nimport _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\nimport _defineProperty from \"@babel/runtime/helpers/defineProperty\";\nimport _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _get from \"@babel/runtime/helpers/get\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\nfunction _createForOfIteratorHelperLoose(o, allowArrayLike) { var it = typeof Symbol !== \"undefined\" && o[Symbol.iterator] || o[\"@@iterator\"]; if (it) return (it = it.call(o)).next.bind(it); if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; return function () { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from \"../constraints\";\nimport { InputSpec, Layer } from \"../engine/topology\";\nimport { NotImplementedError, ValueError } from \"../errors\";\nimport { getInitializer, serializeInitializer } from \"../initializers\";\nimport { getRegularizer, serializeRegularizer } from \"../regularizers\";\nimport * as generic_utils from \"../utils/generic_utils\";\nimport * as math_utils from \"../utils/math_utils\";\nimport { getExactlyOneShape, getExactlyOneTensor } from \"../utils/types_utils\";\nexport function batchNormalization(x, mean, variance, beta, gamma) {\n  var epsilon = arguments.length > 5 && arguments[5] !== undefined ? arguments[5] : 1e-3;\n  var out;\n\n  if (x.rank === 2) {\n    out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 3) {\n    out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n  } else if (x.rank === 4) {\n    out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n  } else {\n    throw new NotImplementedError(\"batchNormalization is not implemented for array of rank \" + x.rank + \" \" + \"yet\");\n  }\n\n  return out;\n}\n\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n  return tidy(function () {\n    var meanAndVariance = tfc.moments(x, reductionAxes);\n    var mean = meanAndVariance.mean;\n    var variance = meanAndVariance.variance;\n    var targetShape = [];\n\n    for (var _iterator = _createForOfIteratorHelperLoose(math_utils.range(0, x.rank)), _step; !(_step = _iterator()).done;) {\n      var axis = _step.value;\n\n      if (reductionAxes.indexOf(axis) !== -1) {\n        targetShape.push(1);\n      } else {\n        targetShape.push(x.shape[axis]);\n      }\n    }\n\n    var broadcastMean = mean.reshape(targetShape);\n    var broadcastVariance = variance.reshape(targetShape);\n    var broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n    var broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n    var normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n    return [normed, mean, variance];\n  });\n}\n\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes) {\n  var epsilon = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : 1e-3;\n\n  if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n    return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  } else {\n    return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n  }\n}\nexport var BatchNormalization = function (_Layer) {\n  _inherits(BatchNormalization, _Layer);\n\n  var _super = _createSuper(BatchNormalization);\n\n  function BatchNormalization(args) {\n    var _this;\n\n    _classCallCheck(this, BatchNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.axis = args.axis == null ? -1 : args.axis;\n    _this.momentum = args.momentum == null ? 0.99 : args.momentum;\n    _this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this.center = args.center == null ? true : args.center;\n    _this.scale = args.scale == null ? true : args.scale;\n    _this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this.movingMeanInitializer = getInitializer(args.movingMeanInitializer || 'zeros');\n    _this.movingVarianceInitializer = getInitializer(args.movingVarianceInitializer || 'ones');\n    _this.betaConstraint = getConstraint(args.betaConstraint);\n    _this.gammaConstraint = getConstraint(args.gammaConstraint);\n    _this.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    return _this;\n  }\n\n  _createClass(BatchNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var axis = this.axis >= 0 ? this.axis : this.axis + inputShape.length;\n      var dim = inputShape[axis];\n\n      if (dim == null) {\n        throw new ValueError(\"Axis \" + axis + \" of input tensor should have a defined dimension but \" + \"the layer received an input with shape \" + (JSON.stringify(inputShape) + \".\"));\n      }\n\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: _defineProperty({}, axis, dim)\n      })];\n      var shape = [dim];\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n      }\n\n      this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n      this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var input = getExactlyOneTensor(inputs);\n        var inputShape = input.shape;\n        var ndim = inputShape.length;\n        var reductionAxes = math_utils.range(0, ndim);\n        var axis = _this2.axis >= 0 ? _this2.axis : _this2.axis + ndim;\n        reductionAxes.splice(axis, 1);\n        var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n        broadcastShape[axis] = inputShape[axis];\n        var sortedReductionAxes = reductionAxes.slice();\n        sortedReductionAxes.sort();\n        var needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n\n        var normalizeInference = function normalizeInference() {\n          if (needsBroadcasting) {\n            var broadcastMovingMean = _this2.movingMean.read().reshape(broadcastShape);\n\n            var broadcastMovingVariance = _this2.movingVariance.read().reshape(broadcastShape);\n\n            var broadcastBeta = _this2.center ? _this2.beta.read().reshape(broadcastShape) : null;\n            var broadcastGamma = _this2.scale ? _this2.gamma.read().reshape(broadcastShape) : null;\n            return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this2.epsilon);\n          } else {\n            return batchNormalization(input, _this2.movingMean.read(), _this2.movingVariance.read(), _this2.beta == null ? null : _this2.beta.read(), _this2.gamma == null ? null : _this2.gamma.read(), _this2.epsilon);\n          }\n        };\n\n        if (!training) {\n          return normalizeInference();\n        }\n\n        var _normalizeBatchInTrai = normalizeBatchInTraining(input, _this2.gamma.read(), _this2.beta.read(), reductionAxes, _this2.epsilon),\n            _normalizeBatchInTrai2 = _slicedToArray(_normalizeBatchInTrai, 3),\n            normedTraining = _normalizeBatchInTrai2[0],\n            mean = _normalizeBatchInTrai2[1],\n            variance = _normalizeBatchInTrai2[2];\n\n        var doMovingAverage = function doMovingAverage(variable, value, momentum) {\n          tfc.tidy(function () {\n            var decay = 1 - momentum;\n            var origValue = variable.read();\n            var updateDelta = origValue.sub(value).mul(decay);\n            variable.write(origValue.sub(updateDelta));\n          });\n        };\n\n        var updateMovingMeanAndVariance = function updateMovingMeanAndVariance() {\n          doMovingAverage(_this2.movingMean, mean, _this2.momentum);\n          doMovingAverage(_this2.movingVariance, variance, _this2.momentum);\n        };\n\n        updateMovingMeanAndVariance();\n        return normedTraining;\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        momentum: this.momentum,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n        movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n        betaConstraint: serializeConstraint(this.betaConstraint),\n        gammaConstraint: serializeConstraint(this.gammaConstraint)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(BatchNormalization.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return BatchNormalization;\n}(Layer);\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport var LayerNormalization = function (_Layer2) {\n  _inherits(LayerNormalization, _Layer2);\n\n  var _super2 = _createSuper(LayerNormalization);\n\n  function LayerNormalization(args) {\n    var _this3;\n\n    _classCallCheck(this, LayerNormalization);\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3 = _super2.call(this, args);\n    _this3.axis = args.axis == null ? -1 : args.axis;\n\n    if (typeof _this3.axis === 'number') {\n      if (!Number.isInteger(_this3.axis)) {\n        throw new Error(\"Expected axis to be an integer, but received \" + _this3.axis);\n      }\n    } else if (Array.isArray(_this3.axis)) {\n      for (var _iterator2 = _createForOfIteratorHelperLoose(_this3.axis), _step2; !(_step2 = _iterator2()).done;) {\n        var axis = _step2.value;\n\n        if (!Number.isInteger(axis)) {\n          throw new Error(\"Expected axis to be an array of integers, \" + (\"but received \" + JSON.stringify(_this3.axis)));\n        }\n      }\n    } else {\n      throw new Error(\"Expected axis to be an integer or an array of integers, \" + (\"but received \" + JSON.stringify(_this3.axis)));\n    }\n\n    _this3.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n    _this3.center = args.center == null ? true : args.center;\n    _this3.scale = args.scale == null ? true : args.scale;\n    _this3.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n    _this3.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n    _this3.betaRegularizer = getRegularizer(args.betaRegularizer);\n    _this3.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    _this3.supportsMasking = true;\n    return _this3;\n  }\n\n  _createClass(LayerNormalization, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var nDims = inputShape.length;\n\n      if (typeof this.axis === 'number') {\n        this.axis = [this.axis];\n      }\n\n      for (var i = 0; i < this.axis.length; ++i) {\n        if (this.axis[i] < 0) {\n          this.axis[i] += nDims;\n        }\n      }\n\n      for (var _iterator3 = _createForOfIteratorHelperLoose(this.axis), _step3; !(_step3 = _iterator3()).done;) {\n        var axis = _step3.value;\n\n        if (axis < 0 || axis >= nDims) {\n          throw new Error(\"Invalid axis: \" + axis);\n        }\n      }\n\n      if (this.axis.length !== generic_utils.unique(this.axis).length) {\n        throw new Error(\"Found duplicate axes in: \" + this.axis);\n      }\n\n      var paramShape = this.axis.map(function (axis) {\n        return inputShape[axis];\n      });\n      var trainable = true;\n\n      if (this.scale) {\n        this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n      } else {\n        this.gamma = null;\n      }\n\n      if (this.center) {\n        this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n      } else {\n        this.beta = null;\n      }\n\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      var input = getExactlyOneTensor(inputs);\n      var inputShape = input.shape;\n      var nDims = inputShape.length;\n      return tidy(function () {\n        var keepDims = true;\n\n        var _moments = moments(input, _this4.axis, keepDims),\n            mean = _moments.mean,\n            variance = _moments.variance;\n\n        var broadcastShape = generic_utils.pyListRepeat(1, nDims);\n\n        for (var _iterator4 = _createForOfIteratorHelperLoose(_this4.axis), _step4; !(_step4 = _iterator4()).done;) {\n          var dim = _step4.value;\n          broadcastShape[dim] = inputShape[dim];\n        }\n\n        var broadcast = function broadcast(v) {\n          if (v != null && v.shape.length !== nDims && _this4.axis !== [nDims - 1]) {\n            return v.reshape(broadcastShape);\n          } else {\n            return v;\n          }\n        };\n\n        var scale = broadcast(_this4.gamma.read());\n        var offset = broadcast(_this4.beta.read());\n        var momentsTiling = [];\n        var scaleOffsetTiling = [];\n\n        for (var i = 0; i < nDims; ++i) {\n          if (_this4.axis.indexOf(i) !== -1) {\n            momentsTiling.push(inputShape[i]);\n            scaleOffsetTiling.push(1);\n          } else {\n            momentsTiling.push(1);\n            scaleOffsetTiling.push(inputShape[i]);\n          }\n        }\n\n        mean = mean.tile(momentsTiling);\n        variance = variance.tile(momentsTiling);\n        scale = scale.tile(scaleOffsetTiling);\n        offset = offset.tile(scaleOffsetTiling);\n        return batchNormalization(input, mean, variance, offset, scale, _this4.epsilon);\n      });\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis,\n        epsilon: this.epsilon,\n        center: this.center,\n        scale: this.scale,\n        betaInitializer: serializeInitializer(this.betaInitializer),\n        gammaInitializer: serializeInitializer(this.gammaInitializer),\n        betaRegularizer: serializeRegularizer(this.betaRegularizer),\n        gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n      };\n\n      var baseConfig = _get(_getPrototypeOf(LayerNormalization.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return LayerNormalization;\n}(Layer);\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);","map":{"version":3,"sources":["../../src/layers/normalization.ts"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;AAAA;;;;;;;;AAQG;AAMH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,OAAR,EAAiB,aAAjB,EAAgF,IAAhF,EAAsF,IAAtF,QAAiG,uBAAjG;AAEA,SAA0C,aAA1C,EAAyD,mBAAzD;AACA,SAAQ,SAAR,EAAmB,KAAnB;AACA,SAAQ,mBAAR,EAA6B,UAA7B;AACA,SAAQ,cAAR,EAA4D,oBAA5D;AAEA,SAAQ,cAAR,EAA4D,oBAA5D;AAEA,OAAO,KAAK,aAAZ;AACA,OAAO,KAAK,UAAZ;AACA,SAAQ,kBAAR,EAA4B,mBAA5B;AAiBA,OAAM,SAAU,kBAAV,CACF,CADE,EACS,IADT,EACuB,QADvB,EACyC,IADzC,EACwD,KADxD,EAEY;EAAA,IAAd,OAAc,uEAAJ,IAAI;EAChB,IAAI,GAAJ;;EACA,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;IAChB,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;EAID,CALD,MAKO,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;IAEvB,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;EAID,CANM,MAMA,IAAI,CAAC,CAAC,IAAF,KAAW,CAAf,EAAkB;IACvB,GAAG,GAAG,GAAG,CAAC,WAAJ,CACF,CADE,EACa,IADb,EAEF,QAFE,EAE+B,IAF/B,EAGF,KAHE,EAG4B,OAH5B,CAAN;EAID,CALM,MAKA;IACL,MAAM,IAAI,mBAAJ,CACF,6DAA2D,CAAC,CAAC,IAA7D,cADE,CAAN;EAGD;;EACD,OAAO,GAAP;AACD;;AAmBD,SAAS,+BAAT,CACI,CADJ,EACe,KADf,EAC8B,IAD9B,EAC4C,aAD5C,EAEkB;EAAA,IAAd,OAAc,uEAAJ,IAAI;EAChB,OAAO,IAAI,CAAC,YAAK;IACR,IAAM,eAAe,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,aAAf,CAAxB;IACA,IAAM,IAAI,GAAG,eAAe,CAAC,IAA7B;IACA,IAAM,QAAQ,GAAG,eAAe,CAAC,QAAjC;IACA,IAAM,MAAM,GACR,kBAAkB,CAAC,CAAD,EAAI,IAAJ,EAAU,QAAV,EAAoB,IAApB,EAA0B,KAA1B,EAAiC,OAAjC,CADtB;IAEA,OAAO,CAAC,MAAD,EAAS,IAAT,EAAe,QAAf,CAAP;EACD,CAPG,CAAX;AAQD;;AAmBD,SAAS,iCAAT,CACI,CADJ,EACe,KADf,EAC8B,IAD9B,EAC4C,aAD5C,EAEkB;EAAA,IAAd,OAAc,uEAAJ,IAAI;EAChB,OAAO,IAAI,CAAC,YAAK;IACR,IAAM,eAAe,GAAG,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,aAAf,CAAxB;IACA,IAAM,IAAI,GAAG,eAAe,CAAC,IAA7B;IACA,IAAM,QAAQ,GAAG,eAAe,CAAC,QAAjC;IACA,IAAM,WAAW,GAAa,EAA9B;;IACA,qDAAmB,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,CAAC,CAAC,IAAtB,CAAnB,wCAAgD;MAAA,IAArC,IAAqC;;MAC9C,IAAI,aAAa,CAAC,OAAd,CAAsB,IAAtB,MAAgC,CAAC,CAArC,EAAwC;QACtC,WAAW,CAAC,IAAZ,CAAiB,CAAjB;MACD,CAFD,MAEO;QACL,WAAW,CAAC,IAAZ,CAAiB,CAAC,CAAC,KAAF,CAAQ,IAAR,CAAjB;MACD;IACF;;IACD,IAAM,aAAa,GAAG,IAAI,CAAC,OAAL,CAAa,WAAb,CAAtB;IACA,IAAM,iBAAiB,GAAG,QAAQ,CAAC,OAAT,CAAiB,WAAjB,CAA1B;IACA,IAAM,cAAc,GAChB,KAAK,IAAI,IAAT,GAAgB,IAAhB,GAAuB,KAAK,CAAC,OAAN,CAAc,WAAd,CAD3B;IAEA,IAAM,aAAa,GACf,IAAI,IAAI,IAAR,GAAe,IAAf,GAAsB,IAAI,CAAC,OAAL,CAAa,WAAb,CAD1B;IAEA,IAAM,MAAM,GAAG,kBAAkB,CAC7B,CAD6B,EAC1B,aAD0B,EACX,iBADW,EACQ,aADR,EAE7B,cAF6B,EAEb,OAFa,CAAjC;IAGA,OAAO,CAAC,MAAD,EAAS,IAAT,EAAe,QAAf,CAAP;EACD,CAtBG,CAAX;AAuBD;;AAaD,OAAM,SAAU,wBAAV,CACF,CADE,EACS,KADT,EACwB,IADxB,EACsC,aADtC,EAEY;EAAA,IAAd,OAAc,uEAAJ,IAAI;;EAChB,IAAI,IAAI,CAAC,WAAL,CACI,aAAa,CAAC,KAAd,GAAsB,IAAtB,EADJ,EACkC,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,CAAC,CAAC,IAAF,GAAS,CAA7B,CADlC,CAAJ,EACwE;IACtE,OAAO,+BAA+B,CAClC,CADkC,EAC/B,KAD+B,EACxB,IADwB,EAClB,aADkB,EACH,OADG,CAAtC;EAED,CAJD,MAIO;IACL,OAAO,iCAAiC,CACpC,CADoC,EACjC,KADiC,EAC1B,IAD0B,EACpB,aADoB,EACL,OADK,CAAxC;EAED;AACF;AAoFD,WAAa,kBAAb;EAAA;;EAAA;;EAqBE,4BAAY,IAAZ,EAA8C;IAAA;;IAAA;;IAC5C,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IACD,0BAAM,IAAN;IAEA,MAAK,eAAL,GAAuB,IAAvB;IACA,MAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyB,IAAI,CAAC,IAA1C;IACA,MAAK,QAAL,GAAgB,IAAI,CAAC,QAAL,IAAiB,IAAjB,GAAwB,IAAxB,GAA+B,IAAI,CAAC,QAApD;IACA,MAAK,OAAL,GAAe,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B,IAAI,CAAC,OAAlD;IACA,MAAK,MAAL,GAAc,IAAI,CAAC,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6B,IAAI,CAAC,MAAhD;IACA,MAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,IAAI,CAAC,KAA9C;IACA,MAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAL,IAAwB,OAAzB,CAArC;IACA,MAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,MAA1B,CAAtC;IACA,MAAK,qBAAL,GACI,cAAc,CAAC,IAAI,CAAC,qBAAL,IAA8B,OAA/B,CADlB;IAEA,MAAK,yBAAL,GACI,cAAc,CAAC,IAAI,CAAC,yBAAL,IAAkC,MAAnC,CADlB;IAEA,MAAK,cAAL,GAAsB,aAAa,CAAC,IAAI,CAAC,cAAN,CAAnC;IACA,MAAK,eAAL,GAAuB,aAAa,CAAC,IAAI,CAAC,eAAN,CAApC;IACA,MAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAN,CAArC;IACA,MAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;IArB4C;EAsB7C;;EA3CH;IAAA;IAAA,OA6CS,eAAM,UAAN,EAA+B;MACpC,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;MACA,IAAM,IAAI,GAAG,KAAK,IAAL,IAAa,CAAb,GAAiB,KAAK,IAAtB,GAA8B,KAAK,IAAL,GAAY,UAAU,CAAC,MAAlE;MACA,IAAM,GAAG,GAAG,UAAU,CAAC,IAAD,CAAtB;;MACA,IAAI,GAAG,IAAI,IAAX,EAAiB;QACf,MAAM,IAAI,UAAJ,CACF,UAAQ,IAAR,0GAEG,IAAI,CAAC,SAAL,CAAe,UAAf,CAFH,OADE,CAAN;MAID;;MACD,KAAK,SAAL,GACI,CAAC,IAAI,SAAJ,CAAc;QAAC,IAAI,EAAE,UAAU,CAAC,MAAlB;QAA0B,IAAI,sBAAI,IAAJ,EAAW,GAAX;MAA9B,CAAd,CAAD,CADJ;MAEA,IAAM,KAAK,GAAG,CAAC,GAAD,CAAd;;MACA,IAAI,KAAK,KAAT,EAAgB;QACd,KAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,KADA,EACO,IADP,EACa,KAAK,gBADlB,EACoC,KAAK,gBADzC,EAET,IAFS,EAEH,KAAK,eAFF,CAAb;MAGD;;MACD,IAAI,KAAK,MAAT,EAAiB;QACf,KAAK,IAAL,GAAY,KAAK,SAAL,CACR,MADQ,EACA,KADA,EACO,IADP,EACa,KAAK,eADlB,EACmC,KAAK,eADxC,EACyD,IADzD,EAER,KAAK,cAFG,CAAZ;MAGD;;MACD,KAAK,UAAL,GAAkB,KAAK,SAAL,CACd,aADc,EACC,KADD,EACQ,IADR,EACc,KAAK,qBADnB,EAC0C,IAD1C,EACgD,KADhD,CAAlB;MAEA,KAAK,cAAL,GAAsB,KAAK,SAAL,CAClB,iBADkB,EACC,KADD,EACQ,IADR,EACc,KAAK,yBADnB,EAC8C,IAD9C,EAElB,KAFkB,CAAtB;MAGA,KAAK,KAAL,GAAa,IAAb;IACD;EA1EH;IAAA;IAAA,OA4EE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QACf,IAAM,QAAQ,GAAG,MAAM,CAAC,UAAD,CAAN,IAAsB,IAAtB,GAA6B,KAA7B,GAAqC,MAAM,CAAC,UAAD,CAA5D;QACA,IAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;QACA,IAAM,UAAU,GAAG,KAAK,CAAC,KAAzB;QACA,IAAM,IAAI,GAAG,UAAU,CAAC,MAAxB;QACA,IAAM,aAAa,GAAG,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,IAApB,CAAtB;QACA,IAAM,IAAI,GAAG,MAAI,CAAC,IAAL,IAAa,CAAb,GAAiB,MAAI,CAAC,IAAtB,GAA8B,MAAI,CAAC,IAAL,GAAY,IAAvD;QACA,aAAa,CAAC,MAAd,CAAqB,IAArB,EAA2B,CAA3B;QACA,IAAM,cAAc,GAAG,aAAa,CAAC,YAAd,CAA2B,CAA3B,EAA8B,IAA9B,CAAvB;QACA,cAAc,CAAC,IAAD,CAAd,GAAuB,UAAU,CAAC,IAAD,CAAjC;QAEA,IAAM,mBAAmB,GAAG,aAAa,CAAC,KAAd,EAA5B;QACA,mBAAmB,CAAC,IAApB;QACA,IAAM,iBAAiB,GAAG,CAAC,IAAI,CAAC,WAAL,CACvB,mBADuB,EACF,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,IAApB,EAA0B,KAA1B,CAAgC,CAAhC,EAAmC,IAAI,GAAG,CAA1C,CADE,CAA3B;;QAGA,IAAM,kBAAkB,GAAiB,SAAnC,kBAAmC,GAAK;UAC5C,IAAI,iBAAJ,EAAuB;YACrB,IAAM,mBAAmB,GACrB,MAAI,CAAC,UAAL,CAAgB,IAAhB,GAAuB,OAAvB,CAA+B,cAA/B,CADJ;;YAEA,IAAM,uBAAuB,GACzB,MAAI,CAAC,cAAL,CAAoB,IAApB,GAA2B,OAA3B,CAAmC,cAAnC,CADJ;;YAEA,IAAM,aAAa,GACf,MAAI,CAAC,MAAL,GAAc,MAAI,CAAC,IAAL,CAAU,IAAV,GAAiB,OAAjB,CAAyB,cAAzB,CAAd,GAAyD,IAD7D;YAEA,IAAM,cAAc,GAChB,MAAI,CAAC,KAAL,GAAa,MAAI,CAAC,KAAL,CAAW,IAAX,GAAkB,OAAlB,CAA0B,cAA1B,CAAb,GAAyD,IAD7D;YAEA,OAAO,kBAAkB,CACrB,KADqB,EACd,mBADc,EACO,uBADP,EAErB,aAFqB,EAEN,cAFM,EAEU,MAAI,CAAC,OAFf,CAAzB;UAGD,CAZD,MAYO;YACL,OAAO,kBAAkB,CACrB,KADqB,EACd,MAAI,CAAC,UAAL,CAAgB,IAAhB,EADc,EACU,MAAI,CAAC,cAAL,CAAoB,IAApB,EADV,EAErB,MAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,IAApB,GAA2B,MAAI,CAAC,IAAL,CAAU,IAAV,EAFN,EAGrB,MAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,MAAI,CAAC,KAAL,CAAW,IAAX,EAHP,EAG0B,MAAI,CAAC,OAH/B,CAAzB;UAID;QACF,CAnBD;;QAqBA,IAAI,CAAC,QAAL,EAAe;UACb,OAAO,kBAAkB,EAAzB;QACD;;QAED,4BAAyC,wBAAwB,CAC7D,KAD6D,EACtD,MAAI,CAAC,KAAL,CAAW,IAAX,EADsD,EACnC,MAAI,CAAC,IAAL,CAAU,IAAV,EADmC,EACjB,aADiB,EAE7D,MAAI,CAAC,OAFwD,CAAjE;QAAA;QAAA,IAAO,cAAP;QAAA,IAAuB,IAAvB;QAAA,IAA6B,QAA7B;;QAIA,IAAM,eAAe,GACjB,SADE,eACF,CAAC,QAAD,EAA0B,KAA1B,EAAyC,QAAzC,EAAmE;UACjE,GAAG,CAAC,IAAJ,CAAS,YAAK;YACZ,IAAM,KAAK,GAAG,IAAI,QAAlB;YACA,IAAM,SAAS,GAAG,QAAQ,CAAC,IAAT,EAAlB;YACA,IAAM,WAAW,GAAG,SAAS,CAAC,GAAV,CAAc,KAAd,EAAqB,GAArB,CAAyB,KAAzB,CAApB;YACA,QAAQ,CAAC,KAAT,CAAe,SAAS,CAAC,GAAV,CAAc,WAAd,CAAf;UACD,CALD;QAMD,CARL;;QAgBA,IAAM,2BAA2B,GAAG,SAA9B,2BAA8B,GAAK;UACvC,eAAe,CAAC,MAAI,CAAC,UAAN,EAAkB,IAAlB,EAAwB,MAAI,CAAC,QAA7B,CAAf;UACA,eAAe,CAAC,MAAI,CAAC,cAAN,EAAsB,QAAtB,EAAgC,MAAI,CAAC,QAArC,CAAf;QACD,CAHD;;QAIA,2BAA2B;QAE3B,OAAO,cAAP;MACD,CApEU,CAAX;IAqED;EAlJH;IAAA;IAAA,OAoJE,qBAAS;MACP,IAAM,MAAM,GAA6B;QACvC,IAAI,EAAE,KAAK,IAD4B;QAEvC,QAAQ,EAAE,KAAK,QAFwB;QAGvC,OAAO,EAAE,KAAK,OAHyB;QAIvC,MAAM,EAAE,KAAK,MAJ0B;QAKvC,KAAK,EAAE,KAAK,KAL2B;QAMvC,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CANE;QAOvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAPC;QAQvC,qBAAqB,EAAE,oBAAoB,CAAC,KAAK,qBAAN,CARJ;QASvC,yBAAyB,EACrB,oBAAoB,CAAC,KAAK,yBAAN,CAVe;QAWvC,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CAXE;QAYvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAZC;QAavC,cAAc,EAAE,mBAAmB,CAAC,KAAK,cAAN,CAbI;QAcvC,eAAe,EAAE,mBAAmB,CAAC,KAAK,eAAN;MAdG,CAAzC;;MAgBA,IAAM,UAAU,oFAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EAxKH;;EAAA;AAAA,EAAwC,KAAxC;AAES,kBAAA,CAAA,SAAA,GAAY,oBAAZ;AAwKT,aAAa,CAAC,aAAd,CAA4B,kBAA5B;AAkDA,WAAa,kBAAb;EAAA;;EAAA;;EAgBE,4BAAY,IAAZ,EAA8C;IAAA;;IAAA;;IAC5C,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IACD,4BAAM,IAAN;IAEA,OAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,CAAC,CAArB,GAAyB,IAAI,CAAC,IAA1C;;IACA,IAAI,OAAO,OAAK,IAAZ,KAAqB,QAAzB,EAAmC;MACjC,IAAI,CAAC,MAAM,CAAC,SAAP,CAAiB,OAAK,IAAtB,CAAL,EAAkC;QAChC,MAAM,IAAI,KAAJ,mDAC8C,OAAK,IADnD,CAAN;MAED;IACF,CALD,MAKO,IAAI,KAAK,CAAC,OAAN,CAAc,OAAK,IAAnB,CAAJ,EAA8B;MACnC,sDAAmB,OAAK,IAAxB,2CAA8B;QAAA,IAAnB,IAAmB;;QAC5B,IAAI,CAAC,MAAM,CAAC,SAAP,CAAiB,IAAjB,CAAL,EAA6B;UAC3B,MAAM,IAAI,KAAJ,CACF,kEACgB,IAAI,CAAC,SAAL,CAAe,OAAK,IAApB,CADhB,CADE,CAAN;QAGD;MACF;IACF,CARM,MAQA;MACL,MAAM,IAAI,KAAJ,CACF,gFACgB,IAAI,CAAC,SAAL,CAAe,OAAK,IAApB,CADhB,CADE,CAAN;IAGD;;IAED,OAAK,OAAL,GAAe,IAAI,CAAC,OAAL,IAAgB,IAAhB,GAAuB,IAAvB,GAA8B,IAAI,CAAC,OAAlD;IACA,OAAK,MAAL,GAAc,IAAI,CAAC,MAAL,IAAe,IAAf,GAAsB,IAAtB,GAA6B,IAAI,CAAC,MAAhD;IACA,OAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,IAArB,GAA4B,IAAI,CAAC,KAA9C;IACA,OAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAL,IAAwB,OAAzB,CAArC;IACA,OAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,MAA1B,CAAtC;IACA,OAAK,eAAL,GAAuB,cAAc,CAAC,IAAI,CAAC,eAAN,CAArC;IACA,OAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;IAEA,OAAK,eAAL,GAAuB,IAAvB;IAlC4C;EAmC7C;;EAnDH;IAAA;IAAA,OAqDS,eAAM,UAAN,EAA+B;MACpC,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;MACA,IAAM,KAAK,GAAG,UAAU,CAAC,MAAzB;;MAGA,IAAI,OAAO,KAAK,IAAZ,KAAqB,QAAzB,EAAmC;QACjC,KAAK,IAAL,GAAY,CAAC,KAAK,IAAN,CAAZ;MACD;;MACD,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,IAAL,CAAU,MAA9B,EAAsC,EAAE,CAAxC,EAA2C;QACzC,IAAI,KAAK,IAAL,CAAU,CAAV,IAAe,CAAnB,EAAsB;UACpB,KAAK,IAAL,CAAU,CAAV,KAAgB,KAAhB;QACD;MACF;;MAGD,sDAAmB,KAAK,IAAxB,2CAA8B;QAAA,IAAnB,IAAmB;;QAC5B,IAAI,IAAI,GAAG,CAAP,IAAY,IAAI,IAAI,KAAxB,EAA+B;UAC7B,MAAM,IAAI,KAAJ,oBAA2B,IAA3B,CAAN;QACD;MACF;;MACD,IAAI,KAAK,IAAL,CAAU,MAAV,KAAqB,aAAa,CAAC,MAAd,CAAqB,KAAK,IAA1B,EAAgC,MAAzD,EAAiE;QAC/D,MAAM,IAAI,KAAJ,+BAAsC,KAAK,IAA3C,CAAN;MACD;;MAED,IAAM,UAAU,GAAG,KAAK,IAAL,CAAU,GAAV,CAAc,UAAA,IAAI;QAAA,OAAI,UAAU,CAAC,IAAD,CAAd;MAAA,CAAlB,CAAnB;MAEA,IAAM,SAAS,GAAG,IAAlB;;MACA,IAAI,KAAK,KAAT,EAAgB;QACd,KAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,UADA,EACY,SADZ,EACuB,KAAK,gBAD5B,EAET,KAAK,gBAFI,EAEc,SAFd,CAAb;MAGD,CAJD,MAIO;QACL,KAAK,KAAL,GAAa,IAAb;MACD;;MACD,IAAI,KAAK,MAAT,EAAiB;QACf,KAAK,IAAL,GAAY,KAAK,SAAL,CACR,MADQ,EACA,UADA,EACY,SADZ,EACuB,KAAK,eAD5B,EAER,KAAK,eAFG,EAEc,SAFd,CAAZ;MAGD,CAJD,MAIO;QACL,KAAK,IAAL,GAAY,IAAZ;MACD;;MAED,KAAK,KAAL,GAAa,IAAb;IACD;EAhGH;IAAA;IAAA,OAkGE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,IAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;MACA,IAAM,UAAU,GAAG,KAAK,CAAC,KAAzB;MACA,IAAM,KAAK,GAAG,UAAU,CAAC,MAAzB;MAEA,OAAO,IAAI,CAAC,YAAK;QACf,IAAM,QAAQ,GAAG,IAAjB;;QACA,eAAuB,OAAO,CAAC,KAAD,EAAQ,MAAI,CAAC,IAAb,EAAmB,QAAnB,CAA9B;QAAA,IAAK,IAAL,YAAK,IAAL;QAAA,IAAW,QAAX,YAAW,QAAX;;QACA,IAAM,cAAc,GAAG,aAAa,CAAC,YAAd,CAA2B,CAA3B,EAA8B,KAA9B,CAAvB;;QACA,sDAAkB,MAAI,CAAC,IAAvB,2CAAyC;UAAA,IAA9B,GAA8B;UACvC,cAAc,CAAC,GAAD,CAAd,GAAsB,UAAU,CAAC,GAAD,CAAhC;QACD;;QAED,IAAM,SAAS,GAAG,SAAZ,SAAY,CAAC,CAAD,EAAc;UAC9B,IAAI,CAAC,IAAI,IAAL,IAAa,CAAC,CAAC,KAAF,CAAQ,MAAR,KAAmB,KAAhC,IACA,MAAI,CAAC,IAAL,KAAc,CAAC,KAAK,GAAG,CAAT,CADlB,EAC+B;YAC7B,OAAO,CAAC,CAAC,OAAF,CAAU,cAAV,CAAP;UACD,CAHD,MAGO;YACL,OAAO,CAAP;UACD;QACF,CAPD;;QASA,IAAI,KAAK,GAAG,SAAS,CAAC,MAAI,CAAC,KAAL,CAAW,IAAX,EAAD,CAArB;QACA,IAAI,MAAM,GAAG,SAAS,CAAC,MAAI,CAAC,IAAL,CAAU,IAAV,EAAD,CAAtB;QAQA,IAAM,aAAa,GAAa,EAAhC;QACA,IAAM,iBAAiB,GAAa,EAApC;;QACA,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAApB,EAA2B,EAAE,CAA7B,EAAgC;UAC9B,IAAK,MAAI,CAAC,IAAL,CAAuB,OAAvB,CAA+B,CAA/B,MAAsC,CAAC,CAA5C,EAA+C;YAC7C,aAAa,CAAC,IAAd,CAAmB,UAAU,CAAC,CAAD,CAA7B;YACA,iBAAiB,CAAC,IAAlB,CAAuB,CAAvB;UACD,CAHD,MAGO;YACL,aAAa,CAAC,IAAd,CAAmB,CAAnB;YACA,iBAAiB,CAAC,IAAlB,CAAuB,UAAU,CAAC,CAAD,CAAjC;UACD;QACF;;QACD,IAAI,GAAG,IAAI,CAAC,IAAL,CAAU,aAAV,CAAP;QACA,QAAQ,GAAG,QAAQ,CAAC,IAAT,CAAc,aAAd,CAAX;QACA,KAAK,GAAG,KAAK,CAAC,IAAN,CAAW,iBAAX,CAAR;QACA,MAAM,GAAG,MAAM,CAAC,IAAP,CAAY,iBAAZ,CAAT;QAEA,OAAO,kBAAkB,CACrB,KADqB,EACd,IADc,EACR,QADQ,EACE,MADF,EACU,KADV,EACiB,MAAI,CAAC,OADtB,CAAzB;MAED,CA5CU,CAAX;IA6CD;EApJH;IAAA;IAAA,OAsJE,qBAAS;MACP,IAAM,MAAM,GAA6B;QACvC,IAAI,EAAE,KAAK,IAD4B;QAEvC,OAAO,EAAE,KAAK,OAFyB;QAGvC,MAAM,EAAE,KAAK,MAH0B;QAIvC,KAAK,EAAE,KAAK,KAJ2B;QAKvC,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CALE;QAMvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CANC;QAOvC,eAAe,EAAE,oBAAoB,CAAC,KAAK,eAAN,CAPE;QAQvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN;MARC,CAAzC;;MAUA,IAAM,UAAU,oFAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EApKH;;EAAA;AAAA,EAAwC,KAAxC;AAES,kBAAA,CAAA,SAAA,GAAY,oBAAZ;AAoKT,aAAa,CAAC,aAAd,CAA4B,kBAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Normalization layers.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { moments, serialization, tidy, util } from '@tensorflow/tfjs-core';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport * as generic_utils from '../utils/generic_utils';\nimport * as math_utils from '../utils/math_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\n/**\n * Applies batch normalization on x given mean, var, beta and gamma.\n *\n * I.e. returns:\n *   `output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta`\n *\n * @param x Input tensor.\n * @param mean Mean of batch.\n * @param variance Variance of batch.\n * @param beta Tensor with which to center the input.\n * @param gamma Tensor by which to scale the input.\n * @param epsilon Fuzz factor.\n * @returns The result of the batch normalization.\n */\nexport function batchNormalization(x, mean, variance, beta, gamma, epsilon = 1e-3) {\n    let out;\n    if (x.rank === 2) {\n        out = tfc.batchNorm2d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 3) {\n        // TODO(cais): Check rank; give proper error message.\n        out = tfc.batchNorm3d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else if (x.rank === 4) {\n        out = tfc.batchNorm4d(x, mean, variance, beta, gamma, epsilon);\n    }\n    else {\n        throw new NotImplementedError(`batchNormalization is not implemented for array of rank ${x.rank} ` +\n            `yet`);\n    }\n    return out;\n}\n/**\n * Non-broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const normed = batchNormalization(x, mean, variance, beta, gamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Broadcasting batch normalization for use in training (not inference).\n *\n * The input is normalized to zero mean and unit variance along the\n * `reductionAxes`, followed by scaling with `gamma` and shifted by `beta`.\n * The result of that is returned as the first element\n * of the returned `Array`. The other two elements are the mean and variance,\n * respectively.\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nfunction broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    return tidy(() => {\n        const meanAndVariance = tfc.moments(x, reductionAxes);\n        const mean = meanAndVariance.mean;\n        const variance = meanAndVariance.variance;\n        const targetShape = [];\n        for (const axis of math_utils.range(0, x.rank)) {\n            if (reductionAxes.indexOf(axis) !== -1) {\n                targetShape.push(1);\n            }\n            else {\n                targetShape.push(x.shape[axis]);\n            }\n        }\n        const broadcastMean = mean.reshape(targetShape);\n        const broadcastVariance = variance.reshape(targetShape);\n        const broadcastGamma = gamma == null ? null : gamma.reshape(targetShape);\n        const broadcastBeta = beta == null ? null : beta.reshape(targetShape);\n        const normed = batchNormalization(x, broadcastMean, broadcastVariance, broadcastBeta, broadcastGamma, epsilon);\n        return [normed, mean, variance];\n    });\n}\n/**\n * Batch normalization for use in training (not inference).\n *\n * @param x Input tensor to be normalized.\n * @param gamma Tensor by which to scale the input.\n * @param beta Tensor by which to center the input.\n * @param reductionAxes Axes over which to normalize.\n * @param epsilon Fuzz factor.\n * @returns An `Array` of three `Tensors`:\n *   [normalized tensor, mean of input, variance of input].\n */\nexport function normalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon = 1e-3) {\n    if (util.arraysEqual(reductionAxes.slice().sort(), math_utils.range(0, x.rank - 1))) {\n        return regularNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n    else {\n        return broadcastNormalizeBatchInTraining(x, gamma, beta, reductionAxes, epsilon);\n    }\n}\nexport class BatchNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.supportsMasking = true;\n        this.axis = args.axis == null ? -1 : args.axis;\n        this.momentum = args.momentum == null ? 0.99 : args.momentum;\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.movingMeanInitializer =\n            getInitializer(args.movingMeanInitializer || 'zeros');\n        this.movingVarianceInitializer =\n            getInitializer(args.movingVarianceInitializer || 'ones');\n        this.betaConstraint = getConstraint(args.betaConstraint);\n        this.gammaConstraint = getConstraint(args.gammaConstraint);\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        const dim = inputShape[axis];\n        if (dim == null) {\n            throw new ValueError(`Axis ${axis} of input tensor should have a defined dimension but ` +\n                `the layer received an input with shape ` +\n                `${JSON.stringify(inputShape)}.`);\n        }\n        this.inputSpec =\n            [new InputSpec({ ndim: inputShape.length, axes: { [axis]: dim } })];\n        const shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const training = kwargs['training'] == null ? false : kwargs['training'];\n            const input = getExactlyOneTensor(inputs);\n            const inputShape = input.shape;\n            const ndim = inputShape.length;\n            const reductionAxes = math_utils.range(0, ndim);\n            const axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n            reductionAxes.splice(axis, 1);\n            const broadcastShape = generic_utils.pyListRepeat(1, ndim);\n            broadcastShape[axis] = inputShape[axis];\n            const sortedReductionAxes = reductionAxes.slice();\n            sortedReductionAxes.sort();\n            const needsBroadcasting = !util.arraysEqual(sortedReductionAxes, math_utils.range(0, ndim).slice(0, ndim - 1));\n            const normalizeInference = () => {\n                if (needsBroadcasting) {\n                    const broadcastMovingMean = this.movingMean.read().reshape(broadcastShape);\n                    const broadcastMovingVariance = this.movingVariance.read().reshape(broadcastShape);\n                    const broadcastBeta = this.center ? this.beta.read().reshape(broadcastShape) : null;\n                    const broadcastGamma = this.scale ? this.gamma.read().reshape(broadcastShape) : null;\n                    return batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, this.epsilon);\n                }\n                else {\n                    return batchNormalization(input, this.movingMean.read(), this.movingVariance.read(), this.beta == null ? null : this.beta.read(), this.gamma == null ? null : this.gamma.read(), this.epsilon);\n                }\n            };\n            if (!training) {\n                return normalizeInference();\n            }\n            const [normedTraining, mean, variance] = normalizeBatchInTraining(input, this.gamma.read(), this.beta.read(), reductionAxes, this.epsilon);\n            const doMovingAverage = (variable, value, momentum) => {\n                tfc.tidy(() => {\n                    const decay = 1 - momentum;\n                    const origValue = variable.read();\n                    const updateDelta = origValue.sub(value).mul(decay);\n                    variable.write(origValue.sub(updateDelta));\n                });\n            };\n            // Perform updates to moving mean and moving variance for training.\n            // Porting Note: In PyKeras, these updates to `movingMean` and\n            //   `movingAverage` are done as a deferred Graph, added to the `Layer`'s\n            //   `update`s using the `add_update()` method. Here we do it imperatively\n            //   and encapsulate the updates in a function that is invoked\n            //   immediately.\n            const updateMovingMeanAndVariance = () => {\n                doMovingAverage(this.movingMean, mean, this.momentum);\n                doMovingAverage(this.movingVariance, variance, this.momentum);\n            };\n            updateMovingMeanAndVariance();\n            return normedTraining;\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: serializeConstraint(this.betaConstraint),\n            gammaConstraint: serializeConstraint(this.gammaConstraint)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nBatchNormalization.className = 'BatchNormalization';\nserialization.registerClass(BatchNormalization);\nexport class LayerNormalization extends Layer {\n    constructor(args) {\n        if (args == null) {\n            args = {};\n        }\n        super(args);\n        this.axis = args.axis == null ? -1 : args.axis;\n        if (typeof this.axis === 'number') {\n            if (!Number.isInteger(this.axis)) {\n                throw new Error(`Expected axis to be an integer, but received ${this.axis}`);\n            }\n        }\n        else if (Array.isArray(this.axis)) {\n            for (const axis of this.axis) {\n                if (!Number.isInteger(axis)) {\n                    throw new Error(`Expected axis to be an array of integers, ` +\n                        `but received ${JSON.stringify(this.axis)}`);\n                }\n            }\n        }\n        else {\n            throw new Error(`Expected axis to be an integer or an array of integers, ` +\n                `but received ${JSON.stringify(this.axis)}`);\n        }\n        this.epsilon = args.epsilon == null ? 1e-3 : args.epsilon;\n        this.center = args.center == null ? true : args.center;\n        this.scale = args.scale == null ? true : args.scale;\n        this.betaInitializer = getInitializer(args.betaInitializer || 'zeros');\n        this.gammaInitializer = getInitializer(args.gammaInitializer || 'ones');\n        this.betaRegularizer = getRegularizer(args.betaRegularizer);\n        this.gammaRegularizer = getRegularizer(args.gammaRegularizer);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const nDims = inputShape.length;\n        // Convert axis to array and resolve negatives.\n        if (typeof this.axis === 'number') {\n            this.axis = [this.axis];\n        }\n        for (let i = 0; i < this.axis.length; ++i) {\n            if (this.axis[i] < 0) {\n                this.axis[i] += nDims;\n            }\n        }\n        // Further validate axes.\n        for (const axis of this.axis) {\n            if (axis < 0 || axis >= nDims) {\n                throw new Error(`Invalid axis: ${axis}`);\n            }\n        }\n        if (this.axis.length !== generic_utils.unique(this.axis).length) {\n            throw new Error(`Found duplicate axes in: ${this.axis}`);\n        }\n        const paramShape = this.axis.map(axis => inputShape[axis]);\n        const trainable = true;\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', paramShape, 'float32', this.gammaInitializer, this.gammaRegularizer, trainable);\n        }\n        else {\n            this.gamma = null;\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', paramShape, 'float32', this.betaInitializer, this.betaRegularizer, trainable);\n        }\n        else {\n            this.beta = null;\n        }\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        const input = getExactlyOneTensor(inputs);\n        const inputShape = input.shape;\n        const nDims = inputShape.length;\n        return tidy(() => {\n            const keepDims = true;\n            let { mean, variance } = moments(input, this.axis, keepDims);\n            const broadcastShape = generic_utils.pyListRepeat(1, nDims);\n            for (const dim of this.axis) {\n                broadcastShape[dim] = inputShape[dim];\n            }\n            const broadcast = (v) => {\n                if (v != null && v.shape.length !== nDims &&\n                    this.axis !== [nDims - 1]) {\n                    return v.reshape(broadcastShape);\n                }\n                else {\n                    return v;\n                }\n            };\n            let scale = broadcast(this.gamma.read());\n            let offset = broadcast(this.beta.read());\n            // TODO(https://github.com/tensorflow/tfjs/issues/2120): The tiling below\n            // is a workaround for the limitation of core's batchNormalization?d don't\n            // support broadcasting in their gradients. In addition, the tiling is\n            // necessary to ensure correctness on the browser CPU backend regardless\n            // of forward or backward computation. Remove this workaround once the\n            // limitation is addressed. See .\n            const momentsTiling = [];\n            const scaleOffsetTiling = [];\n            for (let i = 0; i < nDims; ++i) {\n                if (this.axis.indexOf(i) !== -1) {\n                    momentsTiling.push(inputShape[i]);\n                    scaleOffsetTiling.push(1);\n                }\n                else {\n                    momentsTiling.push(1);\n                    scaleOffsetTiling.push(inputShape[i]);\n                }\n            }\n            mean = mean.tile(momentsTiling);\n            variance = variance.tile(momentsTiling);\n            scale = scale.tile(scaleOffsetTiling);\n            offset = offset.tile(scaleOffsetTiling);\n            return batchNormalization(input, mean, variance, offset, scale, this.epsilon);\n        });\n    }\n    getConfig() {\n        const config = {\n            axis: this.axis,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: serializeInitializer(this.betaInitializer),\n            gammaInitializer: serializeInitializer(this.gammaInitializer),\n            betaRegularizer: serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: serializeRegularizer(this.gammaRegularizer)\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLayerNormalization.className = 'LayerNormalization';\nserialization.registerClass(LayerNormalization);\n//# sourceMappingURL=normalization.js.map"]},"metadata":{},"sourceType":"module"}