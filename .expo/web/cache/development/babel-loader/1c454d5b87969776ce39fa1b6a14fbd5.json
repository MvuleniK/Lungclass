{"ast":null,"code":"import _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\nimport _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\nimport _regeneratorRuntime from \"@babel/runtime/regenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from \"./iterators/lazy_iterator\";\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from \"./util/deep_map\";\nexport var Dataset = function () {\n  function Dataset() {\n    _classCallCheck(this, Dataset);\n\n    this.size = null;\n  }\n\n  _createClass(Dataset, [{\n    key: \"batch\",\n    value: function batch(batchSize) {\n      var smallLastBatch = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n      var base = this;\n      tf.util.assert(batchSize > 0, function () {\n        return \"batchSize needs to be positive, but it is\\n      \" + batchSize;\n      });\n      var size;\n\n      if (this.size === Infinity || this.size == null) {\n        size = this.size;\n      } else if (smallLastBatch) {\n        size = Math.ceil(this.size / batchSize);\n      } else {\n        size = Math.floor(this.size / batchSize);\n      }\n\n      return datasetFromIteratorFn(function _callee() {\n        return _regeneratorRuntime.async(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                _context.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context.abrupt(\"return\", _context.sent.columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat));\n\n              case 3:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"concatenate\",\n    value: function concatenate(dataset) {\n      var base = this;\n      var size;\n\n      if (this.size === Infinity || dataset.size === Infinity) {\n        size = Infinity;\n      } else if (this.size != null && dataset.size != null) {\n        size = this.size + dataset.size;\n      } else {\n        size = null;\n      }\n\n      return datasetFromIteratorFn(function _callee2() {\n        return _regeneratorRuntime.async(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                _context2.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                _context2.t0 = _context2.sent;\n                _context2.next = 5;\n                return _regeneratorRuntime.awrap(dataset.iterator());\n\n              case 5:\n                _context2.t1 = _context2.sent;\n                return _context2.abrupt(\"return\", _context2.t0.concatenate.call(_context2.t0, _context2.t1));\n\n              case 7:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"filter\",\n    value: function filter(predicate) {\n      var base = this;\n      var size;\n\n      if (this.size === Infinity) {\n        size = Infinity;\n      } else {\n        size = null;\n      }\n\n      return datasetFromIteratorFn(function _callee3() {\n        return _regeneratorRuntime.async(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                _context3.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context3.abrupt(\"return\", _context3.sent.filter(function (x) {\n                  return tf.tidy(function () {\n                    return predicate(x);\n                  });\n                }));\n\n              case 3:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"forEachAsync\",\n    value: function forEachAsync(f) {\n      return _regeneratorRuntime.async(function forEachAsync$(_context4) {\n        while (1) {\n          switch (_context4.prev = _context4.next) {\n            case 0:\n              _context4.next = 2;\n              return _regeneratorRuntime.awrap(this.iterator());\n\n            case 2:\n              return _context4.abrupt(\"return\", _context4.sent.forEachAsync(f));\n\n            case 3:\n            case \"end\":\n              return _context4.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"map\",\n    value: function map(transform) {\n      var base = this;\n      return datasetFromIteratorFn(function _callee4() {\n        return _regeneratorRuntime.async(function _callee4$(_context5) {\n          while (1) {\n            switch (_context5.prev = _context5.next) {\n              case 0:\n                _context5.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context5.abrupt(\"return\", _context5.sent.map(function (x) {\n                  return tf.tidy(function () {\n                    return transform(x);\n                  });\n                }));\n\n              case 3:\n              case \"end\":\n                return _context5.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, this.size);\n    }\n  }, {\n    key: \"mapAsync\",\n    value: function mapAsync(transform) {\n      var base = this;\n      return datasetFromIteratorFn(function _callee5() {\n        return _regeneratorRuntime.async(function _callee5$(_context6) {\n          while (1) {\n            switch (_context6.prev = _context6.next) {\n              case 0:\n                _context6.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context6.abrupt(\"return\", _context6.sent.mapAsync(transform));\n\n              case 3:\n              case \"end\":\n                return _context6.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, this.size);\n    }\n  }, {\n    key: \"prefetch\",\n    value: function prefetch(bufferSize) {\n      if (bufferSize == null) {\n        throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n      }\n\n      var base = this;\n      return datasetFromIteratorFn(function _callee6() {\n        return _regeneratorRuntime.async(function _callee6$(_context7) {\n          while (1) {\n            switch (_context7.prev = _context7.next) {\n              case 0:\n                _context7.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context7.abrupt(\"return\", _context7.sent.prefetch(bufferSize));\n\n              case 3:\n              case \"end\":\n                return _context7.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, this.size);\n    }\n  }, {\n    key: \"repeat\",\n    value: function repeat(count) {\n      var base = this;\n      var size;\n\n      if (this.size != null && count > 0) {\n        size = this.size * count;\n      } else if (count === 0) {\n        size = 0;\n      } else if (this.size != null && (count === undefined || count < 0)) {\n        size = Infinity;\n      } else {\n        size = null;\n      }\n\n      return datasetFromIteratorFn(function _callee8() {\n        var iteratorIterator;\n        return _regeneratorRuntime.async(function _callee8$(_context9) {\n          while (1) {\n            switch (_context9.prev = _context9.next) {\n              case 0:\n                iteratorIterator = iteratorFromFunction(function _callee7() {\n                  return _regeneratorRuntime.async(function _callee7$(_context8) {\n                    while (1) {\n                      switch (_context8.prev = _context8.next) {\n                        case 0:\n                          _context8.next = 2;\n                          return _regeneratorRuntime.awrap(base.iterator());\n\n                        case 2:\n                          _context8.t0 = _context8.sent;\n                          return _context8.abrupt(\"return\", {\n                            value: _context8.t0,\n                            done: false\n                          });\n\n                        case 4:\n                        case \"end\":\n                          return _context8.stop();\n                      }\n                    }\n                  }, null, null, null, Promise);\n                });\n                return _context9.abrupt(\"return\", iteratorFromConcatenated(iteratorIterator.take(count)));\n\n              case 2:\n              case \"end\":\n                return _context9.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"skip\",\n    value: function skip(count) {\n      var base = this;\n      var size;\n\n      if (this.size != null && count >= 0 && this.size >= count) {\n        size = this.size - count;\n      } else if (this.size != null && (this.size < count || count === undefined || count < 0)) {\n        size = 0;\n      } else {\n        size = null;\n      }\n\n      return datasetFromIteratorFn(function _callee9() {\n        return _regeneratorRuntime.async(function _callee9$(_context10) {\n          while (1) {\n            switch (_context10.prev = _context10.next) {\n              case 0:\n                _context10.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context10.abrupt(\"return\", _context10.sent.skip(count));\n\n              case 3:\n              case \"end\":\n                return _context10.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"shuffle\",\n    value: function shuffle(bufferSize, seed) {\n      var reshuffleEachIteration = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : true;\n\n      if (bufferSize == null || bufferSize < 0) {\n        if (this.size == null) {\n          throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n        } else {\n          throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' + 'If your data fits in main memory (for regular JS objects), ' + 'and/or GPU memory (for `tf.Tensor`s), consider setting ' + (\"bufferSize to the dataset size (\" + this.size + \" elements)\"));\n        }\n      }\n\n      var base = this;\n      var random = seedrandom.alea(seed || tf.util.now().toString());\n      return datasetFromIteratorFn(function _callee10() {\n        var seed2;\n        return _regeneratorRuntime.async(function _callee10$(_context11) {\n          while (1) {\n            switch (_context11.prev = _context11.next) {\n              case 0:\n                seed2 = random.int32();\n\n                if (reshuffleEachIteration) {\n                  seed2 += random.int32();\n                }\n\n                _context11.next = 4;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 4:\n                return _context11.abrupt(\"return\", _context11.sent.shuffle(bufferSize, seed2.toString()));\n\n              case 5:\n              case \"end\":\n                return _context11.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, this.size);\n    }\n  }, {\n    key: \"take\",\n    value: function take(count) {\n      var base = this;\n      var size;\n\n      if (this.size != null && this.size > count) {\n        size = count;\n      } else if (this.size != null && this.size <= count) {\n        size = this.size;\n      } else {\n        size = null;\n      }\n\n      return datasetFromIteratorFn(function _callee11() {\n        return _regeneratorRuntime.async(function _callee11$(_context12) {\n          while (1) {\n            switch (_context12.prev = _context12.next) {\n              case 0:\n                _context12.next = 2;\n                return _regeneratorRuntime.awrap(base.iterator());\n\n              case 2:\n                return _context12.abrupt(\"return\", _context12.sent.take(count));\n\n              case 3:\n              case \"end\":\n                return _context12.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }, size);\n    }\n  }, {\n    key: \"toArray\",\n    value: function toArray() {\n      return _regeneratorRuntime.async(function toArray$(_context13) {\n        while (1) {\n          switch (_context13.prev = _context13.next) {\n            case 0:\n              if (!(this.size === Infinity)) {\n                _context13.next = 2;\n                break;\n              }\n\n              throw new Error('Can not convert infinite data stream to array.');\n\n            case 2:\n              _context13.next = 4;\n              return _regeneratorRuntime.awrap(this.iterator());\n\n            case 4:\n              return _context13.abrupt(\"return\", _context13.sent.toArray());\n\n            case 5:\n            case \"end\":\n              return _context13.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"toArrayForTest\",\n    value: function toArrayForTest() {\n      return _regeneratorRuntime.async(function toArrayForTest$(_context14) {\n        while (1) {\n          switch (_context14.prev = _context14.next) {\n            case 0:\n              if (!(this.size === Infinity)) {\n                _context14.next = 2;\n                break;\n              }\n\n              throw new Error('Can not convert infinite data stream to array.');\n\n            case 2:\n              _context14.next = 4;\n              return _regeneratorRuntime.awrap(this.iterator());\n\n            case 4:\n              return _context14.abrupt(\"return\", _context14.sent.toArrayForTest());\n\n            case 5:\n            case \"end\":\n              return _context14.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }]);\n\n  return Dataset;\n}();\nDataset.MAX_BUFFER_SIZE = 10000;\nexport function datasetFromIteratorFn(iteratorFn) {\n  var size = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : null;\n  return new (function (_Dataset) {\n    _inherits(_class, _Dataset);\n\n    var _super = _createSuper(_class);\n\n    function _class() {\n      var _this;\n\n      _classCallCheck(this, _class);\n\n      _this = _super.apply(this, arguments);\n      _this.size = size;\n      return _this;\n    }\n\n    _createClass(_class, [{\n      key: \"iterator\",\n      value: function iterator() {\n        return _regeneratorRuntime.async(function iterator$(_context15) {\n          while (1) {\n            switch (_context15.prev = _context15.next) {\n              case 0:\n                return _context15.abrupt(\"return\", iteratorFn());\n\n              case 1:\n              case \"end\":\n                return _context15.stop();\n            }\n          }\n        }, null, null, null, Promise);\n      }\n    }]);\n\n    return _class;\n  }(Dataset))();\n}\nexport function array(items) {\n  return datasetFromIteratorFn(function _callee12() {\n    return _regeneratorRuntime.async(function _callee12$(_context16) {\n      while (1) {\n        switch (_context16.prev = _context16.next) {\n          case 0:\n            return _context16.abrupt(\"return\", iteratorFromItems(items));\n\n          case 1:\n          case \"end\":\n            return _context16.stop();\n        }\n      }\n    }, null, null, null, Promise);\n  }, items.length);\n}\nexport function zip(datasets) {\n  if (!isIterable(datasets)) {\n    throw new Error('The argument to zip() must be an object or array.');\n  }\n\n  var size;\n\n  if (Array.isArray(datasets)) {\n    for (var i = 0; i < datasets.length; i++) {\n      size = size == null ? datasets[i].size : Math.min(size, datasets[i].size);\n    }\n  } else if (datasets instanceof Object) {\n    for (var ds in datasets) {\n      size = size == null ? datasets[ds].size : Math.min(size, datasets[ds].size);\n    }\n  }\n\n  return datasetFromIteratorFn(function _callee13() {\n    var streams;\n    return _regeneratorRuntime.async(function _callee13$(_context17) {\n      while (1) {\n        switch (_context17.prev = _context17.next) {\n          case 0:\n            _context17.next = 2;\n            return _regeneratorRuntime.awrap(deepMapAndAwaitAll(datasets, function (d) {\n              if (d instanceof Dataset) {\n                return {\n                  value: d.iterator(),\n                  recurse: false\n                };\n              } else if (isIterable(d)) {\n                return {\n                  value: null,\n                  recurse: true\n                };\n              } else {\n                throw new Error('Leaves of the structure passed to zip() must be Datasets, ' + 'not primitives.');\n              }\n            }));\n\n          case 2:\n            streams = _context17.sent;\n            return _context17.abrupt(\"return\", iteratorFromZipped(streams, ZipMismatchMode.SHORTEST));\n\n          case 4:\n          case \"end\":\n            return _context17.stop();\n        }\n      }\n    }, null, null, null, Promise);\n  }, size);\n}\n\nfunction deepBatchConcat(rows) {\n  if (rows === null) {\n    return null;\n  }\n\n  var exampleRow = rows[0];\n\n  if (canTensorify(exampleRow)) {\n    var value = batchConcat(rows);\n    return {\n      value: value,\n      recurse: false\n    };\n  }\n\n  return {\n    value: null,\n    recurse: true\n  };\n}\n\nfunction batchConcat(arrays) {\n  if (arrays.length === 0) {\n    throw new Error('Can\\'t make a batch of zero elements.');\n  }\n\n  if (arrays[0] instanceof tf.Tensor) {\n    return tf.stack(arrays);\n  } else {\n    return tf.tensor(arrays);\n  }\n}","map":{"version":3,"sources":["../src/dataset.ts"],"names":[],"mappings":";;;;;;;;;;;;AAAA;;;;;;;;;;;;;;;;AAgBG;AAEH,OAAO,KAAK,EAAZ,MAAoB,uBAApB;AAEA,OAAO,KAAK,UAAZ,MAA4B,YAA5B;AAEA,SAAQ,wBAAR,EAAkC,oBAAlC,EAAwD,iBAAxD,EAA2E,kBAA3E,EAA6G,eAA7G;AAEA,SAAQ,YAAR,EAAsB,kBAAtB,EAAyD,UAAzD;AAqCA,WAAsB,OAAtB;EAAA,mBAAA;IAAA;;IAWW,KAAA,IAAA,GAAe,IAAf;EA2cV;;EAtdD;IAAA;IAAA,OA0EE,eAAM,SAAN,EAA8C;MAAA,IAArB,cAAqB,uEAAJ,IAAI;MAC5C,IAAM,IAAI,GAAG,IAAb;MACA,EAAE,CAAC,IAAH,CAAQ,MAAR,CACI,SAAS,GAAG,CADhB,EACmB;QAAA,6DACf,SADe;MAAA,CADnB;MAGA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,KAAc,QAAd,IAA0B,KAAK,IAAL,IAAa,IAA3C,EAAiD;QAG/C,IAAI,GAAG,KAAK,IAAZ;MACD,CAJD,MAIO,IAAI,cAAJ,EAAoB;QAGzB,IAAI,GAAG,IAAI,CAAC,IAAL,CAAU,KAAK,IAAL,GAAY,SAAtB,CAAP;MACD,CAJM,MAIA;QAGL,IAAI,GAAG,IAAI,CAAC,KAAL,CAAW,KAAK,IAAL,GAAY,SAAvB,CAAP;MACD;;MACD,OAAO,qBAAqB,CAAC;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCACb,IAAI,CAAC,QAAL,EADa;;cAAA;gBAAA,+CAEtB,gBAFsB,CAEL,SAFK,EAEM,cAFN,EAEsB,eAFtB;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAGzB,IAHyB,CAA5B;IAID;EAjGH;IAAA;IAAA,OAkHE,qBAAY,OAAZ,EAA+B;MAC7B,IAAM,IAAI,GAAG,IAAb;MACA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,KAAc,QAAd,IAA0B,OAAO,CAAC,IAAR,KAAiB,QAA/C,EAAyD;QAGvD,IAAI,GAAG,QAAP;MACD,CAJD,MAIO,IAAI,KAAK,IAAL,IAAa,IAAb,IAAqB,OAAO,CAAC,IAAR,IAAgB,IAAzC,EAA+C;QAGpD,IAAI,GAAG,KAAK,IAAL,GAAY,OAAO,CAAC,IAA3B;MACD,CAJM,MAIA;QAGL,IAAI,GAAG,IAAP;MACD;;MACD,OAAO,qBAAqB,CACxB;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCACW,IAAI,CAAC,QAAL,EADX;;cAAA;gBAAA;gBAAA;gBAAA,iCAC8C,OAAO,CAAC,QAAR,EAD9C;;cAAA;gBAAA;gBAAA,+CAC4B,WAD5B;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CADwB,EAGxB,IAHwB,CAA5B;IAID;EAtIH;IAAA;IAAA,OAwJE,gBAAO,SAAP,EAAuC;MACrC,IAAM,IAAI,GAAG,IAAb;MACA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,KAAc,QAAlB,EAA4B;QAE1B,IAAI,GAAG,QAAP;MACD,CAHD,MAGO;QAGL,IAAI,GAAG,IAAP;MACD;;MACD,OAAO,qBAAqB,CAAC;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCACb,IAAI,CAAC,QAAL,EADa;;cAAA;gBAAA,iDACI,MADJ,CACW,UAAA,CAAC;kBAAA,OAAI,EAAE,CAAC,IAAH,CAAQ;oBAAA,OAAM,SAAS,CAAC,CAAD,CAAf;kBAAA,CAAR,CAAJ;gBAAA,CADZ;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAEzB,IAFyB,CAA5B;IAGD;EAtKH;IAAA;IAAA,OAwLE,sBAAmB,CAAnB;MAAA;QAAA;UAAA;YAAA;cAAA;cAAA,iCACgB,KAAK,QAAL,EADhB;;YAAA;cAAA,iDACiC,YADjC,CAC8C,CAD9C;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAxLF;IAAA;IAAA,OA2ME,aAAkC,SAAlC,EAA4D;MAC1D,IAAM,IAAI,GAAG,IAAb;MACA,OAAO,qBAAqB,CAAC;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCACb,IAAI,CAAC,QAAL,EADa;;cAAA;gBAAA,iDACI,GADJ,CACQ,UAAA,CAAC;kBAAA,OAAI,EAAE,CAAC,IAAH,CAAQ;oBAAA,OAAM,SAAS,CAAC,CAAD,CAAf;kBAAA,CAAR,CAAJ;gBAAA,CADT;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAEzB,KAAK,IAFoB,CAA5B;IAGD;EAhNH;IAAA;IAAA,OAyOE,kBAAuC,SAAvC,EAA0E;MAExE,IAAM,IAAI,GAAG,IAAb;MACA,OAAO,qBAAqB,CAAC;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCACb,IAAI,CAAC,QAAL,EADa;;cAAA;gBAAA,iDACI,QADJ,CACa,SADb;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAEzB,KAAK,IAFoB,CAA5B;IAGD;EA/OH;IAAA;IAAA,OA0PE,kBAAS,UAAT,EAA2B;MACzB,IAAI,UAAU,IAAI,IAAlB,EAAwB;QACtB,MAAM,IAAI,UAAJ,CACF,2DADE,CAAN;MAED;;MAED,IAAM,IAAI,GAAG,IAAb;MACA,OAAO,qBAAqB,CACxB;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCAAmB,IAAI,CAAC,QAAL,EAAnB;;cAAA;gBAAA,iDAAoC,QAApC,CAA6C,UAA7C;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CADwB,EACkC,KAAK,IADvC,CAA5B;IAED;EAnQH;IAAA;IAAA,OAuRE,gBAAO,KAAP,EAAqB;MACnB,IAAM,IAAI,GAAG,IAAb;MACA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,IAAa,IAAb,IAAqB,KAAK,GAAG,CAAjC,EAAoC;QAIlC,IAAI,GAAG,KAAK,IAAL,GAAY,KAAnB;MACD,CALD,MAKO,IAAI,KAAK,KAAK,CAAd,EAAiB;QAEtB,IAAI,GAAG,CAAP;MACD,CAHM,MAGA,IAAI,KAAK,IAAL,IAAa,IAAb,KAAsB,KAAK,KAAK,SAAV,IAAuB,KAAK,GAAG,CAArD,CAAJ,EAA6D;QAGlE,IAAI,GAAG,QAAP;MACD,CAJM,MAIA;QAEL,IAAI,GAAG,IAAP;MACD;;MACD,OAAO,qBAAqB,CAAC;QAAA;QAAA;UAAA;YAAA;cAAA;gBACrB,gBADqB,GACF,oBAAoB,CACzC;kBAAA;oBAAA;sBAAA;wBAAA;0BAAA;0BAAA,iCAA2B,IAAI,CAAC,QAAL,EAA3B;;wBAAA;0BAAA;0BAAA;4BAAc,KAAd;4BAA4C,IAA5C,EAAkD;0BAAlD;;wBAAA;wBAAA;0BAAA;sBAAA;oBAAA;kBAAA;gBAAA,CADyC,CADlB;gBAAA,kCAGpB,wBAAwB,CAAC,gBAAgB,CAAC,IAAjB,CAAsB,KAAtB,CAAD,CAHJ;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAIzB,IAJyB,CAA5B;IAKD;EA/SH;IAAA;IAAA,OAkUE,cAAK,KAAL,EAAkB;MAChB,IAAM,IAAI,GAAG,IAAb;MACA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,IAAa,IAAb,IAAqB,KAAK,IAAI,CAA9B,IAAmC,KAAK,IAAL,IAAa,KAApD,EAA2D;QAIzD,IAAI,GAAG,KAAK,IAAL,GAAY,KAAnB;MACD,CALD,MAKO,IACH,KAAK,IAAL,IAAa,IAAb,KACC,KAAK,IAAL,GAAY,KAAZ,IAAqB,KAAK,KAAK,SAA/B,IAA4C,KAAK,GAAG,CADrD,CADG,EAEsD;QAG3D,IAAI,GAAG,CAAP;MACD,CANM,MAMA;QAEL,IAAI,GAAG,IAAP;MACD;;MACD,OAAO,qBAAqB,CACxB;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCAAmB,IAAI,CAAC,QAAL,EAAnB;;cAAA;gBAAA,mDAAoC,IAApC,CAAyC,KAAzC;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CADwB,EACyB,IADzB,CAA5B;IAED;EAtVH;IAAA;IAAA,OAiXE,iBAAQ,UAAR,EAA4B,IAA5B,EAAwE;MAAA,IAA7B,sBAA6B,uEAAJ,IAAI;;MAEtE,IAAI,UAAU,IAAI,IAAd,IAAsB,UAAU,GAAG,CAAvC,EAA0C;QACxC,IAAI,KAAK,IAAL,IAAa,IAAjB,EAAuB;UACrB,MAAM,IAAI,UAAJ,CACF,0DADE,CAAN;QAED,CAHD,MAGO;UACL,MAAM,IAAI,UAAJ,CACF,+DACA,6DADA,GAEA,yDAFA,yCAGmC,KAAK,IAHxC,gBADE,CAAN;QAKD;MACF;;MACD,IAAM,IAAI,GAAG,IAAb;MACA,IAAM,MAAM,GAAG,UAAU,CAAC,IAAX,CAAgB,IAAI,IAAI,EAAE,CAAC,IAAH,CAAQ,GAAR,GAAc,QAAd,EAAxB,CAAf;MACA,OAAO,qBAAqB,CAAC;QAAA;QAAA;UAAA;YAAA;cAAA;gBACvB,KADuB,GACf,MAAM,CAAC,KAAP,EADe;;gBAE3B,IAAI,sBAAJ,EAA4B;kBAC1B,KAAK,IAAI,MAAM,CAAC,KAAP,EAAT;gBACD;;gBAJ0B;gBAAA,iCAKb,IAAI,CAAC,QAAL,EALa;;cAAA;gBAAA,mDAKI,OALJ,CAKY,UALZ,EAKwB,KAAK,CAAC,QAAN,EALxB;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CAAD,EAMzB,KAAK,IANoB,CAA5B;IAOD;EAxYH;IAAA;IAAA,OA2ZE,cAAK,KAAL,EAAkB;MAChB,IAAM,IAAI,GAAG,IAAb;MACA,IAAI,IAAJ;;MACA,IAAI,KAAK,IAAL,IAAa,IAAb,IAAqB,KAAK,IAAL,GAAY,KAArC,EAA4C;QAG1C,IAAI,GAAG,KAAP;MACD,CAJD,MAIO,IAAI,KAAK,IAAL,IAAa,IAAb,IAAqB,KAAK,IAAL,IAAa,KAAtC,EAA6C;QAGlD,IAAI,GAAG,KAAK,IAAZ;MACD,CAJM,MAIA;QAEL,IAAI,GAAG,IAAP;MACD;;MACD,OAAO,qBAAqB,CACxB;QAAA;UAAA;YAAA;cAAA;gBAAA;gBAAA,iCAAmB,IAAI,CAAC,QAAL,EAAnB;;cAAA;gBAAA,mDAAoC,IAApC,CAAyC,KAAzC;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA,CADwB,EACyB,IADzB,CAA5B;IAED;EA5aH;IAAA;IAAA,OA8bE;MAAA;QAAA;UAAA;YAAA;cAAA,MACM,KAAK,IAAL,KAAc,QADpB;gBAAA;gBAAA;cAAA;;cAAA,MAEU,IAAI,KAAJ,CAAU,gDAAV,CAFV;;YAAA;cAAA;cAAA,iCAIgB,KAAK,QAAL,EAJhB;;YAAA;cAAA,mDAIiC,OAJjC;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA9bF;IAAA;IAAA,OAgdE;MAAA;QAAA;UAAA;YAAA;cAAA,MACM,KAAK,IAAL,KAAc,QADpB;gBAAA;gBAAA;cAAA;;cAAA,MAEU,IAAI,KAAJ,CAAU,gDAAV,CAFV;;YAAA;cAAA;cAAA,iCAIgB,KAAK,QAAL,EAJhB;;YAAA;cAAA,mDAIiC,cAJjC;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAhdF;;EAAA;AAAA;AA0VkB,OAAA,CAAA,eAAA,GAAkB,KAAlB;AA0IlB,OAAM,SAAU,qBAAV,CACF,UADE,EAEiB;EAAA,IAAnB,IAAmB,uEAAJ,IAAI;EACrB,OAAO;IAAA;;IAAA;;IAAI,kBAAA;MAAA;;MAAA;;;MACT,MAAA,IAAA,GAAO,IAAP;MADS;IAUV;;IAVM;MAAA;MAAA,OAOL;QAAA;UAAA;YAAA;cAAA;gBAAA,mCACS,UAAU,EADnB;;cAAA;cAAA;gBAAA;YAAA;UAAA;QAAA;MAAA;IAPK;;IAAA;EAAA,EAAkB,OAAlB,IAAP;AAYD;AAoBD,OAAM,SAAU,KAAV,CAA8C,KAA9C,EAAwD;EAC5D,OAAO,qBAAqB,CACxB;IAAA;MAAA;QAAA;UAAA;YAAA,mCAAY,iBAAiB,CAAC,KAAD,CAA7B;;UAAA;UAAA;YAAA;QAAA;MAAA;IAAA;EAAA,CADwB,EACc,KAAK,CAAC,MADpB,CAA5B;AAED;AA2CD,OAAM,SAAU,GAAV,CAA4C,QAA5C,EAAsE;EAG1E,IAAI,CAAC,UAAU,CAAC,QAAD,CAAf,EAA2B;IACzB,MAAM,IAAI,KAAJ,CAAU,mDAAV,CAAN;EACD;;EACD,IAAI,IAAJ;;EACA,IAAI,KAAK,CAAC,OAAN,CAAc,QAAd,CAAJ,EAA6B;IAC3B,KAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,QAAQ,CAAC,MAA7B,EAAqC,CAAC,EAAtC,EAA0C;MACxC,IAAI,GAAG,IAAI,IAAI,IAAR,GAAgB,QAAQ,CAAC,CAAD,CAAR,CAA2B,IAA3C,GACe,IAAI,CAAC,GAAL,CAAS,IAAT,EAAgB,QAAQ,CAAC,CAAD,CAAR,CAA2B,IAA3C,CADtB;IAED;EACF,CALD,MAKO,IAAI,QAAQ,YAAY,MAAxB,EAAgC;IACrC,KAAK,IAAM,EAAX,IAAiB,QAAjB,EAA2B;MACzB,IAAI,GAAG,IAAI,IAAI,IAAR,GAAgB,QAAQ,CAAC,EAAD,CAAR,CAA4B,IAA5C,GACe,IAAI,CAAC,GAAL,CAAS,IAAT,EAAgB,QAAQ,CAAC,EAAD,CAAR,CAA4B,IAA5C,CADtB;IAED;EACF;;EACD,OAAO,qBAAqB,CAAI;IAAA;IAAA;MAAA;QAAA;UAAA;YAAA;YAAA,iCACR,kBAAkB,CAAC,QAAD,EAAW,UAAA,CAAC,EAAG;cACrD,IAAI,CAAC,YAAY,OAAjB,EAA0B;gBACxB,OAAO;kBAAC,KAAK,EAAE,CAAC,CAAC,QAAF,EAAR;kBAAsB,OAAO,EAAE;gBAA/B,CAAP;cACD,CAFD,MAEO,IAAI,UAAU,CAAC,CAAD,CAAd,EAAmB;gBACxB,OAAO;kBAAC,KAAK,EAAE,IAAR;kBAAc,OAAO,EAAE;gBAAvB,CAAP;cACD,CAFM,MAEA;gBACL,MAAM,IAAI,KAAJ,CACF,+DACA,iBAFE,CAAN;cAGD;YACF,CAVuC,CADV;;UAAA;YACxB,OADwB;YAAA,mCAYvB,kBAAkB,CAAI,OAAJ,EAAa,eAAe,CAAC,QAA7B,CAZK;;UAAA;UAAA;YAAA;QAAA;MAAA;IAAA;EAAA,CAAJ,EAazB,IAbyB,CAA5B;AAcD;;AAUD,SAAS,eAAT,CAAyB,IAAzB,EAAoC;EAClC,IAAI,IAAI,KAAK,IAAb,EAAmB;IACjB,OAAO,IAAP;EACD;;EAGD,IAAM,UAAU,GAAG,IAAI,CAAC,CAAD,CAAvB;;EAEA,IAAI,YAAY,CAAC,UAAD,CAAhB,EAA8B;IAE5B,IAAM,KAAK,GAAG,WAAW,CAAC,IAAD,CAAzB;IACA,OAAO;MAAC,KAAK,EAAL,KAAD;MAAQ,OAAO,EAAE;IAAjB,CAAP;EACD;;EAGD,OAAO;IAAC,KAAK,EAAE,IAAR;IAAc,OAAO,EAAE;EAAvB,CAAP;AACD;;AAMD,SAAS,WAAT,CAAwD,MAAxD,EAAmE;EAEjE,IAAI,MAAM,CAAC,MAAP,KAAkB,CAAtB,EAAyB;IAEvB,MAAM,IAAI,KAAJ,CAAU,uCAAV,CAAN;EACD;;EAED,IAAI,MAAM,CAAC,CAAD,CAAN,YAAqB,EAAE,CAAC,MAA5B,EAAoC;IAElC,OAAO,EAAE,CAAC,KAAH,CAAS,MAAT,CAAP;EACD,CAHD,MAGO;IAEL,OAAO,EAAE,CAAC,MAAH,CAAU,MAAV,CAAP;EACD;AACF","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *\n * =============================================================================\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { iteratorFromConcatenated, iteratorFromFunction, iteratorFromItems, iteratorFromZipped, ZipMismatchMode } from './iterators/lazy_iterator';\nimport { canTensorify, deepMapAndAwaitAll, isIterable } from './util/deep_map';\n// TODO(soergel): consider vectorized operations within the pipeline.\n/**\n * Represents a potentially large list of independent data elements (typically\n * 'samples' or 'examples').\n *\n * A 'data example' may be a primitive, an array, a map from string keys to\n * values, or any nested structure of these.\n *\n * A `Dataset` represents an ordered collection of elements, together with a\n * chain of transformations to be performed on those elements. Each\n * transformation is a method of `Dataset` that returns another `Dataset`, so\n * these may be chained, e.g.\n * `const processedDataset = rawDataset.filter(...).map(...).batch(...)`.\n *\n * Data loading and transformation is done in a lazy, streaming fashion.  The\n * dataset may be iterated over multiple times; each iteration starts the data\n * loading anew and recapitulates the transformations.\n *\n * A `Dataset` is typically processed as a stream of unbatched examples --i.e.,\n * its transformations are applied one example at a time. Batching produces a\n * new `Dataset` where each element is a batch. Batching should usually come\n * last in a pipeline, because data transformations are easier to express on a\n * per-example basis than on a per-batch basis.\n *\n * The following code examples are calling `await dataset.forEachAsync(...)` to\n * iterate once over the entire dataset in order to print out the data.\n *\n * @doc {heading: 'Data', subheading: 'Classes', namespace: 'data'}\n */\nexport class Dataset {\n    constructor() {\n        this.size = null;\n    }\n    // TODO(soergel): Make Datasets report whether repeated iterator() calls\n    // produce the same result (e.g., reading from a file) or different results\n    // (e.g., from the webcam).  Currently we don't make this distinction but it\n    // could be important for the user to know.\n    // abstract isDeterministic(): boolean;\n    /**\n     * Groups elements into batches.\n     *\n     * It is assumed that each of the incoming dataset elements has the same\n     * structure-- i.e. the same set of keys at each location in an object\n     * hierarchy.  For each key, the resulting `Dataset` provides a batched\n     * element collecting all of the incoming values for that key.\n     *\n     *  * Incoming primitives are grouped into a 1-D Tensor.\n     *  * Incoming Tensors are grouped into a new Tensor where the 0'th axis is\n     *    the batch dimension.\n     *  * Incoming arrays are converted to Tensor and then batched.\n     *  * A nested array is interpreted as an n-D Tensor, so the batched result\n     *    has n+1 dimensions.\n     *  * An array that cannot be converted to Tensor produces an error.\n     *\n     * If an array should not be batched as a unit, it should first be converted\n     * to an object with integer keys.\n     *\n     * Here are a few examples:\n     *\n     * Batch a dataset of numbers:\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8]).batch(4);\n     * await a.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of arrays:\n     * ```js\n     * const b = tf.data.array([[1], [2], [3], [4], [5], [6], [7], [8]]).batch(4);\n     * await b.forEachAsync(e => e.print());\n     * ```\n     *\n     * Batch a dataset of objects:\n     * ```js\n     * const c = tf.data.array([{a: 1, b: 11}, {a: 2, b: 12}, {a: 3, b: 13},\n     *   {a: 4, b: 14}, {a: 5, b: 15}, {a: 6, b: 16}, {a: 7, b: 17},\n     *   {a: 8, b: 18}]).batch(4);\n     * await c.forEachAsync(e => {\n     *   console.log('{');\n     *   for(var key in e) {\n     *     console.log(key+':');\n     *     e[key].print();\n     *   }\n     *   console.log('}');\n     * })\n     * ```\n     *\n     * @param batchSize The number of elements desired per batch.\n     * @param smallLastBatch Whether to emit the final batch when it has fewer\n     *   than batchSize elements. Default true.\n     * @returns A `Dataset`, from which a stream of batches can be obtained.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    batch(batchSize, smallLastBatch = true) {\n        const base = this;\n        tf.util.assert(batchSize > 0, () => `batchSize needs to be positive, but it is\n      ${batchSize}`);\n        let size;\n        if (this.size === Infinity || this.size == null) {\n            // If the size of this dataset is infinity or null, the new size keeps the\n            // same.\n            size = this.size;\n        }\n        else if (smallLastBatch) {\n            // If the size of this dataset is known and include small last batch, the\n            // new size is full batch count plus last batch.\n            size = Math.ceil(this.size / batchSize);\n        }\n        else {\n            // If the size of this dataset is known and not include small last batch,\n            // the new size is full batch count.\n            size = Math.floor(this.size / batchSize);\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator())\n                .columnMajorBatch(batchSize, smallLastBatch, deepBatchConcat);\n        }, size);\n    }\n    /**\n     * Concatenates this `Dataset` with another.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * const b = tf.data.array([4, 5, 6]);\n     * const c = a.concatenate(b);\n     * await c.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param dataset A `Dataset` to be concatenated onto this one.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    concatenate(dataset) {\n        const base = this;\n        let size;\n        if (this.size === Infinity || dataset.size === Infinity) {\n            // If the size of any of these two dataset is infinity, new size is\n            // infinity.\n            size = Infinity;\n        }\n        else if (this.size != null && dataset.size != null) {\n            // If the size of both datasets are known and not infinity, new size is\n            // sum the size of these two datasets.\n            size = this.size + dataset.size;\n        }\n        else {\n            // If neither of these two datasets has infinite size and any of these two\n            // datasets' size is null, the new size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).concatenate(await dataset.iterator()), size);\n    }\n    /**\n     * Filters this dataset according to `predicate`.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n     *   .filter(x => x%2 === 0);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param predicate A function mapping a dataset element to a boolean or a\n     * `Promise` for one.\n     *\n     * @returns A `Dataset` of elements for which the predicate was true.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    filter(predicate) {\n        const base = this;\n        let size;\n        if (this.size === Infinity) {\n            // If the size of this dataset is infinity, new size is infinity\n            size = Infinity;\n        }\n        else {\n            // If this dataset has limited elements, new size is null because it might\n            // exhausted randomly.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).filter(x => tf.tidy(() => predicate(x)));\n        }, size);\n    }\n    /**\n     * Apply a function to every element of the dataset.\n     *\n     * After the function is applied to a dataset element, any Tensors contained\n     * within that element are disposed.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param f A function to apply to each dataset element.\n     * @returns A `Promise` that resolves after all elements have been processed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async forEachAsync(f) {\n        return (await this.iterator()).forEachAsync(f);\n    }\n    /**\n     * Maps this dataset through a 1-to-1 transform.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).map(x => x*x);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a transformed\n     *   dataset element.\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    map(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).map(x => tf.tidy(() => transform(x)));\n        }, this.size);\n    }\n    /**\n     * Maps this dataset through an async 1-to-1 transform.\n     *\n     * ```js\n     * const a =\n     *  tf.data.array([1, 2, 3]).mapAsync(x => new Promise(function(resolve){\n     *    setTimeout(() => {\n     *      resolve(x * x);\n     *    }, Math.random()*1000 + 500);\n     *  }));\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @param transform A function mapping a dataset element to a `Promise` for a\n     *   transformed dataset element.  This transform is responsible for disposing\n     *   any intermediate `Tensor`s, i.e. by wrapping its computation in\n     *   `tf.tidy()`; that cannot be automated here (as it is in the synchronous\n     *   `map()` case).\n     *\n     * @returns A `Dataset` of transformed elements.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    mapAsync(transform) {\n        const base = this;\n        return datasetFromIteratorFn(async () => {\n            return (await base.iterator()).mapAsync(transform);\n        }, this.size);\n    }\n    /**\n     *  Creates a `Dataset` that prefetches elements from this dataset.\n     *\n     * @param bufferSize: An integer specifying the number of elements to be\n     *   prefetched.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    prefetch(bufferSize) {\n        if (bufferSize == null) {\n            throw new RangeError('`Dataset.prefetch()` requires bufferSize to be specified.');\n        }\n        const base = this;\n        return datasetFromIteratorFn(async () => (await base.iterator()).prefetch(bufferSize), this.size);\n    }\n    /**\n     * Repeats this dataset `count` times.\n     *\n     * NOTE: If this dataset is a function of global state (e.g. a random number\n     * generator), then different repetitions may produce different elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3]).repeat(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: (Optional) An integer, representing the number of times\n     *   the dataset should be repeated. The default behavior (if `count` is\n     *   `undefined` or negative) is for the dataset be repeated indefinitely.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    repeat(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count > 0) {\n            // If this dataset has size and count is positive, new size is current\n            // size multiply count. This also covers the case that current size is\n            // infinity.\n            size = this.size * count;\n        }\n        else if (count === 0) {\n            // If count is 0, new size is 0.\n            size = 0;\n        }\n        else if (this.size != null && (count === undefined || count < 0)) {\n            // If this dataset has size and count is undefined or negative, the\n            // dataset will be repeated indefinitely and new size is infinity.\n            size = Infinity;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => {\n            const iteratorIterator = iteratorFromFunction(async () => ({ value: await base.iterator(), done: false }));\n            return iteratorFromConcatenated(iteratorIterator.take(count));\n        }, size);\n    }\n    /**\n     * Creates a `Dataset` that skips `count` initial elements from this dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).skip(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be skipped\n     *   to form the new dataset.  If `count` is greater than the size of this\n     *   dataset, the new dataset will contain no elements.  If `count`\n     *   is `undefined` or negative, skips the entire dataset.\n     *\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    skip(count) {\n        const base = this;\n        let size;\n        if (this.size != null && count >= 0 && this.size >= count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is current size minus skipped size.This also covers the case that\n            // current size is infinity.\n            size = this.size - count;\n        }\n        else if (this.size != null &&\n            (this.size < count || count === undefined || count < 0)) {\n            // If the size of this dataset is smaller than count, or count is\n            // undefined or negative, skips the entire dataset and the new size is 0.\n            size = 0;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).skip(count), size);\n    }\n    /**\n     * Pseudorandomly shuffles the elements of this dataset. This is done in a\n     * streaming manner, by sampling from a given number of prefetched elements.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).shuffle(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param bufferSize: An integer specifying the number of elements from this\n     *   dataset from which the new dataset will sample.\n     * @param seed: (Optional) An integer specifying the random seed that will\n     *   be used to create the distribution.\n     * @param reshuffleEachIteration: (Optional) A boolean, which if true\n     *   indicates that the dataset should be pseudorandomly reshuffled each time\n     *   it is iterated over. If false, elements will be returned in the same\n     *   shuffled order on each iteration. (Defaults to `true`.)\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    shuffle(bufferSize, seed, reshuffleEachIteration = true) {\n        if (bufferSize == null || bufferSize < 0) {\n            if (this.size == null) {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.');\n            }\n            else {\n                throw new RangeError('`Dataset.shuffle()` requires bufferSize to be specified.  ' +\n                    'If your data fits in main memory (for regular JS objects), ' +\n                    'and/or GPU memory (for `tf.Tensor`s), consider setting ' +\n                    `bufferSize to the dataset size (${this.size} elements)`);\n            }\n        }\n        const base = this;\n        const random = seedrandom.alea(seed || tf.util.now().toString());\n        return datasetFromIteratorFn(async () => {\n            let seed2 = random.int32();\n            if (reshuffleEachIteration) {\n                seed2 += random.int32();\n            }\n            return (await base.iterator()).shuffle(bufferSize, seed2.toString());\n        }, this.size);\n    }\n    /**\n     * Creates a `Dataset` with at most `count` initial elements from this\n     * dataset.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]).take(3);\n     * await a.forEachAsync(e => console.log(e));\n     * ```\n     *\n     * @param count: The number of elements of this dataset that should be taken\n     *   to form the new dataset.  If `count` is `undefined` or negative, or if\n     *   `count` is greater than the size of this dataset, the new dataset will\n     *   contain all elements of this dataset.\n     * @returns A `Dataset`.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    take(count) {\n        const base = this;\n        let size;\n        if (this.size != null && this.size > count) {\n            // If the size of this dataset is greater than count, the new dataset's\n            // size is count.\n            size = count;\n        }\n        else if (this.size != null && this.size <= count) {\n            // If the size of this dataset is equal or smaller than count, the new\n            // dataset's size is the size of this dataset.\n            size = this.size;\n        }\n        else {\n            // If the size of this dataset is null, the new dataset's size is null.\n            size = null;\n        }\n        return datasetFromIteratorFn(async () => (await base.iterator()).take(count), size);\n    }\n    /**\n     * Collect all elements of this dataset into an array.\n     *\n     * Obviously this will succeed only for small datasets that fit in memory.\n     * Useful for testing and generally should be avoided if possible.\n     *\n     * ```js\n     * const a = tf.data.array([1, 2, 3, 4, 5, 6]);\n     * console.log(await a.toArray());\n     * ```\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     *\n     * @doc {heading: 'Data', subheading: 'Classes'}\n     */\n    async toArray() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArray();\n    }\n    /**\n     * Collect all elements of this dataset into an array with prefetching 100\n     * elements. This is useful for testing, because the prefetch changes the\n     * order in which the Promises are resolved along the processing pipeline.\n     * This may help expose bugs where results are dependent on the order of\n     * Promise resolution rather than on the logical order of the stream (i.e.,\n     * due to hidden mutable state).\n     *\n     * @returns A Promise for an array of elements, which will resolve\n     *   when a new stream has been obtained and fully consumed.\n     */\n    async toArrayForTest() {\n        if (this.size === Infinity) {\n            throw new Error('Can not convert infinite data stream to array.');\n        }\n        return (await this.iterator()).toArrayForTest();\n    }\n}\n// TODO(soergel): deep sharded shuffle, where supported\nDataset.MAX_BUFFER_SIZE = 10000;\n/**\n * Create a `Dataset` defined by a provided iterator() function.\n *\n * ```js\n * let i = -1;\n * const func = () =>\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\n * const iter = tf.data.iteratorFromFunction(func);\n * const ds = tf.data.datasetFromIteratorFn(iter);\n * await ds.forEachAsync(e => console.log(e));\n * ```\n */\nexport function datasetFromIteratorFn(iteratorFn, size = null) {\n    return new class extends Dataset {\n        constructor() {\n            super(...arguments);\n            this.size = size;\n        }\n        /*\n         * Provide a new stream of elements.  Note this will also start new streams\n         * from any underlying `Dataset`s.\n         */\n        async iterator() {\n            return iteratorFn();\n        }\n    }();\n}\n/**\n * Create a `Dataset` from an array of elements.\n *\n * Create a Dataset from an array of objects:\n * ```js\n * const a = tf.data.array([{'item': 1}, {'item': 2}, {'item': 3}]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n *\n * Create a Dataset from an array of numbers:\n * ```js\n * const a = tf.data.array([4, 5, 6]);\n * await a.forEachAsync(e => console.log(e));\n * ```\n * @param items An array of elements that will be parsed as items in a dataset.\n *\n * @doc {heading: 'Data', subheading: 'Creation', namespace: 'data'}\n */\nexport function array(items) {\n    return datasetFromIteratorFn(async () => iteratorFromItems(items), items.length);\n}\n/**\n * Create a `Dataset` by zipping together an array, dict, or nested\n * structure of `Dataset`s (and perhaps additional constants).\n * The underlying datasets must provide elements in a consistent order such that\n * they correspond.\n *\n * The number of elements in the resulting dataset is the same as the size of\n * the smallest dataset in datasets.\n *\n * The nested structure of the `datasets` argument determines the\n * structure of elements in the resulting iterator.\n *\n * Note this means that, given an array of two datasets that produce dict\n * elements, the result is a dataset that produces elements that are arrays\n * of two dicts:\n *\n * Zip an array of datasets:\n * ```js\n * console.log('Zip two datasets of objects:');\n * const ds1 = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const ds2 = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const ds3 = tf.data.zip([ds1, ds2]);\n * await ds3.forEachAsync(e => console.log(JSON.stringify(e)));\n *\n * // If the goal is to merge the dicts in order to produce elements like\n * // {a: ..., b: ...}, this requires a second step such as:\n * console.log('Merge the objects:');\n * const ds4 = ds3.map(x => {return {a: x[0].a, b: x[1].b}});\n * await ds4.forEachAsync(e => console.log(e));\n * ```\n *\n * Zip a dict of datasets:\n * ```js\n * const a = tf.data.array([{a: 1}, {a: 2}, {a: 3}]);\n * const b = tf.data.array([{b: 4}, {b: 5}, {b: 6}]);\n * const c = tf.data.zip({c: a, d: b});\n * await c.forEachAsync(e => console.log(JSON.stringify(e)));\n * ```\n *\n * @doc {heading: 'Data', subheading: 'Operations', namespace: 'data'}\n */\nexport function zip(datasets) {\n    // manually type-check the argument for JS users\n    if (!isIterable(datasets)) {\n        throw new Error('The argument to zip() must be an object or array.');\n    }\n    let size;\n    if (Array.isArray(datasets)) {\n        for (let i = 0; i < datasets.length; i++) {\n            size = size == null ? datasets[i].size :\n                Math.min(size, datasets[i].size);\n        }\n    }\n    else if (datasets instanceof Object) {\n        for (const ds in datasets) {\n            size = size == null ? datasets[ds].size :\n                Math.min(size, datasets[ds].size);\n        }\n    }\n    return datasetFromIteratorFn(async () => {\n        const streams = await deepMapAndAwaitAll(datasets, d => {\n            if (d instanceof Dataset) {\n                return { value: d.iterator(), recurse: false };\n            }\n            else if (isIterable(d)) {\n                return { value: null, recurse: true };\n            }\n            else {\n                throw new Error('Leaves of the structure passed to zip() must be Datasets, ' +\n                    'not primitives.');\n            }\n        });\n        return iteratorFromZipped(streams, ZipMismatchMode.SHORTEST);\n    }, size);\n}\n/**\n * A zip function for use with deepZip, passed via the columnMajorBatch call.\n *\n * Accepts an array of identically-structured nested elements and either batches\n * them (if they are primitives, numeric arrays, or Tensors) or requests\n * recursion (if not).\n */\n// tslint:disable-next-line:no-any\nfunction deepBatchConcat(rows) {\n    if (rows === null) {\n        return null;\n    }\n    // use the first item to decide whether to recurse or batch here.\n    const exampleRow = rows[0];\n    if (canTensorify(exampleRow)) {\n        // rows is an array of primitives, Tensors, or arrays.  Batch them.\n        const value = batchConcat(rows);\n        return { value, recurse: false };\n    }\n    // the example row is an object, so recurse into it.\n    return { value: null, recurse: true };\n}\n/**\n * Assembles a list of same-shaped numbers, number arrays, or Tensors\n * into a single new Tensor where axis 0 is the batch dimension.\n */\nfunction batchConcat(arrays) {\n    if (arrays.length === 0) {\n        // We can't return an empty Tensor because we don't know the element shape.\n        throw new Error('Can\\'t make a batch of zero elements.');\n    }\n    if (arrays[0] instanceof tf.Tensor) {\n        // Input is an array of Tensors\n        return tf.stack(arrays);\n    }\n    else {\n        // Input is a possibly-nested array of numbers.\n        return tf.tensor(arrays);\n    }\n}\n//# sourceMappingURL=dataset.js.map"]},"metadata":{},"sourceType":"module"}