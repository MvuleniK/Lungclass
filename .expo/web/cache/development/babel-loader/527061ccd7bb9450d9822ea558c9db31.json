{"ast":null,"code":"import _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\nimport _regeneratorRuntime from \"@babel/runtime/regenerator\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose as _dispose } from \"../globals\";\nimport { variableGrads } from \"../gradients\";\nimport { scalar } from \"../ops/ops\";\nimport { Serializable } from \"../serialization\";\nexport var Optimizer = function (_Serializable) {\n  _inherits(Optimizer, _Serializable);\n\n  var _super = _createSuper(Optimizer);\n\n  function Optimizer() {\n    _classCallCheck(this, Optimizer);\n\n    return _super.apply(this, arguments);\n  }\n\n  _createClass(Optimizer, [{\n    key: \"minimize\",\n    value: function minimize(f) {\n      var returnCost = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      var varList = arguments.length > 2 ? arguments[2] : undefined;\n\n      var _this$computeGradient = this.computeGradients(f, varList),\n          value = _this$computeGradient.value,\n          grads = _this$computeGradient.grads;\n\n      if (varList != null) {\n        var gradArray = varList.map(function (v) {\n          return {\n            name: v.name,\n            tensor: grads[v.name]\n          };\n        });\n        this.applyGradients(gradArray);\n      } else {\n        this.applyGradients(grads);\n      }\n\n      _dispose(grads);\n\n      if (returnCost) {\n        return value;\n      } else {\n        value.dispose();\n        return null;\n      }\n    }\n  }, {\n    key: \"iterations\",\n    get: function get() {\n      if (this.iterations_ == null) {\n        this.iterations_ = 0;\n      }\n\n      return this.iterations_;\n    }\n  }, {\n    key: \"incrementIterations\",\n    value: function incrementIterations() {\n      this.iterations_ = this.iterations + 1;\n    }\n  }, {\n    key: \"computeGradients\",\n    value: function computeGradients(f, varList) {\n      return variableGrads(f, varList);\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.iterations_ != null) {\n        _dispose(this.iterations_);\n      }\n    }\n  }, {\n    key: \"saveIterations\",\n    value: function saveIterations() {\n      return _regeneratorRuntime.async(function saveIterations$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              if (this.iterations_ == null) {\n                this.iterations_ = 0;\n              }\n\n              return _context.abrupt(\"return\", {\n                name: 'iter',\n                tensor: scalar(this.iterations_, 'int32')\n              });\n\n            case 2:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return _regeneratorRuntime.async(function getWeights$(_context2) {\n        while (1) {\n          switch (_context2.prev = _context2.next) {\n            case 0:\n              throw new Error('getWeights() is not implemented for this optimizer yet.');\n\n            case 1:\n            case \"end\":\n              return _context2.stop();\n          }\n        }\n      }, null, null, null, Promise);\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weightValues) {\n      return _regeneratorRuntime.async(function setWeights$(_context3) {\n        while (1) {\n          switch (_context3.prev = _context3.next) {\n            case 0:\n              throw new Error(\"setWeights() is not implemented for this optimizer class \" + (\"\" + this.getClassName()));\n\n            case 1:\n            case \"end\":\n              return _context3.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"extractIterations\",\n    value: function extractIterations(weightValues) {\n      return _regeneratorRuntime.async(function extractIterations$(_context4) {\n        while (1) {\n          switch (_context4.prev = _context4.next) {\n            case 0:\n              _context4.next = 2;\n              return _regeneratorRuntime.awrap(weightValues[0].tensor.data());\n\n            case 2:\n              this.iterations_ = _context4.sent[0];\n              return _context4.abrupt(\"return\", weightValues.slice(1));\n\n            case 4:\n            case \"end\":\n              return _context4.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }]);\n\n  return Optimizer;\n}(Serializable);\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: function value(instance) {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["../../src/optimizers/optimizer.ts"],"names":[],"mappings":";;;;;;;;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,OAAO,IAAP,QAAR;AACA,SAAQ,aAAR;AACA,SAAQ,MAAR;AACA,SAAQ,YAAR;AAqBA,WAAsB,SAAtB;EAAA;;EAAA;;EAAA;IAAA;;IAAA;EAAA;;EAAA;IAAA;IAAA,OAiBE,kBAAS,CAAT,EAAkE;MAAA,IAAxC,UAAwC,uEAA3B,KAA2B;MAAA,IAApB,OAAoB;;MAEhE,4BAAuB,KAAK,gBAAL,CAAsB,CAAtB,EAAyB,OAAzB,CAAvB;MAAA,IAAO,KAAP,yBAAO,KAAP;MAAA,IAAc,KAAd,yBAAc,KAAd;;MAEA,IAAI,OAAO,IAAI,IAAf,EAAqB;QACnB,IAAM,SAAS,GACX,OAAO,CAAC,GAAR,CAAY,UAAA,CAAC;UAAA,OAAK;YAAC,IAAI,EAAE,CAAC,CAAC,IAAT;YAAe,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAH;UAA5B,CAAL;QAAA,CAAb,CADJ;QAEA,KAAK,cAAL,CAAoB,SAApB;MACD,CAJD,MAIO;QACL,KAAK,cAAL,CAAoB,KAApB;MACD;;MAGD,QAAO,CAAC,KAAD,CAAP;;MAEA,IAAI,UAAJ,EAAgB;QACd,OAAO,KAAP;MACD,CAFD,MAEO;QACL,KAAK,CAAC,OAAN;QACA,OAAO,IAAP;MACD;IACF;EAtCH;IAAA;IAAA,KA2CE,eAAc;MACZ,IAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;QAC5B,KAAK,WAAL,GAAmB,CAAnB;MACD;;MACD,OAAO,KAAK,WAAZ;IACD;EAhDH;IAAA;IAAA,OAkDY,+BAAmB;MAC3B,KAAK,WAAL,GAAmB,KAAK,UAAL,GAAkB,CAArC;IACD;EApDH;IAAA;IAAA,OAmEE,0BAAiB,CAAjB,EAAkC,OAAlC,EAAsD;MAEpD,OAAO,aAAa,CAAC,CAAD,EAAI,OAAJ,CAApB;IACD;EAtEH;IAAA;IAAA,OAqFE,mBAAO;MACL,IAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;QAC5B,QAAO,CAAC,KAAK,WAAN,CAAP;MACD;IACF;EAzFH;IAAA;IAAA,OA2FE;MAAA;QAAA;UAAA;YAAA;cACE,IAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;gBAC5B,KAAK,WAAL,GAAmB,CAAnB;cACD;;cAHH,iCAIS;gBACL,IAAI,EAAE,MADD;gBAGL,MAAM,EAAE,MAAM,CAAC,KAAK,WAAN,EAAmB,OAAnB;cAHT,CAJT;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA3FF;IAAA;IAAA,OAsGE;MAAA;QAAA;UAAA;YAAA;cAAA,MACQ,IAAI,KAAJ,CAAU,yDAAV,CADR;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAtGF;IAAA;IAAA,OA0GE,oBAAiB,YAAjB;MAAA;QAAA;UAAA;YAAA;cAAA,MACQ,IAAI,KAAJ,CACF,oEACG,KAAK,YAAL,EADH,CADE,CADR;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA1GF;IAAA;IAAA,OAuHY,2BAAwB,YAAxB;MAAA;QAAA;UAAA;YAAA;cAAA;cAAA,iCAEkB,YAAY,CAAC,CAAD,CAAZ,CAAgB,MAAhB,CAAuB,IAAvB,EAFlB;;YAAA;cAER,KAAK,WAFG,kBAEiD,CAFjD;cAAA,kCAGD,YAAY,CAAC,KAAb,CAAmB,CAAnB,CAHC;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAvHZ;;EAAA;AAAA,EAAwC,YAAxC;AA8HA,MAAM,CAAC,cAAP,CAAsB,SAAtB,EAAiC,MAAM,CAAC,WAAxC,EAAqD;EACnD,KAAK,EAAE,eAAC,QAAD,EAAwB;IAC7B,OAAO,QAAQ,CAAC,QAAT,IAAqB,IAArB,IAA6B,QAAQ,CAAC,gBAAT,IAA6B,IAA1D,IACH,QAAQ,CAAC,cAAT,IAA2B,IAD/B;EAED;AAJkD,CAArD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\nexport class Optimizer extends Serializable {\n    /**\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\n     * gradients of y with respect to the list of trainable variables provided by\n     * `varList`. If no list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to minimize.\n     * @param returnCost Whether to return the scalar cost value produced by\n     * executing `f()`.\n     * @param varList An optional list of variables to update. If specified, only\n     * the trainable variables in varList will be updated by minimize. Defaults to\n     * all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    minimize(f, returnCost = false, varList) {\n        const { value, grads } = this.computeGradients(f, varList);\n        if (varList != null) {\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\n            this.applyGradients(gradArray);\n        }\n        else {\n            this.applyGradients(grads);\n        }\n        // Dispose gradients.\n        dispose(grads);\n        if (returnCost) {\n            return value;\n        }\n        else {\n            value.dispose();\n            return null;\n        }\n    }\n    /**\n     * The number of iterations that this optimizer instance has been invoked for.\n     */\n    get iterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return this.iterations_;\n    }\n    incrementIterations() {\n        this.iterations_ = this.iterations + 1;\n    }\n    /**\n     * Executes f() and computes the gradient of the scalar output of f() with\n     * respect to the list of trainable variables provided by `varList`. If no\n     * list is provided, it defaults to all trainable variables.\n     *\n     * @param f The function to execute and whose output to use for computing\n     * gradients with respect to variables.\n     * @param varList An optional list of variables to compute gradients with\n     * respect to. If specified, only the trainable variables in varList will have\n     * gradients computed with respect to. Defaults to all trainable variables.\n     *\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\n     */\n    computeGradients(f, varList) {\n        return variableGrads(f, varList);\n    }\n    /**\n     * Dispose the variables (if any) owned by this optimizer instance.\n     */\n    dispose() {\n        if (this.iterations_ != null) {\n            dispose(this.iterations_);\n        }\n    }\n    async saveIterations() {\n        if (this.iterations_ == null) {\n            this.iterations_ = 0;\n        }\n        return {\n            name: 'iter',\n            // TODO(cais): Use 'int64' type when available.\n            tensor: scalar(this.iterations_, 'int32')\n        };\n    }\n    async getWeights() {\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\n    }\n    async setWeights(weightValues) {\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\n            `${this.getClassName()}`);\n    }\n    /**\n     * Extract the first element of the weight values and set it\n     * as the iterations counter variable of this instance of optimizer.\n     *\n     * @param weightValues\n     * @returns Weight values with the first element consumed and excluded.\n     */\n    async extractIterations(weightValues) {\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\n        return weightValues.slice(1);\n    }\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n    value: (instance) => {\n        return instance.minimize != null && instance.computeGradients != null &&\n            instance.applyGradients != null;\n    }\n});\n//# sourceMappingURL=optimizer.js.map"]},"metadata":{},"sourceType":"module"}