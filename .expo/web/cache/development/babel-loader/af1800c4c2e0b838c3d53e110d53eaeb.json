{"ast":null,"code":"import _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\n\n/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from \"../../engine\";\nimport { customGrad } from \"../../gradients\";\nimport { _FusedMatMul } from \"../../kernel_names\";\nimport { makeTypesMatch } from \"../../tensor_util\";\nimport { convertToTensor } from \"../../tensor_util_env\";\nimport * as util from \"../../util\";\nimport { add } from \"../add\";\nimport * as broadcast_util from \"../broadcast_util\";\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from \"../fused_util\";\nimport { matMul as unfusedMatMul } from \"../mat_mul\";\nimport { op } from \"../operation\";\nimport { reshape } from \"../reshape\";\n\nfunction fusedMatMul_(_ref) {\n  var a = _ref.a,\n      b = _ref.b,\n      _ref$transposeA = _ref.transposeA,\n      transposeA = _ref$transposeA === void 0 ? false : _ref$transposeA,\n      _ref$transposeB = _ref.transposeB,\n      transposeB = _ref$transposeB === void 0 ? false : _ref$transposeB,\n      bias = _ref.bias,\n      _ref$activation = _ref.activation,\n      activation = _ref$activation === void 0 ? 'linear' : _ref$activation,\n      preluActivationWeights = _ref.preluActivationWeights,\n      leakyreluAlpha = _ref.leakyreluAlpha;\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    var result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n  }\n\n  var $a = convertToTensor(a, 'a', 'fused matMul');\n  var $b = convertToTensor(b, 'b', 'fused matMul');\n\n  var _makeTypesMatch = makeTypesMatch($a, $b);\n\n  var _makeTypesMatch2 = _slicedToArray(_makeTypesMatch, 2);\n\n  $a = _makeTypesMatch2[0];\n  $b = _makeTypesMatch2[1];\n  var innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  var innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  var outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  var outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  var outerDimsA = $a.shape.slice(0, -2);\n  var outerDimsB = $b.shape.slice(0, -2);\n  var batchDimA = util.sizeFromShape(outerDimsA);\n  var batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, function () {\n    return \"Error in fused matMul: inputs must have the same rank of at \" + (\"least 2, got ranks \" + $a.rank + \" and \" + $b.rank + \".\");\n  });\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), function () {\n    return \"Error in fused matMul: outer dimensions (\" + outerDimsA + \") and (\" + (outerDimsB + \") of Tensors with shapes \" + $a.shape + \" and \") + ($b.shape + \" must match.\");\n  });\n  util.assert(innerShapeA === innerShapeB, function () {\n    return \"Error in fused matMul: inner shapes (\" + innerShapeA + \") and (\" + (innerShapeB + \") of Tensors with shapes \" + $a.shape + \" and \") + ($b.shape + \" and transposeA=\" + transposeA) + (\" and transposeB=\" + transposeB + \" must match.\");\n  });\n  var outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  var a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  var b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  var $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n\n    var _makeTypesMatch3 = makeTypesMatch($bias, $a);\n\n    var _makeTypesMatch4 = _slicedToArray(_makeTypesMatch3, 1);\n\n    $bias = _makeTypesMatch4[0];\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  var $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  var grad = function grad(dy, saved) {\n    var _saved = _slicedToArray(saved, 4),\n        a3D = _saved[0],\n        b3D = _saved[1],\n        y = _saved[2],\n        $bias = _saved[3];\n\n    var dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    var aDer;\n    var bDer;\n\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n\n    if (bias != null) {\n      var biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n\n  var inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  var attrs = {\n    transposeA: transposeA,\n    transposeB: transposeB,\n    activation: activation,\n    leakyreluAlpha: leakyreluAlpha\n  };\n\n  if (bias == null) {\n    var customOp = customGrad(function (a3D, b3D, save) {\n      var res = ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    var customOpWithBias = customGrad(function (a3D, b3D, $bias, save) {\n      var res = ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\n\nexport var matMul = op({\n  fusedMatMul_: fusedMatMul_\n});","map":{"version":3,"sources":["../../../src/ops/fused/mat_mul.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR;AACA,SAAQ,UAAR;AACA,SAAQ,YAAR;AAIA,SAAQ,cAAR;AACA,SAAQ,eAAR;AAEA,OAAO,KAAK,IAAZ;AAEA,SAAQ,GAAR;AACA,OAAO,KAAK,cAAZ;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,EAA+C,oBAA/C,EAAqE,UAArE;AACA,SAAQ,MAAM,IAAI,aAAlB;AACA,SAAQ,EAAR;AACA,SAAQ,OAAR;;AAuBA,SAAS,YAAT,OAkBC;EAAA,IAjBC,CAiBD,QAjBC,CAiBD;EAAA,IAhBC,CAgBD,QAhBC,CAgBD;EAAA,2BAfC,UAeD;EAAA,IAfC,UAeD,gCAfc,KAed;EAAA,2BAdC,UAcD;EAAA,IAdC,UAcD,gCAdc,KAcd;EAAA,IAbC,IAaD,QAbC,IAaD;EAAA,2BAZC,UAYD;EAAA,IAZC,UAYD,gCAZc,QAYd;EAAA,IAXC,sBAWD,QAXC,sBAWD;EAAA,IAVC,cAUD,QAVC,cAUD;;EACG,IAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;IAChE,IAAI,MAAM,GAAG,aAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,UAAP,EAAmB,UAAnB,CAA1B;;IACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;IACD;;IAED,OAAO,eAAe,CACX,MADW,EACH,UADG,EACS,sBADT,EACiC,cADjC,CAAtB;EAGD;;EAED,IAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;EACA,IAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;;EAbH,sBAcc,cAAc,CAAC,EAAD,EAAK,EAAL,CAd5B;;EAAA;;EAcI,EAdJ;EAcQ,EAdR;EAgBG,IAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;EAEA,IAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;EAGA,IAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;EAEA,IAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;EAGA,IAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;EACA,IAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;EACA,IAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;EACA,IAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;EAEA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,IAAW,CAAX,IAAgB,EAAE,CAAC,IAAH,IAAW,CAA3B,IAAgC,EAAE,CAAC,IAAH,KAAY,EAAE,CAAC,IADnD,EAEI;IAAA,OAAM,0FACoB,EAAE,CAAC,IADvB,aACmC,EAAE,CAAC,IADtC,OAAN;EAAA,CAFJ;EAKA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,WAAL,CAAiB,UAAjB,EAA6B,UAA7B,CADJ,EAEI;IAAA,OAAM,8CAA4C,UAA5C,gBACC,UADD,iCACuC,EAAE,CAAC,KAD1C,eAEC,EAAE,CAAC,KAFJ,kBAAN;EAAA,CAFJ;EAMA,IAAI,CAAC,MAAL,CACI,WAAW,KAAK,WADpB,EAEI;IAAA,OAAM,0CAAwC,WAAxC,gBACC,WADD,iCACwC,EAAE,CAAC,KAD3C,eAEC,EAAE,CAAC,KAFJ,wBAE4B,UAF5B,0BAGiB,UAHjB,kBAAN;EAAA,CAFJ;EAOA,IAAM,QAAQ,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,EAAsB,MAAtB,CAA6B,CAAC,WAAD,EAAc,WAAd,CAA7B,CAAjB;EAEA,IAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CADqB,GAE5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CAFX;EAGA,IAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CADqB,GAE5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CAFX;EAIA,IAAI,KAAJ;;EACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;;IADgB,uBAEN,cAAc,CAAC,KAAD,EAAQ,EAAR,CAFR;;IAAA;;IAEf,KAFe;IAIhB,cAAc,CAAC,0BAAf,CAA0C,QAA1C,EAAoD,KAAK,CAAC,KAA1D;EACD;;EAED,IAAI,uBAAJ;;EACA,IAAI,sBAAsB,IAAI,IAA9B,EAAoC;IAClC,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;EAED;;EAED,IAAM,IAAI,GAAG,SAAP,IAAO,CAAC,EAAD,EAAe,KAAf,EAAkC;IAC7C,4BAA6B,KAA7B;IAAA,IAAO,GAAP;IAAA,IAAY,GAAZ;IAAA,IAAiB,CAAjB;IAAA,IAAoB,KAApB;;IAIA,IAAM,YAAY,GACd,oBAAoB,CAAC,OAAO,CAAC,EAAD,EAAK,CAAC,CAAC,KAAP,CAAR,EAAuB,CAAvB,EAA0B,UAA1B,CADxB;IAEA,IAAI,IAAJ;IACA,IAAI,IAAJ;;IAEA,IAAI,CAAC,UAAD,IAAe,CAAC,UAApB,EAAgC;MAC9B,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,KAApB,EAA2B,IAA3B,CAApB;MACA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,IAApB,EAA0B,KAA1B,CAApB;IACD,CAHD,MAGO,IAAI,CAAC,UAAD,IAAe,UAAnB,EAA+B;MACpC,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,KAApB,EAA2B,KAA3B,CAApB;MACA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,IAApB,EAA0B,KAA1B,CAApB;IACD,CAHM,MAGA,IAAI,UAAU,IAAI,CAAC,UAAnB,EAA+B;MACpC,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,KAApB,EAA2B,IAA3B,CAApB;MACA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,KAApB,EAA2B,KAA3B,CAApB;IACD,CAHM,MAGA;MACL,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,IAApB,EAA0B,IAA1B,CAApB;MACA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,IAApB,EAA0B,IAA1B,CAApB;IACD;;IAED,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAM,OAAO,GAAG,oBAAoB,CAAC,KAAD,EAAQ,YAAR,CAApC;MACA,OAAO,CAAC,IAAD,EAAO,IAAP,EAAa,OAAb,CAAP;IACD,CAHD,MAGO;MACL,OAAO,CAAC,IAAD,EAAO,IAAP,CAAP;IACD;EACF,CA9BD;;EAgCA,IAAM,MAAM,GAAuB;IACjC,CAAC,EAAE,GAD8B;IAEjC,CAAC,EAAE,GAF8B;IAGjC,IAAI,EAAE,KAH2B;IAIjC,sBAAsB,EAAE;EAJS,CAAnC;EAMA,IAAM,KAAK,GACP;IAAC,UAAU,EAAV,UAAD;IAAa,UAAU,EAAV,UAAb;IAAyB,UAAU,EAAV,UAAzB;IAAqC,cAAc,EAAd;EAArC,CADJ;;EAKA,IAAI,IAAI,IAAI,IAAZ,EAAkB;IAChB,IAAM,QAAQ,GACV,UAAU,CAAC,UAAC,GAAD,EAAgB,GAAhB,EAA+B,IAA/B,EAAqD;MAC9D,IAAM,GAAG,GAEL,MAAM,CAAC,SAAP,CACI,YADJ,EACkB,MADlB,EAEI,KAFJ,CAFJ;MAMA,IAAI,CAAC,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,CAAD,CAAJ;MAEA,OAAO;QAAC,KAAK,EAAE,OAAO,CAAC,GAAD,EAAM,QAAN,CAAf;QAAgC,QAAQ,EAAE;MAA1C,CAAP;IACD,CAVS,CADd;IAYA,OAAO,QAAQ,CAAC,GAAD,EAAM,GAAN,CAAf;EACD,CAdD,MAcO;IACL,IAAM,gBAAgB,GAAG,UAAU,CAC/B,UAAC,GAAD,EAAgB,GAAhB,EAA+B,KAA/B,EAA8C,IAA9C,EAAoE;MAClE,IAAM,GAAG,GAEL,MAAM,CAAC,SAAP,CACI,YADJ,EACkB,MADlB,EAEI,KAFJ,CAFJ;MAMA,IAAI,CAAC,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,EAAgB,KAAhB,CAAD,CAAJ;MAEA,OAAO;QAAC,KAAK,EAAE,OAAO,CAAC,GAAD,EAAM,QAAN,CAAf;QAAgC,QAAQ,EAAE;MAA1C,CAAP;IACD,CAX8B,CAAnC;IAaA,OAAO,gBAAgB,CAAC,GAAD,EAAM,GAAN,EAAW,KAAX,CAAvB;EACD;AACF;;AAED,OAAO,IAAM,MAAM,GAAG,EAAE,CAAC;EAAC,YAAY,EAAZ;AAAD,CAAD,CAAjB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\n * Computes the dot product of two matrices with optional activation and bias.\n *\n * ```js\n * const a = tf.tensor2d([-1, -2], [1, 2]);\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\n * const bias = tf.tensor2d([1, 2], [1, 2]);\n *\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\n * ```\n *\n * @param obj An object with the following properties:\n * - `a` First matrix in dot product operation.\n * - `b` Second matrix in dot product operation.\n * - `transposeA` If true, `a` is transposed before multiplication.\n * - `transposeB` If true, `b` is transposed before multiplication.\n * - `bias` Matrix to be added to the result.\n * - `activation` Name of activation kernel (defaults to `linear`).\n * - `preluActivationWeights` Tensor of prelu weights.\n * - `leakyreluAlpha` Alpha of leakyrelu.\n */\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights, leakyreluAlpha, }) {\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\n        if (bias != null) {\n            result = add(result, bias);\n        }\n        return applyActivation(result, activation, preluActivationWeights, leakyreluAlpha);\n    }\n    let $a = convertToTensor(a, 'a', 'fused matMul');\n    let $b = convertToTensor(b, 'b', 'fused matMul');\n    [$a, $b] = makeTypesMatch($a, $b);\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n    const outerDimsA = $a.shape.slice(0, -2);\n    const outerDimsB = $b.shape.slice(0, -2);\n    const batchDimA = util.sizeFromShape(outerDimsA);\n    const batchDimB = util.sizeFromShape(outerDimsB);\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at ` +\n        `least 2, got ranks ${$a.rank} and ${$b.rank}.`);\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} must match.`);\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\n        `${$b.shape} and transposeA=${transposeA}` +\n        ` and transposeB=${transposeB} must match.`);\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n    const a3D = transposeA ?\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n    const b3D = transposeB ?\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n    let $bias;\n    if (bias != null) {\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\n        [$bias] = makeTypesMatch($bias, $a);\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n    }\n    let $preluActivationWeights;\n    if (preluActivationWeights != null) {\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n    }\n    const grad = (dy, saved) => {\n        const [a3D, b3D, y, $bias] = saved;\n        // we reshape dy because the result of the forward is not\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\n        // the customOp.\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n        let aDer;\n        let bDer;\n        if (!transposeA && !transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\n        }\n        else if (!transposeA && transposeB) {\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\n        }\n        else if (transposeA && !transposeB) {\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\n        }\n        else {\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\n        }\n        if (bias != null) {\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\n            return [aDer, bDer, biasDer];\n        }\n        else {\n            return [aDer, bDer];\n        }\n    };\n    const inputs = {\n        a: a3D,\n        b: b3D,\n        bias: $bias,\n        preluActivationWeights: $preluActivationWeights\n    };\n    const attrs = { transposeA, transposeB, activation, leakyreluAlpha };\n    // Depending on the the params passed in we will have different number of\n    // inputs and thus a a different number of elements in the gradient.\n    if (bias == null) {\n        const customOp = customGrad((a3D, b3D, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOp(a3D, b3D);\n    }\n    else {\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n            const res = \n            // tslint:disable-next-line: no-unnecessary-type-assertion\n            ENGINE.runKernel(_FusedMatMul, inputs, attrs);\n            save([a3D, b3D, res, $bias]);\n            return { value: reshape(res, outShape), gradFunc: grad };\n        });\n        return customOpWithBias(a3D, b3D, $bias);\n    }\n}\nexport const matMul = op({ fusedMatMul_ });\n//# sourceMappingURL=mat_mul.js.map"]},"metadata":{},"sourceType":"module"}