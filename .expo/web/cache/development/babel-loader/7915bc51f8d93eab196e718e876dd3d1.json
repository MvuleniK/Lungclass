{"ast":null,"code":"import _extends from \"@babel/runtime/helpers/extends\";\nimport _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _get from \"@babel/runtime/helpers/get\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\n\nfunction _createForOfIteratorHelperLoose(o, allowArrayLike) { var it = typeof Symbol !== \"undefined\" && o[Symbol.iterator] || o[\"@@iterator\"]; if (it) return (it = it.call(o)).next.bind(it); if (Array.isArray(o) || (it = _unsupportedIterableToArray(o)) || allowArrayLike && o && typeof o.length === \"number\") { if (it) o = it; var i = 0; return function () { if (i >= o.length) return { done: true }; return { done: false, value: o[i++] }; }; } throw new TypeError(\"Invalid attempt to iterate non-iterable instance.\\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.\"); }\n\nfunction _unsupportedIterableToArray(o, minLen) { if (!o) return; if (typeof o === \"string\") return _arrayLikeToArray(o, minLen); var n = Object.prototype.toString.call(o).slice(8, -1); if (n === \"Object\" && o.constructor) n = o.constructor.name; if (n === \"Map\" || n === \"Set\") return Array.from(o); if (n === \"Arguments\" || /^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)) return _arrayLikeToArray(o, minLen); }\n\nfunction _arrayLikeToArray(arr, len) { if (len == null || len > arr.length) len = arr.length; for (var i = 0, arr2 = new Array(len); i < len; i++) { arr2[i] = arr[i]; } return arr2; }\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from \"../activations\";\nimport { cast } from \"../backend/tfjs_backend\";\nimport { getConstraint, serializeConstraint } from \"../constraints\";\nimport { InputSpec, Layer } from \"../engine/topology\";\nimport { NotImplementedError, ValueError } from \"../errors\";\nimport { getInitializer, serializeInitializer } from \"../initializers\";\nimport { getRegularizer, serializeRegularizer } from \"../regularizers\";\nimport { getExactlyOneShape, getExactlyOneTensor } from \"../utils/types_utils\";\nexport var ReLU = function (_Layer) {\n  _inherits(ReLU, _Layer);\n\n  var _super = _createSuper(ReLU);\n\n  function ReLU(args) {\n    var _this;\n\n    _classCallCheck(this, ReLU);\n\n    _this = _super.call(this, args == null ? {} : args);\n    _this.supportsMasking = true;\n\n    if (args != null) {\n      _this.maxValue = args.maxValue;\n    }\n\n    return _this;\n  }\n\n  _createClass(ReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      var output = relu(inputs);\n\n      if (this.maxValue != null) {\n        output = clipByValue(output, 0, this.maxValue);\n      }\n\n      return output;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        maxValue: this.maxValue\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ReLU.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return ReLU;\n}(Layer);\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport var LeakyReLU = function (_Layer2) {\n  _inherits(LeakyReLU, _Layer2);\n\n  var _super2 = _createSuper(LeakyReLU);\n\n  function LeakyReLU(args) {\n    var _this2;\n\n    _classCallCheck(this, LeakyReLU);\n\n    _this2 = _super2.call(this, args == null ? {} : args);\n    _this2.DEFAULT_ALPHA = 0.3;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this2.alpha = args.alpha == null ? _this2.DEFAULT_ALPHA : args.alpha;\n    return _this2;\n  }\n\n  _createClass(LeakyReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return leakyRelu(x, this.alpha);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n\n      var baseConfig = _get(_getPrototypeOf(LeakyReLU.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return LeakyReLU;\n}(Layer);\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport var PReLU = function (_Layer3) {\n  _inherits(PReLU, _Layer3);\n\n  var _super3 = _createSuper(PReLU);\n\n  function PReLU(args) {\n    var _this3;\n\n    _classCallCheck(this, PReLU);\n\n    _this3 = _super3.call(this, args == null ? {} : args);\n    _this3.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this3.supportsMasking = true;\n    _this3.alphaInitializer = getInitializer(args.alphaInitializer || _this3.DEFAULT_ALPHA_INITIALIZER);\n    _this3.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n    _this3.alphaConstraint = getConstraint(args.alphaConstraint);\n\n    if (args.sharedAxes == null) {\n      _this3.sharedAxes = null;\n    } else if (Array.isArray(args.sharedAxes)) {\n      _this3.sharedAxes = args.sharedAxes;\n    } else if (typeof args.sharedAxes === 'number') {\n      _this3.sharedAxes = [args.sharedAxes];\n    } else {\n      throw new ValueError(\"Expected sharedAxes to be a number or an array of numbers, \" + (\"but got \" + args.sharedAxes));\n    }\n\n    return _this3;\n  }\n\n  _createClass(PReLU, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var paramShape = inputShape.slice(1);\n\n      if (this.sharedAxes != null) {\n        for (var _iterator = _createForOfIteratorHelperLoose(this.sharedAxes), _step; !(_step = _iterator()).done;) {\n          var i = _step.value;\n          paramShape[i - 1] = 1;\n        }\n      }\n\n      this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n      var axes = {};\n\n      if (this.sharedAxes != null) {\n        for (var _i = 1; _i < inputShape.length; ++_i) {\n          axes[_i] = inputShape[_i];\n        }\n      }\n\n      this.inputSpec = [new InputSpec({\n        ndim: inputShape.length,\n        axes: axes\n      })];\n      this.built = true;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      inputs = getExactlyOneTensor(inputs);\n      return prelu(inputs, this.alpha.read());\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alphaInitializer: serializeInitializer(this.alphaInitializer),\n        alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n        alphaConstraint: serializeConstraint(this.alphaConstraint),\n        sharedAxes: this.sharedAxes\n      };\n\n      var baseConfig = _get(_getPrototypeOf(PReLU.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return PReLU;\n}(Layer);\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport var ELU = function (_Layer4) {\n  _inherits(ELU, _Layer4);\n\n  var _super4 = _createSuper(ELU);\n\n  function ELU(args) {\n    var _this4;\n\n    _classCallCheck(this, ELU);\n\n    _this4 = _super4.call(this, args == null ? {} : args);\n    _this4.DEFAULT_ALPHA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    if (args.alpha != null && args.alpha !== _this4.DEFAULT_ALPHA) {\n      throw new NotImplementedError(\"Non-default alpha value (\" + args.alpha + \") is not supported by the \" + \"ELU layer yet.\");\n    }\n\n    _this4.alpha = args.alpha == null ? _this4.DEFAULT_ALPHA : args.alpha;\n    return _this4;\n  }\n\n  _createClass(ELU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return elu(x);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        alpha: this.alpha\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ELU.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return ELU;\n}(Layer);\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport var ThresholdedReLU = function (_Layer5) {\n  _inherits(ThresholdedReLU, _Layer5);\n\n  var _super5 = _createSuper(ThresholdedReLU);\n\n  function ThresholdedReLU(args) {\n    var _this5;\n\n    _classCallCheck(this, ThresholdedReLU);\n\n    _this5 = _super5.call(this, args == null ? {} : args);\n    _this5.DEFAULT_THETA = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this5.theta = args.theta == null ? _this5.DEFAULT_THETA : args.theta;\n    return _this5;\n  }\n\n  _createClass(ThresholdedReLU, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return x.mul(cast(x.greater(this.theta), 'float32'));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        theta: this.theta\n      };\n\n      var baseConfig = _get(_getPrototypeOf(ThresholdedReLU.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return ThresholdedReLU;\n}(Layer);\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport var Softmax = function (_Layer6) {\n  _inherits(Softmax, _Layer6);\n\n  var _super6 = _createSuper(Softmax);\n\n  function Softmax(args) {\n    var _this6;\n\n    _classCallCheck(this, Softmax);\n\n    _this6 = _super6.call(this, args == null ? {} : args);\n    _this6.DEFAULT_AXIS = 1.0;\n\n    if (args == null) {\n      args = {};\n    }\n\n    _this6.softmax = new softmaxActivation().apply;\n    _this6.axis = args.axis == null ? _this6.DEFAULT_AXIS : args.axis;\n    return _this6;\n  }\n\n  _createClass(Softmax, [{\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var x = getExactlyOneTensor(inputs);\n      return this.softmax(x, this.axis);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        axis: this.axis\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Softmax.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }]);\n\n  return Softmax;\n}(Layer);\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);","map":{"version":3,"sources":["../../src/layers/advanced_activations.ts"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;AAAA;;;;;;;;AAQG;AAMH,SAAQ,WAAR,EAAqB,GAArB,EAA0B,SAA1B,EAAqC,KAArC,EAA4C,IAA5C,EAAkD,aAAlD,QAA8E,uBAA9E;AAEA,SAAQ,OAAO,IAAI,iBAAnB;AACA,SAAQ,IAAR;AACA,SAAoB,aAApB,EAAmC,mBAAnC;AACA,SAAQ,SAAR,EAAmB,KAAnB;AACA,SAAQ,mBAAR,EAA6B,UAA7B;AACA,SAAQ,cAAR,EAA4D,oBAA5D;AAEA,SAAQ,cAAR,EAAqC,oBAArC;AAEA,SAAQ,kBAAR,EAA4B,mBAA5B;AAUA,WAAa,IAAb;EAAA;;EAAA;;EAKE,cAAY,IAAZ,EAAgC;IAAA;;IAAA;;IAC9B,0BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IACA,MAAK,eAAL,GAAuB,IAAvB;;IACA,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,MAAK,QAAL,GAAgB,IAAI,CAAC,QAArB;IACD;;IAL6B;EAM/B;;EAXH;IAAA;IAAA,OAaE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B;MACA,IAAI,MAAM,GAAG,IAAI,CAAC,MAAD,CAAjB;;MACA,IAAI,KAAK,QAAL,IAAiB,IAArB,EAA2B;QACzB,MAAM,GAAG,WAAW,CAAC,MAAD,EAAS,CAAT,EAAY,KAAK,QAAjB,CAApB;MACD;;MACD,OAAO,MAAP;IACD;EApBH;IAAA;IAAA,OAsBE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAxBH;IAAA;IAAA,OA0BE,qBAAS;MACP,IAAM,MAAM,GAA6B;QAAC,QAAQ,EAAE,KAAK;MAAhB,CAAzC;;MACA,IAAM,UAAU,sEAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA/BH;;EAAA;AAAA,EAA0B,KAA1B;AAES,IAAA,CAAA,SAAA,GAAY,MAAZ;AA+BT,aAAa,CAAC,aAAd,CAA4B,IAA5B;AASA,WAAa,SAAb;EAAA;;EAAA;;EAOE,mBAAY,IAAZ,EAAqC;IAAA;;IAAA;;IACnC,4BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IAHO,OAAA,aAAA,GAAgB,GAAhB;;IAIP,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IACD,OAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,OAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;IALmC;EAMpC;;EAbH;IAAA;IAAA,OAeE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,IAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;MACA,OAAO,SAAS,CAAC,CAAD,EAAI,KAAK,KAAT,CAAhB;IACD;EAlBH;IAAA;IAAA,OAoBE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAtBH;IAAA;IAAA,OAwBE,qBAAS;MACP,IAAM,MAAM,GAA6B;QAAC,KAAK,EAAE,KAAK;MAAb,CAAzC;;MACA,IAAM,UAAU,2EAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA7BH;;EAAA;AAAA,EAA+B,KAA/B;AAES,SAAA,CAAA,SAAA,GAAY,WAAZ;AA6BT,aAAa,CAAC,aAAd,CAA4B,SAA5B;AA6BA,WAAa,KAAb;EAAA;;EAAA;;EAWE,eAAY,IAAZ,EAAiC;IAAA;;IAAA;;IAC/B,4BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IAHO,OAAA,yBAAA,GAAmD,OAAnD;;IAIP,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IAED,OAAK,eAAL,GAAuB,IAAvB;IACA,OAAK,gBAAL,GACI,cAAc,CAAC,IAAI,CAAC,gBAAL,IAAyB,OAAK,yBAA/B,CADlB;IAEA,OAAK,gBAAL,GAAwB,cAAc,CAAC,IAAI,CAAC,gBAAN,CAAtC;IACA,OAAK,eAAL,GAAuB,aAAa,CAAC,IAAI,CAAC,eAAN,CAApC;;IACA,IAAI,IAAI,CAAC,UAAL,IAAmB,IAAvB,EAA6B;MAC3B,OAAK,UAAL,GAAkB,IAAlB;IACD,CAFD,MAEO,IAAI,KAAK,CAAC,OAAN,CAAc,IAAI,CAAC,UAAnB,CAAJ,EAAoC;MACzC,OAAK,UAAL,GAAkB,IAAI,CAAC,UAAvB;IACD,CAFM,MAEA,IAAI,OAAO,IAAI,CAAC,UAAZ,KAA2B,QAA/B,EAAyC;MAC9C,OAAK,UAAL,GAAkB,CAAC,IAAI,CAAC,UAAN,CAAlB;IACD,CAFM,MAEA;MACL,MAAM,IAAI,UAAJ,CACF,8EACW,IAAI,CAAC,UADhB,CADE,CAAN;IAGD;;IArB8B;EAsBhC;;EAjCH;IAAA;IAAA,OAmCE,eAAM,UAAN,EAA+B;MAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;MACA,IAAM,UAAU,GAAU,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAA1B;;MACA,IAAI,KAAK,UAAL,IAAmB,IAAvB,EAA6B;QAC3B,qDAAgB,KAAK,UAArB,wCAAiC;UAAA,IAAtB,CAAsB;UAC/B,UAAU,CAAC,CAAC,GAAG,CAAL,CAAV,GAAoB,CAApB;QACD;MACF;;MACD,KAAK,KAAL,GAAa,KAAK,SAAL,CACT,OADS,EACA,UADA,EACY,SADZ,EACuB,KAAK,gBAD5B,EAET,KAAK,gBAFI,EAEc,IAFd,EAEoB,KAAK,eAFzB,CAAb;MAIA,IAAM,IAAI,GAA6B,EAAvC;;MACA,IAAI,KAAK,UAAL,IAAmB,IAAvB,EAA6B;QAC3B,KAAK,IAAI,EAAC,GAAG,CAAb,EAAgB,EAAC,GAAG,UAAU,CAAC,MAA/B,EAAuC,EAAE,EAAzC,EAA4C;UAC1C,IAAI,CAAC,EAAD,CAAJ,GAAU,UAAU,CAAC,EAAD,CAApB;QACD;MACF;;MACD,KAAK,SAAL,GAAiB,CAAC,IAAI,SAAJ,CAAc;QAC9B,IAAI,EAAE,UAAU,CAAC,MADa;QAE9B,IAAI,EAAJ;MAF8B,CAAd,CAAD,CAAjB;MAIA,KAAK,KAAL,GAAa,IAAb;IACD;EA1DH;IAAA;IAAA,OA4DE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B;MACA,OAAO,KAAK,CAAC,MAAD,EAAS,KAAK,KAAL,CAAW,IAAX,EAAT,CAAZ;IACD;EA/DH;IAAA;IAAA,OAiEE,qBAAS;MACP,IAAM,MAAM,GAA6B;QACvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CADC;QAEvC,gBAAgB,EAAE,oBAAoB,CAAC,KAAK,gBAAN,CAFC;QAGvC,eAAe,EAAE,mBAAmB,CAAC,KAAK,eAAN,CAHG;QAIvC,UAAU,EAAE,KAAK;MAJsB,CAAzC;;MAMA,IAAM,UAAU,uEAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA3EH;;EAAA;AAAA,EAA2B,KAA3B;AAES,KAAA,CAAA,SAAA,GAAY,OAAZ;AA2ET,aAAa,CAAC,aAAd,CAA4B,KAA5B;AASA,WAAa,GAAb;EAAA;;EAAA;;EAOE,aAAY,IAAZ,EAA+B;IAAA;;IAAA;;IAC7B,4BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IAHO,OAAA,aAAA,GAAgB,GAAhB;;IAIP,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IAED,IAAI,IAAI,CAAC,KAAL,IAAc,IAAd,IAAsB,IAAI,CAAC,KAAL,KAAe,OAAK,aAA9C,EAA6D;MAC3D,MAAM,IAAI,mBAAJ,CACF,8BAA4B,IAAI,CAAC,KAAjC,kDADE,CAAN;IAGD;;IAED,OAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,OAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;IAZ6B;EAa9B;;EApBH;IAAA;IAAA,OAsBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,IAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;MACA,OAAO,GAAG,CAAC,CAAD,CAAV;IACD;EAzBH;IAAA;IAAA,OA2BE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EA7BH;IAAA;IAAA,OA+BE,qBAAS;MACP,IAAM,MAAM,GAA6B;QAAC,KAAK,EAAE,KAAK;MAAb,CAAzC;;MACA,IAAM,UAAU,qEAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EApCH;;EAAA;AAAA,EAAyB,KAAzB;AAES,GAAA,CAAA,SAAA,GAAY,KAAZ;AAoCT,aAAa,CAAC,aAAd,CAA4B,GAA5B;AASA,WAAa,eAAb;EAAA;;EAAA;;EAOE,yBAAY,IAAZ,EAA2C;IAAA;;IAAA;;IACzC,4BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IAHO,OAAA,aAAA,GAAgB,GAAhB;;IAIP,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IAED,OAAK,KAAL,GAAa,IAAI,CAAC,KAAL,IAAc,IAAd,GAAqB,OAAK,aAA1B,GAA0C,IAAI,CAAC,KAA5D;IANyC;EAO1C;;EAdH;IAAA;IAAA,OAgBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,IAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;MACA,OAAO,CAAC,CAAC,GAAF,CAAM,IAAI,CAAC,CAAC,CAAC,OAAF,CAAU,KAAK,KAAf,CAAD,EAAwB,SAAxB,CAAV,CAAP;IACD;EAnBH;IAAA;IAAA,OAqBE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAvBH;IAAA;IAAA,OAyBE,qBAAS;MACP,IAAM,MAAM,GAA6B;QAAC,KAAK,EAAE,KAAK;MAAb,CAAzC;;MACA,IAAM,UAAU,iFAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA9BH;;EAAA;AAAA,EAAqC,KAArC;AAES,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA8BT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAUA,WAAa,OAAb;EAAA;;EAAA;;EAOE,iBAAY,IAAZ,EAAmC;IAAA;;IAAA;;IACjC,4BAAM,IAAI,IAAI,IAAR,GAAe,EAAf,GAAoB,IAA1B;IAHO,OAAA,YAAA,GAAe,GAAf;;IAIP,IAAI,IAAI,IAAI,IAAZ,EAAkB;MAChB,IAAI,GAAG,EAAP;IACD;;IACD,OAAK,OAAL,GAAe,IAAI,iBAAJ,GAAwB,KAAvC;IACA,OAAK,IAAL,GAAY,IAAI,CAAC,IAAL,IAAa,IAAb,GAAoB,OAAK,YAAzB,GAAwC,IAAI,CAAC,IAAzD;IANiC;EAOlC;;EAdH;IAAA;IAAA,OAgBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAC1C,IAAM,CAAC,GAAG,mBAAmB,CAAC,MAAD,CAA7B;MACA,OAAO,KAAK,OAAL,CAAa,CAAb,EAAgB,KAAK,IAArB,CAAP;IACD;EAnBH;IAAA;IAAA,OAqBE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAvBH;IAAA;IAAA,OAyBE,qBAAS;MACP,IAAM,MAAM,GAA6B;QAAC,IAAI,EAAE,KAAK;MAAZ,CAAzC;;MACA,IAAM,UAAU,yEAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA9BH;;EAAA;AAAA,EAA6B,KAA7B;AAES,OAAA,CAAA,SAAA,GAAY,SAAZ;AA8BT,aAAa,CAAC,aAAd,CAA4B,OAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n *  Advanced activation layers.\n */\nimport { clipByValue, elu, leakyRelu, prelu, relu, serialization } from '@tensorflow/tfjs-core';\nimport { Softmax as softmaxActivation } from '../activations';\nimport { cast } from '../backend/tfjs_backend';\nimport { getConstraint, serializeConstraint } from '../constraints';\nimport { InputSpec, Layer } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { getInitializer, serializeInitializer } from '../initializers';\nimport { getRegularizer, serializeRegularizer } from '../regularizers';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nexport class ReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.supportsMasking = true;\n        if (args != null) {\n            this.maxValue = args.maxValue;\n        }\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        let output = relu(inputs);\n        if (this.maxValue != null) {\n            output = clipByValue(output, 0, this.maxValue);\n        }\n        return output;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { maxValue: this.maxValue };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nReLU.className = 'ReLU';\nserialization.registerClass(ReLU);\nexport class LeakyReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 0.3;\n        if (args == null) {\n            args = {};\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return leakyRelu(x, this.alpha);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nLeakyReLU.className = 'LeakyReLU';\nserialization.registerClass(LeakyReLU);\nexport class PReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA_INITIALIZER = 'zeros';\n        if (args == null) {\n            args = {};\n        }\n        this.supportsMasking = true;\n        this.alphaInitializer =\n            getInitializer(args.alphaInitializer || this.DEFAULT_ALPHA_INITIALIZER);\n        this.alphaRegularizer = getRegularizer(args.alphaRegularizer);\n        this.alphaConstraint = getConstraint(args.alphaConstraint);\n        if (args.sharedAxes == null) {\n            this.sharedAxes = null;\n        }\n        else if (Array.isArray(args.sharedAxes)) {\n            this.sharedAxes = args.sharedAxes;\n        }\n        else if (typeof args.sharedAxes === 'number') {\n            this.sharedAxes = [args.sharedAxes];\n        }\n        else {\n            throw new ValueError(`Expected sharedAxes to be a number or an array of numbers, ` +\n                `but got ${args.sharedAxes}`);\n        }\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const paramShape = inputShape.slice(1);\n        if (this.sharedAxes != null) {\n            for (const i of this.sharedAxes) {\n                paramShape[i - 1] = 1;\n            }\n        }\n        this.alpha = this.addWeight('alpha', paramShape, 'float32', this.alphaInitializer, this.alphaRegularizer, true, this.alphaConstraint);\n        // Set input spec.\n        const axes = {};\n        if (this.sharedAxes != null) {\n            for (let i = 1; i < inputShape.length; ++i) {\n                axes[i] = inputShape[i];\n            }\n        }\n        this.inputSpec = [new InputSpec({\n                ndim: inputShape.length,\n                axes,\n            })];\n        this.built = true;\n    }\n    call(inputs, kwargs) {\n        inputs = getExactlyOneTensor(inputs);\n        return prelu(inputs, this.alpha.read());\n    }\n    getConfig() {\n        const config = {\n            alphaInitializer: serializeInitializer(this.alphaInitializer),\n            alphaRegularizer: serializeRegularizer(this.alphaRegularizer),\n            alphaConstraint: serializeConstraint(this.alphaConstraint),\n            sharedAxes: this.sharedAxes\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nPReLU.className = 'PReLU';\nserialization.registerClass(PReLU);\nexport class ELU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_ALPHA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        if (args.alpha != null && args.alpha !== this.DEFAULT_ALPHA) {\n            throw new NotImplementedError(`Non-default alpha value (${args.alpha}) is not supported by the ` +\n                `ELU layer yet.`);\n        }\n        this.alpha = args.alpha == null ? this.DEFAULT_ALPHA : args.alpha;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return elu(x);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { alpha: this.alpha };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nELU.className = 'ELU';\nserialization.registerClass(ELU);\nexport class ThresholdedReLU extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_THETA = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.theta = args.theta == null ? this.DEFAULT_THETA : args.theta;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return x.mul(cast(x.greater(this.theta), 'float32'));\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { theta: this.theta };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nThresholdedReLU.className = 'ThresholdedReLU';\nserialization.registerClass(ThresholdedReLU);\nexport class Softmax extends Layer {\n    constructor(args) {\n        super(args == null ? {} : args);\n        this.DEFAULT_AXIS = 1.0;\n        if (args == null) {\n            args = {};\n        }\n        this.softmax = new softmaxActivation().apply;\n        this.axis = args.axis == null ? this.DEFAULT_AXIS : args.axis;\n    }\n    call(inputs, kwargs) {\n        const x = getExactlyOneTensor(inputs);\n        return this.softmax(x, this.axis);\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const config = { axis: this.axis };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n}\n/** @nocollapse */\nSoftmax.className = 'Softmax';\nserialization.registerClass(Softmax);\n//# sourceMappingURL=advanced_activations.js.map"]},"metadata":{},"sourceType":"module"}