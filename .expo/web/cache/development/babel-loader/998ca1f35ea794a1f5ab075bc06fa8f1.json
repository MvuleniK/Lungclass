{"ast":null,"code":"import _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _regeneratorRuntime from \"@babel/runtime/regenerator\";\n\n/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from \"../operations/operation_mapper\";\nimport { GraphExecutor } from \"./graph_executor\";\nimport { ResourceManager } from \"./resource_manager\";\nexport var TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport var DEFAULT_MODEL_NAME = 'model.json';\nexport var GraphModel = function () {\n  function GraphModel(modelUrl) {\n    var loadOptions = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n\n    _classCallCheck(this, GraphModel);\n\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n\n    this.resourceManager = new ResourceManager();\n  }\n\n  _createClass(GraphModel, [{\n    key: \"modelVersion\",\n    get: function get() {\n      return this.version;\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this.executor.inputNodes;\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this.executor.outputNodes;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this.executor.inputs;\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this.executor.outputs;\n    }\n  }, {\n    key: \"weights\",\n    get: function get() {\n      return this.executor.weightMap;\n    }\n  }, {\n    key: \"metadata\",\n    get: function get() {\n      return this.artifacts.userDefinedMetadata;\n    }\n  }, {\n    key: \"modelSignature\",\n    get: function get() {\n      return this.signature;\n    }\n  }, {\n    key: \"findIOHandler\",\n    value: function findIOHandler() {\n      var path = this.modelUrl;\n\n      if (path.load != null) {\n        this.handler = path;\n      } else if (this.loadOptions.requestInit != null) {\n        this.handler = io.browserHTTPRequest(path, this.loadOptions);\n      } else {\n        var handlers = io.getLoadHandlers(path, this.loadOptions);\n\n        if (handlers.length === 0) {\n          handlers.push(io.browserHTTPRequest(path, this.loadOptions));\n        } else if (handlers.length > 1) {\n          throw new Error(\"Found more than one (\" + handlers.length + \") load handlers for \" + (\"URL '\" + [path] + \"'\"));\n        }\n\n        this.handler = handlers[0];\n      }\n    }\n  }, {\n    key: \"load\",\n    value: function load() {\n      var artifacts;\n      return _regeneratorRuntime.async(function load$(_context) {\n        while (1) {\n          switch (_context.prev = _context.next) {\n            case 0:\n              this.findIOHandler();\n\n              if (!(this.handler.load == null)) {\n                _context.next = 3;\n                break;\n              }\n\n              throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n\n            case 3:\n              _context.next = 5;\n              return _regeneratorRuntime.awrap(this.handler.load());\n\n            case 5:\n              artifacts = _context.sent;\n              return _context.abrupt(\"return\", this.loadSync(artifacts));\n\n            case 7:\n            case \"end\":\n              return _context.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"loadSync\",\n    value: function loadSync(artifacts) {\n      this.artifacts = artifacts;\n      var graph = this.artifacts.modelTopology;\n      var signature;\n\n      if (this.artifacts.userDefinedMetadata != null && this.artifacts.userDefinedMetadata.signature != null) {\n        signature = this.artifacts.userDefinedMetadata.signature;\n      } else {\n        signature = this.artifacts.signature;\n      }\n\n      this.signature = signature;\n      this.version = graph.versions.producer + \".\" + graph.versions.minConsumer;\n      var weightMap = io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n      this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n      this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n      this.executor.resourceManager = this.resourceManager;\n\n      if (artifacts.modelInitializer != null && artifacts.modelInitializer.node != null) {\n        var initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n        this.initializer = new GraphExecutor(initializer);\n        this.initializer.weightMap = this.executor.weightMap;\n        this.initializer.resourceManager = this.resourceManager;\n        this.initializer.executeAsync({}, []);\n      }\n\n      return true;\n    }\n  }, {\n    key: \"save\",\n    value: function save(handlerOrURL, config) {\n      var handlers;\n      return _regeneratorRuntime.async(function save$(_context2) {\n        while (1) {\n          switch (_context2.prev = _context2.next) {\n            case 0:\n              if (!(typeof handlerOrURL === 'string')) {\n                _context2.next = 9;\n                break;\n              }\n\n              handlers = io.getSaveHandlers(handlerOrURL);\n\n              if (!(handlers.length === 0)) {\n                _context2.next = 6;\n                break;\n              }\n\n              throw new Error(\"Cannot find any save handlers for URL '\" + handlerOrURL + \"'\");\n\n            case 6:\n              if (!(handlers.length > 1)) {\n                _context2.next = 8;\n                break;\n              }\n\n              throw new Error(\"Found more than one (\" + handlers.length + \") save handlers for \" + (\"URL '\" + handlerOrURL + \"'\"));\n\n            case 8:\n              handlerOrURL = handlers[0];\n\n            case 9:\n              if (!(handlerOrURL.save == null)) {\n                _context2.next = 11;\n                break;\n              }\n\n              throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n\n            case 11:\n              return _context2.abrupt(\"return\", handlerOrURL.save(this.artifacts));\n\n            case 12:\n            case \"end\":\n              return _context2.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"predict\",\n    value: function predict(inputs, config) {\n      return this.execute(inputs, this.outputNodes);\n    }\n  }, {\n    key: \"normalizeInputs\",\n    value: function normalizeInputs(inputs) {\n      if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n        return inputs;\n      }\n\n      inputs = Array.isArray(inputs) ? inputs : [inputs];\n\n      if (inputs.length !== this.inputNodes.length) {\n        throw new Error('Input tensor count mismatch,' + (\"the graph model has \" + this.inputNodes.length + \" placeholders, \") + (\"while there are \" + inputs.length + \" input tensors.\"));\n      }\n\n      return this.inputNodes.reduce(function (map, inputName, i) {\n        map[inputName] = inputs[i];\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"normalizeOutputs\",\n    value: function normalizeOutputs(outputs) {\n      outputs = outputs || this.outputNodes;\n      return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      inputs = this.normalizeInputs(inputs);\n      outputs = this.normalizeOutputs(outputs);\n      var result = this.executor.execute(inputs, outputs);\n      return result.length > 1 ? result : result[0];\n    }\n  }, {\n    key: \"executeAsync\",\n    value: function executeAsync(inputs, outputs) {\n      var result;\n      return _regeneratorRuntime.async(function executeAsync$(_context3) {\n        while (1) {\n          switch (_context3.prev = _context3.next) {\n            case 0:\n              inputs = this.normalizeInputs(inputs);\n              outputs = this.normalizeOutputs(outputs);\n              _context3.next = 4;\n              return _regeneratorRuntime.awrap(this.executor.executeAsync(inputs, outputs));\n\n            case 4:\n              result = _context3.sent;\n              return _context3.abrupt(\"return\", result.length > 1 ? result : result[0]);\n\n            case 6:\n            case \"end\":\n              return _context3.stop();\n          }\n        }\n      }, null, this, null, Promise);\n    }\n  }, {\n    key: \"convertTensorMapToTensorsMap\",\n    value: function convertTensorMapToTensorsMap(map) {\n      return Object.keys(map).reduce(function (newMap, key) {\n        newMap[key] = [map[key]];\n        return newMap;\n      }, {});\n    }\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.executor.dispose();\n\n      if (this.initializer) {\n        this.initializer.dispose();\n      }\n\n      this.resourceManager.dispose();\n    }\n  }]);\n\n  return GraphModel;\n}();\nexport function loadGraphModel(modelUrl) {\n  var options,\n      model,\n      _args4 = arguments;\n  return _regeneratorRuntime.async(function loadGraphModel$(_context4) {\n    while (1) {\n      switch (_context4.prev = _context4.next) {\n        case 0:\n          options = _args4.length > 1 && _args4[1] !== undefined ? _args4[1] : {};\n\n          if (!(modelUrl == null)) {\n            _context4.next = 3;\n            break;\n          }\n\n          throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n\n        case 3:\n          if (options == null) {\n            options = {};\n          }\n\n          if (options.fromTFHub) {\n            if (modelUrl.load == null) {\n              if (!modelUrl.endsWith('/')) {\n                modelUrl = modelUrl + '/';\n              }\n\n              modelUrl = \"\" + modelUrl + DEFAULT_MODEL_NAME + TFHUB_SEARCH_PARAM;\n            }\n          }\n\n          model = new GraphModel(modelUrl, options);\n          _context4.next = 8;\n          return _regeneratorRuntime.awrap(model.load());\n\n        case 8:\n          return _context4.abrupt(\"return\", model);\n\n        case 9:\n        case \"end\":\n          return _context4.stop();\n      }\n    }\n  }, null, null, null, Promise);\n}","map":{"version":3,"sources":["../../src/executor/graph_model.ts"],"names":[],"mappings":";;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAwB,EAAxB,EAAgE,MAAhE,QAA6E,uBAA7E;AAIA,SAAQ,eAAR;AAEA,SAAQ,aAAR;AACA,SAAQ,eAAR;AAEA,OAAO,IAAM,kBAAkB,GAAG,mBAA3B;AACP,OAAO,IAAM,kBAAkB,GAAG,YAA3B;AAWP,WAAa,UAAb;EAmDE,oBACY,QADZ,EAE4C;IAAA,IAAhC,WAAgC,uEAAF,EAAE;;IAAA;;IADhC,KAAA,QAAA,GAAA,QAAA;IACA,KAAA,WAAA,GAAA,WAAA;IAnDJ,KAAA,OAAA,GAAU,KAAV;;IAoDN,IAAI,WAAW,IAAI,IAAnB,EAAyB;MACvB,KAAK,WAAL,GAAmB,EAAnB;IACD;;IACD,KAAK,eAAL,GAAuB,IAAI,eAAJ,EAAvB;EACD;;EA1DH;IAAA;IAAA,KAUE,eAAgB;MACd,OAAO,KAAK,OAAZ;IACD;EAZH;IAAA;IAAA,KAcE,eAAc;MACZ,OAAO,KAAK,QAAL,CAAc,UAArB;IACD;EAhBH;IAAA;IAAA,KAkBE,eAAe;MACb,OAAO,KAAK,QAAL,CAAc,WAArB;IACD;EApBH;IAAA;IAAA,KAsBE,eAAU;MACR,OAAO,KAAK,QAAL,CAAc,MAArB;IACD;EAxBH;IAAA;IAAA,KA0BE,eAAW;MACT,OAAO,KAAK,QAAL,CAAc,OAArB;IACD;EA5BH;IAAA;IAAA,KA8BE,eAAW;MACT,OAAO,KAAK,QAAL,CAAc,SAArB;IACD;EAhCH;IAAA;IAAA,KAkCE,eAAY;MACV,OAAO,KAAK,SAAL,CAAe,mBAAtB;IACD;EApCH;IAAA;IAAA,KAsCE,eAAkB;MAChB,OAAO,KAAK,SAAZ;IACD;EAxCH;IAAA;IAAA,OA4DU,yBAAa;MACnB,IAAM,IAAI,GAAG,KAAK,QAAlB;;MACA,IAAK,IAAqB,CAAC,IAAtB,IAA8B,IAAnC,EAAyC;QAEvC,KAAK,OAAL,GAAe,IAAf;MACD,CAHD,MAGO,IAAI,KAAK,WAAL,CAAiB,WAAjB,IAAgC,IAApC,EAA0C;QAC/C,KAAK,OAAL,GAAe,EAAE,CAAC,kBAAH,CAAsB,IAAtB,EAAsC,KAAK,WAA3C,CAAf;MACD,CAFM,MAEA;QACL,IAAM,QAAQ,GAAG,EAAE,CAAC,eAAH,CAAmB,IAAnB,EAAmC,KAAK,WAAxC,CAAjB;;QACA,IAAI,QAAQ,CAAC,MAAT,KAAoB,CAAxB,EAA2B;UAGzB,QAAQ,CAAC,IAAT,CAAc,EAAE,CAAC,kBAAH,CAAsB,IAAtB,EAAsC,KAAK,WAA3C,CAAd;QACD,CAJD,MAIO,IAAI,QAAQ,CAAC,MAAT,GAAkB,CAAtB,EAAyB;UAC9B,MAAM,IAAI,KAAJ,CACF,0BAAwB,QAAQ,CAAC,MAAjC,uCACQ,CAAC,IAAD,CADR,OADE,CAAN;QAGD;;QACD,KAAK,OAAL,GAAe,QAAQ,CAAC,CAAD,CAAvB;MACD;IACF;EAhFH;IAAA;IAAA,OAsFE;MAAA;MAAA;QAAA;UAAA;YAAA;cACE,KAAK,aAAL;;cADF,MAEM,KAAK,OAAL,CAAa,IAAb,IAAqB,IAF3B;gBAAA;gBAAA;cAAA;;cAAA,MAGU,IAAI,KAAJ,CACF,sEACA,8CAFE,CAHV;;YAAA;cAAA;cAAA,iCAO0B,KAAK,OAAL,CAAa,IAAb,EAP1B;;YAAA;cAOQ,SAPR;cAAA,iCASS,KAAK,QAAL,CAAc,SAAd,CATT;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EAtFF;IAAA;IAAA,OAwGE,kBAAS,SAAT,EAAqC;MACnC,KAAK,SAAL,GAAiB,SAAjB;MACA,IAAM,KAAK,GAAG,KAAK,SAAL,CAAe,aAA7B;MAEA,IAAI,SAAJ;;MACA,IAAI,KAAK,SAAL,CAAe,mBAAf,IAAsC,IAAtC,IACA,KAAK,SAAL,CAAe,mBAAf,CAAmC,SAAnC,IAAgD,IADpD,EAC0D;QACxD,SAAS,GACJ,KAAK,SAAL,CAAe,mBAAf,CAA2C,SADhD;MAGD,CALD,MAKO;QACL,SAAS,GAAG,KAAK,SAAL,CAAe,SAA3B;MACD;;MACD,KAAK,SAAL,GAAiB,SAAjB;MAEA,KAAK,OAAL,GAAkB,KAAK,CAAC,QAAN,CAAe,QAAjC,SAA6C,KAAK,CAAC,QAAN,CAAe,WAA5D;MACA,IAAM,SAAS,GACX,EAAE,CAAC,aAAH,CAAiB,KAAK,SAAL,CAAe,UAAhC,EAA4C,KAAK,SAAL,CAAe,WAA3D,CADJ;MAEA,KAAK,QAAL,GAAgB,IAAI,aAAJ,CACZ,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,KAAxC,EAA+C,KAAK,SAApD,CADY,CAAhB;MAEA,KAAK,QAAL,CAAc,SAAd,GAA0B,KAAK,4BAAL,CAAkC,SAAlC,CAA1B;MAGA,KAAK,QAAL,CAAc,eAAd,GAAgC,KAAK,eAArC;;MAEA,IAAI,SAAS,CAAC,gBAAV,IAA8B,IAA9B,IACC,SAAS,CAAC,gBAAV,CAAoD,IAApD,IAA4D,IADjE,EACuE;QACrE,IAAM,WAAW,GACb,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,SAAS,CAAC,gBAAlD,CADJ;QAEA,KAAK,WAAL,GAAmB,IAAI,aAAJ,CAAkB,WAAlB,CAAnB;QACA,KAAK,WAAL,CAAiB,SAAjB,GAA6B,KAAK,QAAL,CAAc,SAA3C;QAIA,KAAK,WAAL,CAAiB,eAAjB,GAAmC,KAAK,eAAxC;QACA,KAAK,WAAL,CAAiB,YAAjB,CAA8B,EAA9B,EAAkC,EAAlC;MACD;;MAED,OAAO,IAAP;IACD;EA/IH;IAAA;IAAA,OA6LE,cAAW,YAAX,EAA8C,MAA9C;MAAA;MAAA;QAAA;UAAA;YAAA;cAAA,MAEM,OAAO,YAAP,KAAwB,QAF9B;gBAAA;gBAAA;cAAA;;cAGU,QAHV,GAGqB,EAAE,CAAC,eAAH,CAAmB,YAAnB,CAHrB;;cAAA,MAIQ,QAAQ,CAAC,MAAT,KAAoB,CAJ5B;gBAAA;gBAAA;cAAA;;cAAA,MAKY,IAAI,KAAJ,6CACwC,YADxC,OALZ;;YAAA;cAAA,MAOe,QAAQ,CAAC,MAAT,GAAkB,CAPjC;gBAAA;gBAAA;cAAA;;cAAA,MAQY,IAAI,KAAJ,CACF,0BAAwB,QAAQ,CAAC,MAAjC,uCACQ,YADR,OADE,CARZ;;YAAA;cAYI,YAAY,GAAG,QAAQ,CAAC,CAAD,CAAvB;;YAZJ;cAAA,MAcM,YAAY,CAAC,IAAb,IAAqB,IAd3B;gBAAA;gBAAA;cAAA;;cAAA,MAeU,IAAI,KAAJ,CACF,4DACA,sDAFE,CAfV;;YAAA;cAAA,kCAoBS,YAAY,CAAC,IAAb,CAAkB,KAAK,SAAvB,CApBT;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA7LF;IAAA;IAAA,OA0PE,iBAAQ,MAAR,EAAgD,MAAhD,EAA2E;MAEzE,OAAO,KAAK,OAAL,CAAa,MAAb,EAAqB,KAAK,WAA1B,CAAP;IACD;EA7PH;IAAA;IAAA,OA+PU,yBAAgB,MAAhB,EAC8B;MACpC,IAAI,EAAE,MAAM,YAAY,MAApB,KAA+B,CAAC,KAAK,CAAC,OAAN,CAAc,MAAd,CAApC,EAA2D;QAEzD,OAAO,MAAP;MACD;;MACD,MAAM,GAAG,KAAK,CAAC,OAAN,CAAc,MAAd,IAAwB,MAAxB,GAAiC,CAAC,MAAD,CAA1C;;MACA,IAAI,MAAM,CAAC,MAAP,KAAkB,KAAK,UAAL,CAAgB,MAAtC,EAA8C;QAC5C,MAAM,IAAI,KAAJ,CACF,2DACuB,KAAK,UAAL,CAAgB,MADvC,8CAEmB,MAAM,CAAC,MAF1B,qBADE,CAAN;MAID;;MACD,OAAO,KAAK,UAAL,CAAgB,MAAhB,CAAuB,UAAC,GAAD,EAAM,SAAN,EAAiB,CAAjB,EAAsB;QAClD,GAAG,CAAC,SAAD,CAAH,GAAkB,MAAmB,CAAC,CAAD,CAArC;QACA,OAAO,GAAP;MACD,CAHM,EAGJ,EAHI,CAAP;IAID;EAhRH;IAAA;IAAA,OAkRU,0BAAiB,OAAjB,EAAyC;MAC/C,OAAO,GAAG,OAAO,IAAI,KAAK,WAA1B;MACA,OAAO,CAAC,KAAK,CAAC,OAAN,CAAc,OAAd,CAAD,GAA0B,CAAC,OAAD,CAA1B,GAAsC,OAA7C;IACD;EArRH;IAAA;IAAA,OAuSE,iBAAQ,MAAR,EAAgD,OAAhD,EAAyE;MAEvE,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;MACA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;MACA,IAAM,MAAM,GAAG,KAAK,QAAL,CAAc,OAAd,CAAsB,MAAtB,EAA8B,OAA9B,CAAf;MACA,OAAO,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CAA1C;IACD;EA7SH;IAAA;IAAA,OA8TE,sBACI,MADJ,EAEI,OAFJ;MAAA;MAAA;QAAA;UAAA;YAAA;cAGE,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;cACA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;cAJF;cAAA,iCAKuB,KAAK,QAAL,CAAc,YAAd,CAA2B,MAA3B,EAAmC,OAAnC,CALvB;;YAAA;cAKQ,MALR;cAAA,kCAMS,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CAN5C;;YAAA;YAAA;cAAA;UAAA;QAAA;MAAA;IAAA;EA9TF;IAAA;IAAA,OAuUU,sCAA6B,GAA7B,EAAgD;MACtD,OAAO,MAAM,CAAC,IAAP,CAAY,GAAZ,EAAiB,MAAjB,CAAwB,UAAC,MAAD,EAA0B,GAA1B,EAAiC;QAC9D,MAAM,CAAC,GAAD,CAAN,GAAc,CAAC,GAAG,CAAC,GAAD,CAAJ,CAAd;QACA,OAAO,MAAP;MACD,CAHM,EAGJ,EAHI,CAAP;IAID;EA5UH;IAAA;IAAA,OAmVE,mBAAO;MACL,KAAK,QAAL,CAAc,OAAd;;MAEA,IAAI,KAAK,WAAT,EAAsB;QACpB,KAAK,WAAL,CAAiB,OAAjB;MACD;;MAED,KAAK,eAAL,CAAqB,OAArB;IACD;EA3VH;;EAAA;AAAA;AA4XA,OAAO,SAAe,cAAf,CACH,QADG;EAAA;EAAA;EAAA;EAAA;IAAA;MAAA;QAAA;UAEH,OAFG,8DAEuB,EAFvB;;UAAA,MAGD,QAAQ,IAAI,IAHX;YAAA;YAAA;UAAA;;UAAA,MAIG,IAAI,KAAJ,CACF,uEACA,sCAFE,CAJH;;QAAA;UAQL,IAAI,OAAO,IAAI,IAAf,EAAqB;YACnB,OAAO,GAAG,EAAV;UACD;;UAED,IAAI,OAAO,CAAC,SAAZ,EAAuB;YACrB,IAAK,QAAyB,CAAC,IAA1B,IAAkC,IAAvC,EAA6C;cAC3C,IAAI,CAAE,QAAmB,CAAC,QAApB,CAA6B,GAA7B,CAAN,EAAyC;gBACvC,QAAQ,GAAI,QAAmB,GAAG,GAAlC;cACD;;cACD,QAAQ,QAAM,QAAN,GAAiB,kBAAjB,GAAsC,kBAA9C;YACD;UACF;;UACK,KApBD,GAoBS,IAAI,UAAJ,CAAe,QAAf,EAAyB,OAAzB,CApBT;UAAA;UAAA,iCAqBC,KAAK,CAAC,IAAN,EArBD;;QAAA;UAAA,kCAsBE,KAtBF;;QAAA;QAAA;UAAA;MAAA;IAAA;EAAA;AAAA","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { io, Tensor } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nimport { ResourceManager } from './resource_manager';\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport const DEFAULT_MODEL_NAME = 'model.json';\n/**\n * A `tf.GraphModel` is a directed, acyclic graph built from a\n * SavedModel GraphDef and allows inference execution.\n *\n * A `tf.GraphModel` can only be created by loading from a model converted from\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\n * the command line converter tool and loaded via `tf.loadGraphModel`.\n *\n * @doc {heading: 'Models', subheading: 'Classes'}\n */\nexport class GraphModel {\n    /**\n     * @param modelUrl url for the model, or an `io.IOHandler`.\n     * @param weightManifestUrl url for the weight file generated by\n     * scripts/convert.py script.\n     * @param requestOption options for Request, which allows to send credentials\n     * and custom headers.\n     * @param onProgress Optional, progress callback function, fired periodically\n     * before the load is completed.\n     */\n    constructor(modelUrl, loadOptions = {}) {\n        this.modelUrl = modelUrl;\n        this.loadOptions = loadOptions;\n        this.version = 'n/a';\n        if (loadOptions == null) {\n            this.loadOptions = {};\n        }\n        this.resourceManager = new ResourceManager();\n    }\n    // Returns the version information for the tensorflow model GraphDef.\n    get modelVersion() {\n        return this.version;\n    }\n    get inputNodes() {\n        return this.executor.inputNodes;\n    }\n    get outputNodes() {\n        return this.executor.outputNodes;\n    }\n    get inputs() {\n        return this.executor.inputs;\n    }\n    get outputs() {\n        return this.executor.outputs;\n    }\n    get weights() {\n        return this.executor.weightMap;\n    }\n    get metadata() {\n        return this.artifacts.userDefinedMetadata;\n    }\n    get modelSignature() {\n        return this.signature;\n    }\n    findIOHandler() {\n        const path = this.modelUrl;\n        if (path.load != null) {\n            // Path is an IO Handler.\n            this.handler = path;\n        }\n        else if (this.loadOptions.requestInit != null) {\n            this.handler = io.browserHTTPRequest(path, this.loadOptions);\n        }\n        else {\n            const handlers = io.getLoadHandlers(path, this.loadOptions);\n            if (handlers.length === 0) {\n                // For backward compatibility: if no load handler can be found,\n                // assume it is a relative http path.\n                handlers.push(io.browserHTTPRequest(path, this.loadOptions));\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) load handlers for ` +\n                    `URL '${[path]}'`);\n            }\n            this.handler = handlers[0];\n        }\n    }\n    /**\n     * Loads the model and weight files, construct the in memory weight map and\n     * compile the inference graph.\n     */\n    async load() {\n        this.findIOHandler();\n        if (this.handler.load == null) {\n            throw new Error('Cannot proceed with model loading because the IOHandler provided ' +\n                'does not have the `load` method implemented.');\n        }\n        const artifacts = await this.handler.load();\n        return this.loadSync(artifacts);\n    }\n    /**\n     * Synchronously construct the in memory weight map and\n     * compile the inference graph. Also initialize hashtable if any.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    loadSync(artifacts) {\n        this.artifacts = artifacts;\n        const graph = this.artifacts.modelTopology;\n        let signature;\n        if (this.artifacts.userDefinedMetadata != null &&\n            this.artifacts.userDefinedMetadata.signature != null) {\n            signature = // tslint:disable-next-line:no-any\n                this.artifacts.userDefinedMetadata.signature;\n        }\n        else {\n            signature = this.artifacts.signature;\n        }\n        this.signature = signature;\n        this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\n        const weightMap = io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n        this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, this.signature));\n        this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n        // Attach a model-level resourceManager to each executor to share resources,\n        // such as `HashTable`.\n        this.executor.resourceManager = this.resourceManager;\n        if (artifacts.modelInitializer != null &&\n            artifacts.modelInitializer.node != null) {\n            const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n            this.initializer = new GraphExecutor(initializer);\n            this.initializer.weightMap = this.executor.weightMap;\n            // Attach a model-level resourceManager to the initializer, the\n            // hashTables created from when executing the initializer will be stored\n            // in the resourceManager.\n            this.initializer.resourceManager = this.resourceManager;\n            this.initializer.executeAsync({}, []);\n        }\n        return true;\n    }\n    /**\n     * Save the configuration and/or weights of the GraphModel.\n     *\n     * An `IOHandler` is an object that has a `save` method of the proper\n     * signature defined. The `save` method manages the storing or\n     * transmission of serialized data (\"artifacts\") that represent the\n     * model's topology and weights onto or via a specific medium, such as\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\n     * requests to a server. TensorFlow.js provides `IOHandler`\n     * implementations for a number of frequently used saving mediums, such as\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\n     * for more details.\n     *\n     * This method also allows you to refer to certain types of `IOHandler`s\n     * as URL-like string shortcuts, such as 'localstorage://' and\n     * 'indexeddb://'.\n     *\n     * Example 1: Save `model`'s topology and weights to browser [local\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\n     * then load it back.\n     *\n     * ```js\n     * const modelUrl =\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n     * const model = await tf.loadGraphModel(modelUrl);\n     * const zeros = tf.zeros([1, 224, 224, 3]);\n     * model.predict(zeros).print();\n     *\n     * const saveResults = await model.save('localstorage://my-model-1');\n     *\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\n     * console.log('Prediction from loaded model:');\n     * model.predict(zeros).print();\n     * ```\n     *\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\n     * scheme-based string shortcut for `IOHandler`.\n     * @param config Options for saving the model.\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\n     * the saving, such as byte sizes of the saved artifacts for the model's\n     *   topology and weight values.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\n     */\n    async save(handlerOrURL, config) {\n        if (typeof handlerOrURL === 'string') {\n            const handlers = io.getSaveHandlers(handlerOrURL);\n            if (handlers.length === 0) {\n                throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\n            }\n            else if (handlers.length > 1) {\n                throw new Error(`Found more than one (${handlers.length}) save handlers for ` +\n                    `URL '${handlerOrURL}'`);\n            }\n            handlerOrURL = handlers[0];\n        }\n        if (handlerOrURL.save == null) {\n            throw new Error('GraphModel.save() cannot proceed because the IOHandler ' +\n                'provided does not have the `save` attribute defined.');\n        }\n        return handlerOrURL.save(this.artifacts);\n    }\n    /**\n     * Execute the inference for the input tensors.\n     *\n     * @param input The input tensors, when there is single input for the model,\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\n     * inputs params should be in either `tf.Tensor`[] if the input order is\n     * fixed, or otherwise NamedTensorMap format.\n     *\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\n     * follow the\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\n     *\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\n     * input type. For example, given the graph\n     *    InputNode => Intermediate => OutputNode,\n     * you can execute the subgraph Intermediate => OutputNode by calling\n     *    model.execute('IntermediateNode' : tf.tensor(...));\n     *\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\n     * state needs to be fed manually.\n     *\n     * For batch inference execution, the tensors for each input need to be\n     * concatenated together. For example with mobilenet, the required input shape\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\n     * If we are provide a batched data of 100 images, the input tensor should be\n     * in the shape of [100, 244, 244, 3].\n     *\n     * @param config Prediction configuration for specifying the batch size and\n     * output node names. Currently the batch size option is ignored for graph\n     * model.\n     *\n     * @returns Inference result tensors. The output would be single `tf.Tensor`\n     * if model has single output node, otherwise Tensor[] or NamedTensorMap[]\n     * will be returned for model with multiple outputs.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    predict(inputs, config) {\n        return this.execute(inputs, this.outputNodes);\n    }\n    normalizeInputs(inputs) {\n        if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n            // The input is already a NamedTensorMap.\n            return inputs;\n        }\n        inputs = Array.isArray(inputs) ? inputs : [inputs];\n        if (inputs.length !== this.inputNodes.length) {\n            throw new Error('Input tensor count mismatch,' +\n                `the graph model has ${this.inputNodes.length} placeholders, ` +\n                `while there are ${inputs.length} input tensors.`);\n        }\n        return this.inputNodes.reduce((map, inputName, i) => {\n            map[inputName] = inputs[i];\n            return map;\n        }, {});\n    }\n    normalizeOutputs(outputs) {\n        outputs = outputs || this.outputNodes;\n        return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n    /**\n     * Executes inference for the model for given input tensors.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the Tensorflow model, if no\n     * outputs are specified, the default outputs of the model would be used.\n     * You can inspect intermediate nodes of the model by adding them to the\n     * outputs array.\n     *\n     * @returns A single tensor if provided with a single output or no outputs\n     * are provided and there is only one default output, otherwise return a\n     * tensor array. The order of the tensor array is the same as the outputs\n     * if provided, otherwise the order of outputNodes attribute of the model.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    execute(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = this.executor.execute(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    /**\n     * Executes inference for the model for given input tensors in async\n     * fashion, use this method when your model contains control flow ops.\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\n     * model, keyed by the input node names.\n     * @param outputs output node name from the Tensorflow model, if no outputs\n     * are specified, the default outputs of the model would be used. You can\n     * inspect intermediate nodes of the model by adding them to the outputs\n     * array.\n     *\n     * @returns A Promise of single tensor if provided with a single output or\n     * no outputs are provided and there is only one default output, otherwise\n     * return a tensor map.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    async executeAsync(inputs, outputs) {\n        inputs = this.normalizeInputs(inputs);\n        outputs = this.normalizeOutputs(outputs);\n        const result = await this.executor.executeAsync(inputs, outputs);\n        return result.length > 1 ? result : result[0];\n    }\n    convertTensorMapToTensorsMap(map) {\n        return Object.keys(map).reduce((newMap, key) => {\n            newMap[key] = [map[key]];\n            return newMap;\n        }, {});\n    }\n    /**\n     * Releases the memory used by the weight tensors and resourceManager.\n     *\n     * @doc {heading: 'Models', subheading: 'Classes'}\n     */\n    dispose() {\n        this.executor.dispose();\n        if (this.initializer) {\n            this.initializer.dispose();\n        }\n        this.resourceManager.dispose();\n    }\n}\n/**\n * Load a graph model given a URL to the model definition.\n *\n * Example of loading MobileNetV2 from a URL and making a prediction with a\n * zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\n * const model = await tf.loadGraphModel(modelUrl);\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n *\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction with\n * a zeros input:\n *\n * ```js\n * const modelUrl =\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\n * const zeros = tf.zeros([1, 224, 224, 3]);\n * model.predict(zeros).print();\n * ```\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\n * @param options Options for the HTTP request, which allows to send credentials\n *    and custom headers.\n *\n * @doc {heading: 'Models', subheading: 'Loading'}\n */\nexport async function loadGraphModel(modelUrl, options = {}) {\n    if (modelUrl == null) {\n        throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\n            'or an IOHandler that loads the model');\n    }\n    if (options == null) {\n        options = {};\n    }\n    if (options.fromTFHub) {\n        if (modelUrl.load == null) {\n            if (!modelUrl.endsWith('/')) {\n                modelUrl = modelUrl + '/';\n            }\n            modelUrl = `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\n        }\n    }\n    const model = new GraphModel(modelUrl, options);\n    await model.load();\n    return model;\n}\n//# sourceMappingURL=graph_model.js.map"]},"metadata":{},"sourceType":"module"}