{"ast":null,"code":"import _toConsumableArray from \"@babel/runtime/helpers/toConsumableArray\";\nimport _extends from \"@babel/runtime/helpers/extends\";\nimport _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _get from \"@babel/runtime/helpers/get\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from \"../backend/tfjs_backend\";\nimport { nameScope } from \"../common\";\nimport { InputSpec, Layer, SymbolicTensor } from \"../engine/topology\";\nimport { NotImplementedError, ValueError } from \"../errors\";\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from \"../keras_format/common\";\nimport * as generic_utils from \"../utils/generic_utils\";\nimport { getExactlyOneShape, getExactlyOneTensor } from \"../utils/types_utils\";\nimport { rnn, standardizeArgs } from \"./recurrent\";\nimport { deserialize } from \"./serialization\";\nexport var Wrapper = function (_Layer) {\n  _inherits(Wrapper, _Layer);\n\n  var _super = _createSuper(Wrapper);\n\n  function Wrapper(args) {\n    var _this;\n\n    _classCallCheck(this, Wrapper);\n\n    _this = _super.call(this, args);\n    _this.layer = args.layer;\n    return _this;\n  }\n\n  _createClass(Wrapper, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      this.built = true;\n    }\n  }, {\n    key: \"trainable\",\n    get: function get() {\n      if (this.layer != null) {\n        return this.layer.trainable;\n      } else {\n        return false;\n      }\n    },\n    set: function set(value) {\n      if (this.layer != null) {\n        this.layer.trainable = value;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.layer.trainableWeights;\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.layer.nonTrainableWeights;\n    }\n  }, {\n    key: \"updates\",\n    get: function get() {\n      return this.layer._updates;\n    }\n  }, {\n    key: \"losses\",\n    get: function get() {\n      return this.layer.losses;\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.layer.getWeights();\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      this.layer.setWeights(weights);\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'layer': {\n          'className': this.layer.getClassName(),\n          'config': this.layer.getConfig()\n        }\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Wrapper.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Wrapper.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.layer != null) {\n        this.layer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var customObjects = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : {};\n      var layerConfig = config['layer'];\n      var layer = deserialize(layerConfig, customObjects);\n      delete config['layer'];\n      var newConfig = {\n        layer: layer\n      };\n\n      _extends(newConfig, config);\n\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Wrapper;\n}(Layer);\nexport var TimeDistributed = function (_Wrapper) {\n  _inherits(TimeDistributed, _Wrapper);\n\n  var _super2 = _createSuper(TimeDistributed);\n\n  function TimeDistributed(args) {\n    var _this2;\n\n    _classCallCheck(this, TimeDistributed);\n\n    _this2 = _super2.call(this, args);\n    _this2.supportsMasking = true;\n    return _this2;\n  }\n\n  _createClass(TimeDistributed, [{\n    key: \"build\",\n    value: function build(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n\n      if (inputShape.length < 3) {\n        throw new ValueError(\"TimeDistributed layer expects an input shape >= 3D, but received \" + (\"input shape \" + JSON.stringify(inputShape)));\n      }\n\n      this.inputSpec = [{\n        shape: inputShape\n      }];\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n\n      if (!this.layer.built) {\n        this.layer.build(childInputShape);\n        this.layer.built = true;\n      }\n\n      _get(_getPrototypeOf(TimeDistributed.prototype), \"build\", this).call(this, inputShape);\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      inputShape = getExactlyOneShape(inputShape);\n      var childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n      var childOutputShape = this.layer.computeOutputShape(childInputShape);\n      var timesteps = inputShape[1];\n      return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this3 = this;\n\n      return tidy(function () {\n        inputs = getExactlyOneTensor(inputs);\n\n        var step = function step(inputs, states) {\n          var output = getExactlyOneTensor(_this3.layer.call(inputs, kwargs));\n          return [output, []];\n        };\n\n        var rnnOutputs = rnn(step, inputs, [], false, null, null, false, true);\n        var y = rnnOutputs[1];\n        return y;\n      });\n    }\n  }]);\n\n  return TimeDistributed;\n}(Wrapper);\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n  generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nvar DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport var Bidirectional = function (_Wrapper2) {\n  _inherits(Bidirectional, _Wrapper2);\n\n  var _super3 = _createSuper(Bidirectional);\n\n  function Bidirectional(args) {\n    var _this4;\n\n    _classCallCheck(this, Bidirectional);\n\n    _this4 = _super3.call(this, args);\n    var layerConfig = args.layer.getConfig();\n    var forwDict = {};\n    forwDict['className'] = args.layer.getClassName();\n    forwDict['config'] = layerConfig;\n    _this4.forwardLayer = deserialize(forwDict);\n    layerConfig['goBackwards'] = layerConfig['goBackwards'] === true ? false : true;\n    var backDict = {};\n    backDict['className'] = args.layer.getClassName();\n    backDict['config'] = layerConfig;\n    _this4.backwardLayer = deserialize(backDict);\n    _this4.forwardLayer.name = 'forward_' + _this4.forwardLayer.name;\n    _this4.backwardLayer.name = 'backward_' + _this4.backwardLayer.name;\n    _this4.mergeMode = args.mergeMode === undefined ? DEFAULT_BIDIRECTIONAL_MERGE_MODE : args.mergeMode;\n    checkBidirectionalMergeMode(_this4.mergeMode);\n\n    if (args.weights) {\n      throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n    }\n\n    _this4._stateful = args.layer.stateful;\n    _this4.returnSequences = args.layer.returnSequences;\n    _this4.returnState = args.layer.returnState;\n    _this4.supportsMasking = true;\n    _this4._trainable = true;\n    _this4.inputSpec = args.layer.inputSpec;\n    _this4.numConstants = null;\n    return _this4;\n  }\n\n  _createClass(Bidirectional, [{\n    key: \"trainable\",\n    get: function get() {\n      return this._trainable;\n    },\n    set: function set(value) {\n      this._trainable = value;\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.trainable = value;\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.trainable = value;\n      }\n    }\n  }, {\n    key: \"getWeights\",\n    value: function getWeights() {\n      return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n  }, {\n    key: \"setWeights\",\n    value: function setWeights(weights) {\n      var numWeights = weights.length;\n      var numeightsOver2 = Math.floor(numWeights / 2);\n      this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n      this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      var layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n\n      if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n        layerShapes = [layerShapes];\n      }\n\n      layerShapes = layerShapes;\n      var outputShape;\n      var outputShapes;\n      var stateShape;\n\n      if (this.returnState) {\n        stateShape = layerShapes.slice(1);\n        outputShape = layerShapes[0];\n      } else {\n        outputShape = layerShapes[0];\n      }\n\n      outputShape = outputShape;\n\n      if (this.mergeMode === 'concat') {\n        outputShape[outputShape.length - 1] *= 2;\n        outputShapes = [outputShape];\n      } else if (this.mergeMode == null) {\n        outputShapes = [outputShape, outputShape.slice()];\n      } else {\n        outputShapes = [outputShape];\n      }\n\n      if (this.returnState) {\n        if (this.mergeMode == null) {\n          return outputShapes.concat(stateShape).concat(stateShape.slice());\n        }\n\n        return [outputShape].concat(stateShape).concat(stateShape.slice());\n      }\n\n      return generic_utils.singletonOrArray(outputShapes);\n    }\n  }, {\n    key: \"apply\",\n    value: function apply(inputs, kwargs) {\n      var initialState = kwargs == null ? null : kwargs['initialState'];\n      var constants = kwargs == null ? null : kwargs['constants'];\n\n      if (kwargs == null) {\n        kwargs = {};\n      }\n\n      var standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n      inputs = standardized.inputs;\n      initialState = standardized.initialState;\n      constants = standardized.constants;\n\n      if (Array.isArray(inputs)) {\n        initialState = inputs.slice(1);\n        inputs = inputs[0];\n      }\n\n      if ((initialState == null || initialState.length === 0) && constants == null) {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n\n      var additionalInputs = [];\n      var additionalSpecs = [];\n\n      if (initialState != null) {\n        var numStates = initialState.length;\n\n        if (numStates % 2 > 0) {\n          throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' + 'the state should be an Array containing the states of ' + 'the underlying RNNs.');\n        }\n\n        kwargs['initialState'] = initialState;\n        additionalInputs.push.apply(additionalInputs, _toConsumableArray(initialState));\n        var stateSpecs = initialState.map(function (state) {\n          return new InputSpec({\n            shape: state.shape\n          });\n        });\n        this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n        this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n        additionalSpecs.push.apply(additionalSpecs, _toConsumableArray(stateSpecs));\n      }\n\n      if (constants != null) {\n        throw new NotImplementedError('Support for constants in Bidirectional layers is not ' + 'implemented yet.');\n      }\n\n      var isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n\n      for (var _i = 0, _additionalInputs = additionalInputs; _i < _additionalInputs.length; _i++) {\n        var tensor = _additionalInputs[_i];\n\n        if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n          throw new ValueError('The initial state of a Bidirectional layer cannot be ' + 'specified as a mix of symbolic and non-symbolic tensors');\n        }\n      }\n\n      if (isSymbolicTensor) {\n        var fullInput = [inputs].concat(additionalInputs);\n        var fullInputSpec = this.inputSpec.concat(additionalSpecs);\n        var originalInputSpec = this.inputSpec;\n        this.inputSpec = fullInputSpec;\n\n        var output = _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, fullInput, kwargs);\n\n        this.inputSpec = originalInputSpec;\n        return output;\n      } else {\n        return _get(_getPrototypeOf(Bidirectional.prototype), \"apply\", this).call(this, inputs, kwargs);\n      }\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this5 = this;\n\n      return tidy(function () {\n        var initialState = kwargs['initialState'];\n        var y;\n        var yRev;\n\n        if (initialState == null) {\n          y = _this5.forwardLayer.call(inputs, kwargs);\n          yRev = _this5.backwardLayer.call(inputs, kwargs);\n        } else {\n          var forwardState = initialState.slice(0, initialState.length / 2);\n          var backwardState = initialState.slice(initialState.length / 2);\n          y = _this5.forwardLayer.call(inputs, _extends(kwargs, {\n            initialState: forwardState\n          }));\n          yRev = _this5.backwardLayer.call(inputs, _extends(kwargs, {\n            initialState: backwardState\n          }));\n        }\n\n        var states;\n\n        if (_this5.returnState) {\n          if (Array.isArray(y)) {\n            states = y.slice(1).concat(yRev.slice(1));\n          } else {}\n\n          y = y[0];\n          yRev = yRev[0];\n        }\n\n        if (_this5.returnSequences) {\n          yRev = tfc.reverse(yRev, 1);\n        }\n\n        var output;\n\n        if (_this5.mergeMode === 'concat') {\n          output = K.concatenate([y, yRev]);\n        } else if (_this5.mergeMode === 'sum') {\n          output = tfc.add(y, yRev);\n        } else if (_this5.mergeMode === 'ave') {\n          output = tfc.mul(.5, tfc.add(y, yRev));\n        } else if (_this5.mergeMode === 'mul') {\n          output = tfc.mul(y, yRev);\n        } else if (_this5.mergeMode == null) {\n          output = [y, yRev];\n        }\n\n        if (_this5.returnState) {\n          if (_this5.mergeMode == null) {\n            return output.concat(states);\n          }\n\n          return [output].concat(states);\n        }\n\n        return output;\n      });\n    }\n  }, {\n    key: \"resetStates\",\n    value: function resetStates(states) {\n      this.forwardLayer.resetStates();\n      this.backwardLayer.resetStates();\n    }\n  }, {\n    key: \"build\",\n    value: function build(inputShape) {\n      var _this6 = this;\n\n      nameScope(this.forwardLayer.name, function () {\n        _this6.forwardLayer.build(inputShape);\n      });\n      nameScope(this.backwardLayer.name, function () {\n        _this6.backwardLayer.build(inputShape);\n      });\n      this.built = true;\n    }\n  }, {\n    key: \"computeMask\",\n    value: function computeMask(inputs, mask) {\n      if (Array.isArray(mask)) {\n        mask = mask[0];\n      }\n\n      var outputMask;\n\n      if (this.returnSequences) {\n        if (this.mergeMode == null) {\n          outputMask = [mask, mask];\n        } else {\n          outputMask = mask;\n        }\n      } else {\n        if (this.mergeMode == null) {\n          outputMask = [null, null];\n        } else {\n          outputMask = null;\n        }\n      }\n\n      if (this.returnState) {\n        var states = this.forwardLayer.states;\n        var stateMask = states.map(function (state) {\n          return null;\n        });\n\n        if (Array.isArray(outputMask)) {\n          return outputMask.concat(stateMask).concat(stateMask);\n        } else {\n          return [outputMask].concat(stateMask).concat(stateMask);\n        }\n      } else {\n        return outputMask;\n      }\n    }\n  }, {\n    key: \"trainableWeights\",\n    get: function get() {\n      return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n  }, {\n    key: \"nonTrainableWeights\",\n    get: function get() {\n      return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n  }, {\n    key: \"setFastWeightInitDuringBuild\",\n    value: function setFastWeightInitDuringBuild(value) {\n      _get(_getPrototypeOf(Bidirectional.prototype), \"setFastWeightInitDuringBuild\", this).call(this, value);\n\n      if (this.forwardLayer != null) {\n        this.forwardLayer.setFastWeightInitDuringBuild(value);\n      }\n\n      if (this.backwardLayer != null) {\n        this.backwardLayer.setFastWeightInitDuringBuild(value);\n      }\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var config = {\n        'mergeMode': this.mergeMode\n      };\n\n      var baseConfig = _get(_getPrototypeOf(Bidirectional.prototype), \"getConfig\", this).call(this);\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }], [{\n    key: \"fromConfig\",\n    value: function fromConfig(cls, config) {\n      var rnnLayer = deserialize(config['layer']);\n      delete config['layer'];\n\n      if (config['numConstants'] != null) {\n        throw new NotImplementedError(\"Deserialization of a Bidirectional layer with numConstants \" + \"present is not supported yet.\");\n      }\n\n      var newConfig = config;\n      newConfig['layer'] = rnnLayer;\n      return new cls(newConfig);\n    }\n  }]);\n\n  return Bidirectional;\n}(Wrapper);\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);","map":{"version":3,"sources":["../../src/layers/wrappers.ts"],"names":[],"mappings":";;;;;;;;;;;;;AAAA;;;;;;;;AAQG;AAMH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,aAAR,EAA+B,IAA/B,QAA0C,uBAA1C;AACA,OAAO,KAAK,CAAZ;AACA,SAAQ,SAAR;AACA,SAAQ,SAAR,EAAmB,KAAnB,EAAqC,cAArC;AACA,SAAQ,mBAAR,EAA6B,UAA7B;AACA,SAAuC,+BAAvC;AAGA,OAAO,KAAK,aAAZ;AACA,SAAQ,kBAAR,EAA4B,mBAA5B;AAGA,SAAQ,GAAR,EAAkB,eAAlB;AACA,SAAQ,WAAR;AAgBA,WAAsB,OAAtB;EAAA;;EAAA;;EAGE,iBAAY,IAAZ,EAAkC;IAAA;;IAAA;;IAQhC,0BAAM,IAAN;IACA,MAAK,KAAL,GAAa,IAAI,CAAC,KAAlB;IATgC;EAUjC;;EAbH;IAAA;IAAA,OAeE,eAAM,UAAN,EAA+B;MAC7B,KAAK,KAAL,GAAa,IAAb;IACD;EAjBH;IAAA;IAAA,KAqBE,eAAa;MAIX,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;QACtB,OAAO,KAAK,KAAL,CAAW,SAAlB;MACD,CAFD,MAEO;QACL,OAAO,KAAP;MACD;IACF,CA9BH;IAAA,KAgCE,aAAc,KAAd,EAA4B;MAI1B,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;QACtB,KAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;MACD;IACF;EAvCH;IAAA;IAAA,KAyCE,eAAoB;MAClB,OAAO,KAAK,KAAL,CAAW,gBAAlB;IACD;EA3CH;IAAA;IAAA,KA8CE,eAAuB;MACrB,OAAO,KAAK,KAAL,CAAW,mBAAlB;IACD;EAhDH;IAAA;IAAA,KAmDE,eAAW;MAET,OAAQ,KAAK,KAAL,CAAmB,QAA3B;IACD;EAtDH;IAAA;IAAA,KA0DE,eAAU;MACR,OAAO,KAAK,KAAL,CAAW,MAAlB;IACD;EA5DH;IAAA;IAAA,OAgEE,sBAAU;MACR,OAAO,KAAK,KAAL,CAAW,UAAX,EAAP;IACD;EAlEH;IAAA;IAAA,OAoEE,oBAAW,OAAX,EAA4B;MAC1B,KAAK,KAAL,CAAW,UAAX,CAAsB,OAAtB;IACD;EAtEH;IAAA;IAAA,OAwEE,qBAAS;MACP,IAAM,MAAM,GAA6B;QACvC,SAAS;UACP,aAAa,KAAK,KAAL,CAAW,YAAX,EADN;UAEP,UAAU,KAAK,KAAL,CAAW,SAAX;QAFH;MAD8B,CAAzC;;MAMA,IAAM,UAAU,yEAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EAlFH;IAAA;IAAA,OAoFE,sCAA6B,KAA7B,EAA2C;MACzC,0FAAmC,KAAnC;;MACA,IAAI,KAAK,KAAL,IAAc,IAAlB,EAAwB;QACtB,KAAK,KAAL,CAAW,4BAAX,CAAwC,KAAxC;MACD;IACF;EAzFH;IAAA;IAAA,OA4FE,oBACI,GADJ,EAEI,MAFJ,EAGkD;MAAA,IAA9C,aAA8C,uEAA9B,EAA8B;MAChD,IAAM,WAAW,GAAG,MAAM,CAAC,OAAD,CAA1B;MACA,IAAM,KAAK,GAAG,WAAW,CAAC,WAAD,EAAc,aAAd,CAAzB;MACA,OAAO,MAAM,CAAC,OAAD,CAAb;MACA,IAAM,SAAS,GAAG;QAAC,KAAK,EAAL;MAAD,CAAlB;;MACA,SAAc,SAAd,EAAyB,MAAzB;;MACA,OAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;IACD;EAtGH;;EAAA;AAAA,EAAsC,KAAtC;AAyGA,WAAa,eAAb;EAAA;;EAAA;;EAGE,yBAAY,IAAZ,EAAkC;IAAA;;IAAA;;IAChC,4BAAM,IAAN;IACA,OAAK,eAAL,GAAuB,IAAvB;IAFgC;EAGjC;;EANH;IAAA;IAAA,OAQE,eAAM,UAAN,EAA+B;MAC7B,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;;MACA,IAAI,UAAU,CAAC,MAAX,GAAoB,CAAxB,EAA2B;QACzB,MAAM,IAAI,UAAJ,CACF,wFACe,IAAI,CAAC,SAAL,CAAe,UAAf,CADf,CADE,CAAN;MAGD;;MACD,KAAK,SAAL,GAAiB,CAAC;QAAC,KAAK,EAAE;MAAR,CAAD,CAAjB;MACA,IAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;;MACA,IAAI,CAAC,KAAK,KAAL,CAAW,KAAhB,EAAuB;QACrB,KAAK,KAAL,CAAW,KAAX,CAAiB,eAAjB;QACA,KAAK,KAAL,CAAW,KAAX,GAAmB,IAAnB;MACD;;MACD,2EAAY,UAAZ;IACD;EAtBH;IAAA;IAAA,OAwBE,4BAAmB,UAAnB,EAA4C;MAC1C,UAAU,GAAG,kBAAkB,CAAC,UAAD,CAA/B;MACA,IAAM,eAAe,GAAG,CAAC,UAAU,CAAC,CAAD,CAAX,EAAgB,MAAhB,CAAuB,UAAU,CAAC,KAAX,CAAiB,CAAjB,CAAvB,CAAxB;MACA,IAAM,gBAAgB,GAClB,KAAK,KAAL,CAAW,kBAAX,CAA8B,eAA9B,CADJ;MAEA,IAAM,SAAS,GAAG,UAAU,CAAC,CAAD,CAA5B;MACA,OAAO,CAAC,gBAAgB,CAAC,CAAD,CAAjB,EAAsB,SAAtB,EAAiC,MAAjC,CAAwC,gBAAgB,CAAC,KAAjB,CAAuB,CAAvB,CAAxC,CAAP;IACD;EA/BH;IAAA;IAAA,OAiCE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QAEf,MAAM,GAAG,mBAAmB,CAAC,MAAD,CAA5B;;QAIA,IAAM,IAAI,GAAoB,SAAxB,IAAwB,CAAC,MAAD,EAAiB,MAAjB,EAAqC;UAKjE,IAAM,MAAM,GAAG,mBAAmB,CAAC,MAAI,CAAC,KAAL,CAAW,IAAX,CAAgB,MAAhB,EAAwB,MAAxB,CAAD,CAAlC;UACA,OAAO,CAAC,MAAD,EAAS,EAAT,CAAP;QACD,CAPD;;QAQA,IAAM,UAAU,GACZ,GAAG,CAAC,IAAD,EAAO,MAAP,EAAe,EAAf,EAAmB,KAAnB,EAA4C,IAA5C,EACC,IADD,EACuB,KADvB,EAEC,IAFD,CADP;QAIA,IAAM,CAAC,GAAG,UAAU,CAAC,CAAD,CAApB;QAGA,OAAO,CAAP;MACD,CAtBU,CAAX;IAuBD;EAzDH;;EAAA;AAAA,EAAqC,OAArC;AAES,eAAA,CAAA,SAAA,GAAY,iBAAZ;AA2DT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAEA,OAAM,SAAU,2BAAV,CAAsC,KAAtC,EAAoD;EACxD,aAAa,CAAC,yBAAd,CACI,+BADJ,EACqC,wBADrC,EAC+D,KAD/D;AAED;AAkBD,IAAM,gCAAgC,GAA2B,QAAjE;AAEA,WAAa,aAAb;EAAA;;EAAA;;EAWE,uBAAY,IAAZ,EAAwC;IAAA;;IAAA;;IACtC,4BAAM,IAAN;IAUA,IAAM,WAAW,GAAG,IAAI,CAAC,KAAL,CAAW,SAAX,EAApB;IACA,IAAM,QAAQ,GAA6B,EAA3C;IACA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;IACA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;IACA,OAAK,YAAL,GAAoB,WAAW,CAAC,QAAD,CAA/B;IACA,WAAW,CAAC,aAAD,CAAX,GACI,WAAW,CAAC,aAAD,CAAX,KAA+B,IAA/B,GAAsC,KAAtC,GAA8C,IADlD;IAEA,IAAM,QAAQ,GAA6B,EAA3C;IACA,QAAQ,CAAC,WAAD,CAAR,GAAwB,IAAI,CAAC,KAAL,CAAW,YAAX,EAAxB;IACA,QAAQ,CAAC,QAAD,CAAR,GAAqB,WAArB;IACA,OAAK,aAAL,GAAqB,WAAW,CAAC,QAAD,CAAhC;IACA,OAAK,YAAL,CAAkB,IAAlB,GAAyB,aAAa,OAAK,YAAL,CAAkB,IAAxD;IACA,OAAK,aAAL,CAAmB,IAAnB,GAA0B,cAAc,OAAK,aAAL,CAAmB,IAA3D;IAEA,OAAK,SAAL,GAAiB,IAAI,CAAC,SAAL,KAAmB,SAAnB,GACb,gCADa,GAEb,IAAI,CAAC,SAFT;IAGA,2BAA2B,CAAC,OAAK,SAAN,CAA3B;;IACA,IAAI,IAAI,CAAC,OAAT,EAAkB;MAChB,MAAM,IAAI,mBAAJ,CACF,iEADE,CAAN;IAED;;IACD,OAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,QAA5B;IACA,OAAK,eAAL,GAAuB,IAAI,CAAC,KAAL,CAAW,eAAlC;IACA,OAAK,WAAL,GAAmB,IAAI,CAAC,KAAL,CAAW,WAA9B;IACA,OAAK,eAAL,GAAuB,IAAvB;IACA,OAAK,UAAL,GAAkB,IAAlB;IACA,OAAK,SAAL,GAAiB,IAAI,CAAC,KAAL,CAAW,SAA5B;IACA,OAAK,YAAL,GAAoB,IAApB;IAvCsC;EAwCvC;;EAnDH;IAAA;IAAA,KAqDE,eAAa;MACX,OAAO,KAAK,UAAZ;IACD,CAvDH;IAAA,KAyDE,aAAc,KAAd,EAA4B;MAI1B,KAAK,UAAL,GAAkB,KAAlB;;MACA,IAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;QAC7B,KAAK,YAAL,CAAkB,SAAlB,GAA8B,KAA9B;MACD;;MACD,IAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;QAC9B,KAAK,aAAL,CAAmB,SAAnB,GAA+B,KAA/B;MACD;IACF;EApEH;IAAA;IAAA,OAsEE,sBAAU;MACR,OAAO,KAAK,YAAL,CAAkB,UAAlB,GAA+B,MAA/B,CACH,KAAK,aAAL,CAAmB,UAAnB,EADG,CAAP;IAED;EAzEH;IAAA;IAAA,OA2EE,oBAAW,OAAX,EAA4B;MAC1B,IAAM,UAAU,GAAG,OAAO,CAAC,MAA3B;MACA,IAAM,cAAc,GAAG,IAAI,CAAC,KAAL,CAAW,UAAU,GAAG,CAAxB,CAAvB;MACA,KAAK,YAAL,CAAkB,UAAlB,CAA6B,OAAO,CAAC,KAAR,CAAc,CAAd,EAAiB,cAAjB,CAA7B;MACA,KAAK,aAAL,CAAmB,UAAnB,CAA8B,OAAO,CAAC,KAAR,CAAc,cAAd,CAA9B;IACD;EAhFH;IAAA;IAAA,OAkFE,4BAAmB,UAAnB,EAA4C;MAC1C,IAAI,WAAW,GACX,KAAK,YAAL,CAAkB,kBAAlB,CAAqC,UAArC,CADJ;;MAEA,IAAI,EAAE,KAAK,CAAC,OAAN,CAAc,WAAd,KAA8B,KAAK,CAAC,OAAN,CAAc,WAAW,CAAC,CAAD,CAAzB,CAAhC,CAAJ,EAAoE;QAClE,WAAW,GAAG,CAAC,WAAD,CAAd;MACD;;MACD,WAAW,GAAG,WAAd;MAEA,IAAI,WAAJ;MACA,IAAI,YAAJ;MACA,IAAI,UAAJ;;MACA,IAAI,KAAK,WAAT,EAAsB;QACpB,UAAU,GAAG,WAAW,CAAC,KAAZ,CAAkB,CAAlB,CAAb;QACA,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;MACD,CAHD,MAGO;QACL,WAAW,GAAG,WAAW,CAAC,CAAD,CAAzB;MACD;;MACD,WAAW,GAAG,WAAd;;MACA,IAAI,KAAK,SAAL,KAAmB,QAAvB,EAAiC;QAC/B,WAAW,CAAC,WAAW,CAAC,MAAZ,GAAqB,CAAtB,CAAX,IAAuC,CAAvC;QACA,YAAY,GAAG,CAAC,WAAD,CAAf;MACD,CAHD,MAGO,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;QACjC,YAAY,GAAG,CAAC,WAAD,EAAc,WAAW,CAAC,KAAZ,EAAd,CAAf;MACD,CAFM,MAEA;QACL,YAAY,GAAG,CAAC,WAAD,CAAf;MACD;;MAED,IAAI,KAAK,WAAT,EAAsB;QACpB,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;UAC1B,OAAO,YAAY,CAAC,MAAb,CAAoB,UAApB,EAAgC,MAAhC,CAAuC,UAAU,CAAC,KAAX,EAAvC,CAAP;QACD;;QACD,OAAO,CAAC,WAAD,EAAc,MAAd,CAAqB,UAArB,EAAiC,MAAjC,CAAwC,UAAU,CAAC,KAAX,EAAxC,CAAP;MACD;;MACD,OAAO,aAAa,CAAC,gBAAd,CAA+B,YAA/B,CAAP;IACD;EApHH;IAAA;IAAA,OAsHE,eACI,MADJ,EAEI,MAFJ,EAEmB;MACjB,IAAI,YAAY,GACZ,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,cAAD,CADlC;MAEA,IAAI,SAAS,GACT,MAAM,IAAI,IAAV,GAAiB,IAAjB,GAAwB,MAAM,CAAC,WAAD,CADlC;;MAEA,IAAI,MAAM,IAAI,IAAd,EAAoB;QAClB,MAAM,GAAG,EAAT;MACD;;MACD,IAAM,YAAY,GACd,eAAe,CAAC,MAAD,EAAS,YAAT,EAAuB,SAAvB,EAAkC,KAAK,YAAvC,CADnB;MAEA,MAAM,GAAG,YAAY,CAAC,MAAtB;MACA,YAAY,GAAG,YAAY,CAAC,YAA5B;MACA,SAAS,GAAG,YAAY,CAAC,SAAzB;;MAEA,IAAI,KAAK,CAAC,OAAN,CAAc,MAAd,CAAJ,EAA2B;QACzB,YAAY,GAAI,MAAsC,CAAC,KAAvC,CAA6C,CAA7C,CAAhB;QACA,MAAM,GAAI,MAAsC,CAAC,CAAD,CAAhD;MACD;;MAED,IAAI,CAAC,YAAY,IAAI,IAAhB,IAAwB,YAAY,CAAC,MAAb,KAAwB,CAAjD,KACA,SAAS,IAAI,IADjB,EACuB;QACrB,gFAAmB,MAAnB,EAA2B,MAA3B;MACD;;MACD,IAAM,gBAAgB,GAAiC,EAAvD;MACA,IAAM,eAAe,GAAgB,EAArC;;MACA,IAAI,YAAY,IAAI,IAApB,EAA0B;QACxB,IAAM,SAAS,GAAG,YAAY,CAAC,MAA/B;;QACA,IAAI,SAAS,GAAG,CAAZ,GAAgB,CAApB,EAAuB;UACrB,MAAM,IAAI,UAAJ,CACF,wDACA,wDADA,GAEA,sBAHE,CAAN;QAID;;QACD,MAAM,CAAC,cAAD,CAAN,GAAyB,YAAzB;QACA,gBAAgB,CAAC,IAAjB,OAAA,gBAAgB,qBAAS,YAAT,EAAhB;QACA,IAAM,UAAU,GAAI,YAA6C,CACzC,GADJ,CACQ,UAAA,KAAK;UAAA,OAAI,IAAI,SAAJ,CAAc;YAAC,KAAK,EAAE,KAAK,CAAC;UAAd,CAAd,CAAJ;QAAA,CADb,CAApB;QAEA,KAAK,YAAL,CAAkB,SAAlB,GAA8B,UAAU,CAAC,KAAX,CAAiB,CAAjB,EAAoB,SAAS,GAAG,CAAhC,CAA9B;QACA,KAAK,aAAL,CAAmB,SAAnB,GAA+B,UAAU,CAAC,KAAX,CAAiB,SAAS,GAAG,CAA7B,CAA/B;QACA,eAAe,CAAC,IAAhB,OAAA,eAAe,qBAAS,UAAT,EAAf;MACD;;MACD,IAAI,SAAS,IAAI,IAAjB,EAAuB;QACrB,MAAM,IAAI,mBAAJ,CACF,0DACA,kBAFE,CAAN;MAGD;;MAED,IAAM,gBAAgB,GAAG,gBAAgB,CAAC,CAAD,CAAhB,YAA+B,cAAxD;;MACA,qCAAqB,gBAArB,uCAAuC;QAAlC,IAAM,MAAM,wBAAZ;;QACH,IAAI,MAAM,YAAY,cAAlB,KAAqC,gBAAzC,EAA2D;UACzD,MAAM,IAAI,UAAJ,CACF,0DACA,yDAFE,CAAN;QAGD;MACF;;MAED,IAAI,gBAAJ,EAAsB;QAEpB,IAAM,SAAS,GAAG,CAAC,MAAD,EAAS,MAAT,CAAgB,gBAAhB,CAAlB;QACA,IAAM,aAAa,GAAG,KAAK,SAAL,CAAe,MAAf,CAAsB,eAAtB,CAAtB;QAUA,IAAM,iBAAiB,GAAG,KAAK,SAA/B;QACA,KAAK,SAAL,GAAiB,aAAjB;;QACA,IAAM,MAAM,4EACI,SADJ,EAC8C,MAD9C,CAAZ;;QAEA,KAAK,SAAL,GAAiB,iBAAjB;QACA,OAAO,MAAP;MACD,CAnBD,MAmBO;QACL,gFAAmB,MAAnB,EAA2B,MAA3B;MACD;IACF;EAtMH;IAAA;IAAA,OAwME,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QACf,IAAM,YAAY,GAAG,MAAM,CAAC,cAAD,CAA3B;QAEA,IAAI,CAAJ;QACA,IAAI,IAAJ;;QACA,IAAI,YAAY,IAAI,IAApB,EAA0B;UACxB,CAAC,GAAG,MAAI,CAAC,YAAL,CAAkB,IAAlB,CAAuB,MAAvB,EAA+B,MAA/B,CAAJ;UACA,IAAI,GAAG,MAAI,CAAC,aAAL,CAAmB,IAAnB,CAAwB,MAAxB,EAAgC,MAAhC,CAAP;QACD,CAHD,MAGO;UACL,IAAM,YAAY,GAAG,YAAY,CAAC,KAAb,CAAmB,CAAnB,EAAsB,YAAY,CAAC,MAAb,GAAsB,CAA5C,CAArB;UACA,IAAM,aAAa,GAAG,YAAY,CAAC,KAAb,CAAmB,YAAY,CAAC,MAAb,GAAsB,CAAzC,CAAtB;UACA,CAAC,GAAG,MAAI,CAAC,YAAL,CAAkB,IAAlB,CACA,MADA,EACQ,SAAc,MAAd,EAAsB;YAAC,YAAY,EAAE;UAAf,CAAtB,CADR,CAAJ;UAEA,IAAI,GAAG,MAAI,CAAC,aAAL,CAAmB,IAAnB,CACH,MADG,EACK,SAAc,MAAd,EAAsB;YAAC,YAAY,EAAE;UAAf,CAAtB,CADL,CAAP;QAED;;QAED,IAAI,MAAJ;;QACA,IAAI,MAAI,CAAC,WAAT,EAAsB;UACpB,IAAI,KAAK,CAAC,OAAN,CAAc,CAAd,CAAJ,EAAsB;YACpB,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,EAAW,MAAX,CAAmB,IAAiB,CAAC,KAAlB,CAAwB,CAAxB,CAAnB,CAAT;UACD,CAFD,MAEO,CACN;;UACD,CAAC,GAAI,CAAc,CAAC,CAAD,CAAnB;UACA,IAAI,GAAI,IAAiB,CAAC,CAAD,CAAzB;QACD;;QAED,IAAI,MAAI,CAAC,eAAT,EAA0B;UACxB,IAAI,GAAG,GAAG,CAAC,OAAJ,CAAY,IAAZ,EAA4B,CAA5B,CAAP;QACD;;QAED,IAAI,MAAJ;;QACA,IAAI,MAAI,CAAC,SAAL,KAAmB,QAAvB,EAAiC;UAC/B,MAAM,GAAG,CAAC,CAAC,WAAF,CAAc,CAAC,CAAD,EAAc,IAAd,CAAd,CAAT;QACD,CAFD,MAEO,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;UACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;QACD,CAFM,MAEA,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;UACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAZ,CAAT;QACD,CAFM,MAEA,IAAI,MAAI,CAAC,SAAL,KAAmB,KAAvB,EAA8B;UACnC,MAAM,GAAG,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAqB,IAArB,CAAT;QACD,CAFM,MAEA,IAAI,MAAI,CAAC,SAAL,IAAkB,IAAtB,EAA4B;UACjC,MAAM,GAAG,CAAC,CAAD,EAAc,IAAd,CAAT;QACD;;QAGD,IAAI,MAAI,CAAC,WAAT,EAAsB;UACpB,IAAI,MAAI,CAAC,SAAL,IAAkB,IAAtB,EAA4B;YAC1B,OAAQ,MAAmB,CAAC,MAApB,CAA2B,MAA3B,CAAR;UACD;;UACD,OAAO,CAAC,MAAD,EAAmB,MAAnB,CAA0B,MAA1B,CAAP;QACD;;QACD,OAAO,MAAP;MACD,CApDU,CAAX;IAqDD;EA9PH;IAAA;IAAA,OAgQE,qBAAY,MAAZ,EAAoC;MAClC,KAAK,YAAL,CAAkB,WAAlB;MACA,KAAK,aAAL,CAAmB,WAAnB;IACD;EAnQH;IAAA;IAAA,OAqQE,eAAM,UAAN,EAA+B;MAAA;;MAC7B,SAAS,CAAC,KAAK,YAAL,CAAkB,IAAnB,EAAyB,YAAK;QACrC,MAAI,CAAC,YAAL,CAAkB,KAAlB,CAAwB,UAAxB;MACD,CAFQ,CAAT;MAGA,SAAS,CAAC,KAAK,aAAL,CAAmB,IAApB,EAA0B,YAAK;QACtC,MAAI,CAAC,aAAL,CAAmB,KAAnB,CAAyB,UAAzB;MACD,CAFQ,CAAT;MAGA,KAAK,KAAL,GAAa,IAAb;IACD;EA7QH;IAAA;IAAA,OA+QE,qBAAY,MAAZ,EAAqC,IAArC,EAA2D;MAEzD,IAAI,KAAK,CAAC,OAAN,CAAc,IAAd,CAAJ,EAAyB;QACvB,IAAI,GAAG,IAAI,CAAC,CAAD,CAAX;MACD;;MACD,IAAI,UAAJ;;MACA,IAAI,KAAK,eAAT,EAA0B;QACxB,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;UAC1B,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;QACD,CAFD,MAEO;UACL,UAAU,GAAG,IAAb;QACD;MACF,CAND,MAMO;QACL,IAAI,KAAK,SAAL,IAAkB,IAAtB,EAA4B;UAC1B,UAAU,GAAG,CAAC,IAAD,EAAO,IAAP,CAAb;QACD,CAFD,MAEO;UACL,UAAU,GAAG,IAAb;QACD;MACF;;MACD,IAAI,KAAK,WAAT,EAAsB;QACpB,IAAM,MAAM,GAAG,KAAK,YAAL,CAAkB,MAAjC;QACA,IAAM,SAAS,GAAa,MAAM,CAAC,GAAP,CAAW,UAAA,KAAK;UAAA,OAAI,IAAJ;QAAA,CAAhB,CAA5B;;QACA,IAAI,KAAK,CAAC,OAAN,CAAc,UAAd,CAAJ,EAA+B;UAC7B,OAAO,UAAU,CAAC,MAAX,CAAkB,SAAlB,EAA6B,MAA7B,CAAoC,SAApC,CAAP;QACD,CAFD,MAEO;UACL,OAAO,CAAC,UAAD,EAAa,MAAb,CAAoB,SAApB,EAA+B,MAA/B,CAAsC,SAAtC,CAAP;QACD;MACF,CARD,MAQO;QACL,OAAO,UAAP;MACD;IACF;EA7SH;IAAA;IAAA,KA+SE,eAAoB;MAClB,OAAO,KAAK,YAAL,CAAkB,gBAAlB,CAAmC,MAAnC,CACH,KAAK,aAAL,CAAmB,gBADhB,CAAP;IAED;EAlTH;IAAA;IAAA,KAoTE,eAAuB;MACrB,OAAO,KAAK,YAAL,CAAkB,mBAAlB,CAAsC,MAAtC,CACH,KAAK,aAAL,CAAmB,mBADhB,CAAP;IAED;EAvTH;IAAA;IAAA,OA2TE,sCAA6B,KAA7B,EAA2C;MACzC,gGAAmC,KAAnC;;MACA,IAAI,KAAK,YAAL,IAAqB,IAAzB,EAA+B;QAC7B,KAAK,YAAL,CAAkB,4BAAlB,CAA+C,KAA/C;MACD;;MACD,IAAI,KAAK,aAAL,IAAsB,IAA1B,EAAgC;QAC9B,KAAK,aAAL,CAAmB,4BAAnB,CAAgD,KAAhD;MACD;IACF;EAnUH;IAAA;IAAA,OAqUE,qBAAS;MACP,IAAM,MAAM,GAA6B;QACvC,aAAa,KAAK;MADqB,CAAzC;;MAIA,IAAM,UAAU,+EAAhB;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA7UH;IAAA;IAAA,OAgVE,oBACI,GADJ,EAEI,MAFJ,EAEoC;MAClC,IAAM,QAAQ,GACV,WAAW,CAAC,MAAM,CAAC,OAAD,CAAP,CADf;MAEA,OAAO,MAAM,CAAC,OAAD,CAAb;;MAEA,IAAI,MAAM,CAAC,cAAD,CAAN,IAA0B,IAA9B,EAAoC;QAClC,MAAM,IAAI,mBAAJ,CACF,+FADE,CAAN;MAGD;;MAED,IAAM,SAAS,GAAyB,MAAxC;MACA,SAAS,CAAC,OAAD,CAAT,GAAqB,QAArB;MACA,OAAO,IAAI,GAAJ,CAAQ,SAAR,CAAP;IACD;EAhWH;;EAAA;AAAA,EAAmC,OAAnC;AAES,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgWT,aAAa,CAAC,aAAd,CAA4B,aAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * Layers that augment the functionality of a base layer.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { nameScope } from '../common';\nimport { InputSpec, Layer, SymbolicTensor } from '../engine/topology';\nimport { NotImplementedError, ValueError } from '../errors';\nimport { VALID_BIDIRECTIONAL_MERGE_MODES } from '../keras_format/common';\nimport * as generic_utils from '../utils/generic_utils';\nimport { getExactlyOneShape, getExactlyOneTensor } from '../utils/types_utils';\nimport { rnn, standardizeArgs } from './recurrent';\nimport { deserialize } from './serialization';\n/**\n * Abstract wrapper base class.\n *\n * Wrappers take another layer and augment it in various ways.\n * Do not use this class as a layer, it is only an abstract base class.\n * Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n */\nexport class Wrapper extends Layer {\n    constructor(args) {\n        // Porting Note: In PyKeras, `self.layer` is set prior to the calling\n        //   `super()`. But we can't do that here due to TypeScript's restriction.\n        //   See: https://github.com/Microsoft/TypeScript/issues/8277\n        //   As a result, we have to add checks in `get trainable()` and\n        //   `set trainable()` below in order to prevent using `this.layer` when\n        //   its value is `undefined`. The super constructor does use the getter\n        //   and the setter of `this.layer`.\n        super(args);\n        this.layer = args.layer;\n    }\n    build(inputShape) {\n        this.built = true;\n    }\n    // TODO(cais): Implement activityRegularizer getter.\n    get trainable() {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            return this.layer.trainable;\n        }\n        else {\n            return false;\n        }\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        if (this.layer != null) {\n            this.layer.trainable = value;\n        }\n    }\n    get trainableWeights() {\n        return this.layer.trainableWeights;\n    }\n    // TODO(cais): Implement setter for trainableWeights.\n    get nonTrainableWeights() {\n        return this.layer.nonTrainableWeights;\n    }\n    // TODO(cais): Implement setter for nonTrainableWeights.\n    get updates() {\n        // tslint:disable-next-line:no-any\n        return this.layer._updates;\n    }\n    // TODO(cais): Implement getUpdatesFor().\n    get losses() {\n        return this.layer.losses;\n    }\n    // TODO(cais): Implement getLossesFor().\n    getWeights() {\n        return this.layer.getWeights();\n    }\n    setWeights(weights) {\n        this.layer.setWeights(weights);\n    }\n    getConfig() {\n        const config = {\n            'layer': {\n                'className': this.layer.getClassName(),\n                'config': this.layer.getConfig(),\n            }\n        };\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.layer != null) {\n            this.layer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config, customObjects = {}) {\n        const layerConfig = config['layer'];\n        const layer = deserialize(layerConfig, customObjects);\n        delete config['layer'];\n        const newConfig = { layer };\n        Object.assign(newConfig, config);\n        return new cls(newConfig);\n    }\n}\nexport class TimeDistributed extends Wrapper {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n    }\n    build(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        if (inputShape.length < 3) {\n            throw new ValueError(`TimeDistributed layer expects an input shape >= 3D, but received ` +\n                `input shape ${JSON.stringify(inputShape)}`);\n        }\n        this.inputSpec = [{ shape: inputShape }];\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        if (!this.layer.built) {\n            this.layer.build(childInputShape);\n            this.layer.built = true;\n        }\n        super.build(inputShape);\n    }\n    computeOutputShape(inputShape) {\n        inputShape = getExactlyOneShape(inputShape);\n        const childInputShape = [inputShape[0]].concat(inputShape.slice(2));\n        const childOutputShape = this.layer.computeOutputShape(childInputShape);\n        const timesteps = inputShape[1];\n        return [childOutputShape[0], timesteps].concat(childOutputShape.slice(1));\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            // TODO(cais): Add 'training' and 'useLearningPhase' to kwargs.\n            inputs = getExactlyOneTensor(inputs);\n            // Porting Note: In tfjs-layers, `inputs` are always concrete tensor\n            // values. Hence the inputs can't have an undetermined first (batch)\n            // dimension, which is why we always use the K.rnn approach here.\n            const step = (inputs, states) => {\n                // TODO(cais): Add useLearningPhase.\n                // NOTE(cais): `layer.call` may return a length-1 array of Tensor in\n                //   some cases (e.g., `layer` is a `Sequential` instance), which is\n                //   why `getExactlyOneTensor` is used below.\n                const output = getExactlyOneTensor(this.layer.call(inputs, kwargs));\n                return [output, []];\n            };\n            const rnnOutputs = rnn(step, inputs, [], false /* goBackwards */, null /* mask */, null /* constants */, false /* unroll */, true /* needPerStepOutputs */);\n            const y = rnnOutputs[1];\n            // TODO(cais): Add activity regularization.\n            // TODO(cais): Add useLearningPhase.\n            return y;\n        });\n    }\n}\n/** @nocollapse */\nTimeDistributed.className = 'TimeDistributed';\nserialization.registerClass(TimeDistributed);\nexport function checkBidirectionalMergeMode(value) {\n    generic_utils.checkStringTypeUnionValue(VALID_BIDIRECTIONAL_MERGE_MODES, 'BidirectionalMergeMode', value);\n}\nconst DEFAULT_BIDIRECTIONAL_MERGE_MODE = 'concat';\nexport class Bidirectional extends Wrapper {\n    constructor(args) {\n        super(args);\n        // Note: When creating `this.forwardLayer`, the original Layer object\n        //   (`config.layer`) ought to be cloned. This is why we call\n        //   `getConfig()` followed by `deserialize()`. Without this cloning,\n        //   the layer names saved during serialization will incorrectly contain\n        //   the 'forward_' prefix. In Python Keras, this is done using\n        //   `copy.copy` (shallow copy), which does not have a simple equivalent\n        //   in JavaScript. JavaScript's `Object.assign()` does not copy\n        //   methods.\n        const layerConfig = args.layer.getConfig();\n        const forwDict = {};\n        forwDict['className'] = args.layer.getClassName();\n        forwDict['config'] = layerConfig;\n        this.forwardLayer = deserialize(forwDict);\n        layerConfig['goBackwards'] =\n            layerConfig['goBackwards'] === true ? false : true;\n        const backDict = {};\n        backDict['className'] = args.layer.getClassName();\n        backDict['config'] = layerConfig;\n        this.backwardLayer = deserialize(backDict);\n        this.forwardLayer.name = 'forward_' + this.forwardLayer.name;\n        this.backwardLayer.name = 'backward_' + this.backwardLayer.name;\n        this.mergeMode = args.mergeMode === undefined ?\n            DEFAULT_BIDIRECTIONAL_MERGE_MODE :\n            args.mergeMode;\n        checkBidirectionalMergeMode(this.mergeMode);\n        if (args.weights) {\n            throw new NotImplementedError('weights support is not implemented for Bidirectional layer yet.');\n        }\n        this._stateful = args.layer.stateful;\n        this.returnSequences = args.layer.returnSequences;\n        this.returnState = args.layer.returnState;\n        this.supportsMasking = true;\n        this._trainable = true;\n        this.inputSpec = args.layer.inputSpec;\n        this.numConstants = null;\n    }\n    get trainable() {\n        return this._trainable;\n    }\n    set trainable(value) {\n        // Porting Note: the check of `this.layer` here is necessary due to the\n        //   way the `constructor` of this class is written (see Porting Note\n        //   above).\n        this._trainable = value;\n        if (this.forwardLayer != null) {\n            this.forwardLayer.trainable = value;\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.trainable = value;\n        }\n    }\n    getWeights() {\n        return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights());\n    }\n    setWeights(weights) {\n        const numWeights = weights.length;\n        const numeightsOver2 = Math.floor(numWeights / 2);\n        this.forwardLayer.setWeights(weights.slice(0, numeightsOver2));\n        this.backwardLayer.setWeights(weights.slice(numeightsOver2));\n    }\n    computeOutputShape(inputShape) {\n        let layerShapes = this.forwardLayer.computeOutputShape(inputShape);\n        if (!(Array.isArray(layerShapes) && Array.isArray(layerShapes[0]))) {\n            layerShapes = [layerShapes];\n        }\n        layerShapes = layerShapes;\n        let outputShape;\n        let outputShapes;\n        let stateShape;\n        if (this.returnState) {\n            stateShape = layerShapes.slice(1);\n            outputShape = layerShapes[0];\n        }\n        else {\n            outputShape = layerShapes[0];\n        }\n        outputShape = outputShape;\n        if (this.mergeMode === 'concat') {\n            outputShape[outputShape.length - 1] *= 2;\n            outputShapes = [outputShape];\n        }\n        else if (this.mergeMode == null) {\n            outputShapes = [outputShape, outputShape.slice()];\n        }\n        else {\n            outputShapes = [outputShape];\n        }\n        if (this.returnState) {\n            if (this.mergeMode == null) {\n                return outputShapes.concat(stateShape).concat(stateShape.slice());\n            }\n            return [outputShape].concat(stateShape).concat(stateShape.slice());\n        }\n        return generic_utils.singletonOrArray(outputShapes);\n    }\n    apply(inputs, kwargs) {\n        let initialState = kwargs == null ? null : kwargs['initialState'];\n        let constants = kwargs == null ? null : kwargs['constants'];\n        if (kwargs == null) {\n            kwargs = {};\n        }\n        const standardized = standardizeArgs(inputs, initialState, constants, this.numConstants);\n        inputs = standardized.inputs;\n        initialState = standardized.initialState;\n        constants = standardized.constants;\n        if (Array.isArray(inputs)) {\n            initialState = inputs.slice(1);\n            inputs = inputs[0];\n        }\n        if ((initialState == null || initialState.length === 0) &&\n            constants == null) {\n            return super.apply(inputs, kwargs);\n        }\n        const additionalInputs = [];\n        const additionalSpecs = [];\n        if (initialState != null) {\n            const numStates = initialState.length;\n            if (numStates % 2 > 0) {\n                throw new ValueError('When passing `initialState` to a Bidrectional RNN, ' +\n                    'the state should be an Array containing the states of ' +\n                    'the underlying RNNs.');\n            }\n            kwargs['initialState'] = initialState;\n            additionalInputs.push(...initialState);\n            const stateSpecs = initialState\n                .map(state => new InputSpec({ shape: state.shape }));\n            this.forwardLayer.stateSpec = stateSpecs.slice(0, numStates / 2);\n            this.backwardLayer.stateSpec = stateSpecs.slice(numStates / 2);\n            additionalSpecs.push(...stateSpecs);\n        }\n        if (constants != null) {\n            throw new NotImplementedError('Support for constants in Bidirectional layers is not ' +\n                'implemented yet.');\n        }\n        const isSymbolicTensor = additionalInputs[0] instanceof SymbolicTensor;\n        for (const tensor of additionalInputs) {\n            if (tensor instanceof SymbolicTensor !== isSymbolicTensor) {\n                throw new ValueError('The initial state of a Bidirectional layer cannot be ' +\n                    'specified as a mix of symbolic and non-symbolic tensors');\n            }\n        }\n        if (isSymbolicTensor) {\n            // Compute the full input and specs, including the states.\n            const fullInput = [inputs].concat(additionalInputs);\n            const fullInputSpec = this.inputSpec.concat(additionalSpecs);\n            // Perform the call temporarily and replace inputSpec.\n            // Note: with initial states symbolic calls and non-symbolic calls to\n            // this method differ in how the initial states are passed. For\n            // symbolic calls, the initial states are passed in the first arg, as\n            // an Array of SymbolicTensors; for non-symbolic calls, they are\n            // passed in the second arg as a part of the kwargs. Hence the need to\n            // temporarily modify inputSpec here.\n            // TODO(cais): Make refactoring so that this hacky code below is no\n            // longer needed.\n            const originalInputSpec = this.inputSpec;\n            this.inputSpec = fullInputSpec;\n            const output = super.apply(fullInput, kwargs);\n            this.inputSpec = originalInputSpec;\n            return output;\n        }\n        else {\n            return super.apply(inputs, kwargs);\n        }\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            const initialState = kwargs['initialState'];\n            let y;\n            let yRev;\n            if (initialState == null) {\n                y = this.forwardLayer.call(inputs, kwargs);\n                yRev = this.backwardLayer.call(inputs, kwargs);\n            }\n            else {\n                const forwardState = initialState.slice(0, initialState.length / 2);\n                const backwardState = initialState.slice(initialState.length / 2);\n                y = this.forwardLayer.call(inputs, Object.assign(kwargs, { initialState: forwardState }));\n                yRev = this.backwardLayer.call(inputs, Object.assign(kwargs, { initialState: backwardState }));\n            }\n            let states;\n            if (this.returnState) {\n                if (Array.isArray(y)) {\n                    states = y.slice(1).concat(yRev.slice(1));\n                }\n                else {\n                }\n                y = y[0];\n                yRev = yRev[0];\n            }\n            if (this.returnSequences) {\n                yRev = tfc.reverse(yRev, 1);\n            }\n            let output;\n            if (this.mergeMode === 'concat') {\n                output = K.concatenate([y, yRev]);\n            }\n            else if (this.mergeMode === 'sum') {\n                output = tfc.add(y, yRev);\n            }\n            else if (this.mergeMode === 'ave') {\n                output = tfc.mul(.5, tfc.add(y, yRev));\n            }\n            else if (this.mergeMode === 'mul') {\n                output = tfc.mul(y, yRev);\n            }\n            else if (this.mergeMode == null) {\n                output = [y, yRev];\n            }\n            // TODO(cais): Properly set learning phase.\n            if (this.returnState) {\n                if (this.mergeMode == null) {\n                    return output.concat(states);\n                }\n                return [output].concat(states);\n            }\n            return output;\n        });\n    }\n    resetStates(states) {\n        this.forwardLayer.resetStates();\n        this.backwardLayer.resetStates();\n    }\n    build(inputShape) {\n        nameScope(this.forwardLayer.name, () => {\n            this.forwardLayer.build(inputShape);\n        });\n        nameScope(this.backwardLayer.name, () => {\n            this.backwardLayer.build(inputShape);\n        });\n        this.built = true;\n    }\n    computeMask(inputs, mask) {\n        if (Array.isArray(mask)) {\n            mask = mask[0];\n        }\n        let outputMask;\n        if (this.returnSequences) {\n            if (this.mergeMode == null) {\n                outputMask = [mask, mask];\n            }\n            else {\n                outputMask = mask;\n            }\n        }\n        else {\n            if (this.mergeMode == null) {\n                outputMask = [null, null];\n            }\n            else {\n                outputMask = null;\n            }\n        }\n        if (this.returnState) {\n            const states = this.forwardLayer.states;\n            const stateMask = states.map(state => null);\n            if (Array.isArray(outputMask)) {\n                return outputMask.concat(stateMask).concat(stateMask);\n            }\n            else {\n                return [outputMask].concat(stateMask).concat(stateMask);\n            }\n        }\n        else {\n            return outputMask;\n        }\n    }\n    get trainableWeights() {\n        return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights);\n    }\n    get nonTrainableWeights() {\n        return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights);\n    }\n    // TODO(cais): Implement constraints().\n    setFastWeightInitDuringBuild(value) {\n        super.setFastWeightInitDuringBuild(value);\n        if (this.forwardLayer != null) {\n            this.forwardLayer.setFastWeightInitDuringBuild(value);\n        }\n        if (this.backwardLayer != null) {\n            this.backwardLayer.setFastWeightInitDuringBuild(value);\n        }\n    }\n    getConfig() {\n        const config = {\n            'mergeMode': this.mergeMode,\n        };\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        const baseConfig = super.getConfig();\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    /** @nocollapse */\n    static fromConfig(cls, config) {\n        const rnnLayer = deserialize(config['layer']);\n        delete config['layer'];\n        // TODO(cais): Add logic for `numConstants` once the property is added.\n        if (config['numConstants'] != null) {\n            throw new NotImplementedError(`Deserialization of a Bidirectional layer with numConstants ` +\n                `present is not supported yet.`);\n        }\n        // tslint:disable-next-line:no-any\n        const newConfig = config;\n        newConfig['layer'] = rnnLayer;\n        return new cls(newConfig);\n    }\n}\n/** @nocollapse */\nBidirectional.className = 'Bidirectional';\nserialization.registerClass(Bidirectional);\n//# sourceMappingURL=wrappers.js.map"]},"metadata":{},"sourceType":"module"}