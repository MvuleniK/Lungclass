{"ast":null,"code":"import _extends from \"@babel/runtime/helpers/extends\";\nimport _classCallCheck from \"@babel/runtime/helpers/classCallCheck\";\nimport _createClass from \"@babel/runtime/helpers/createClass\";\nimport _get from \"@babel/runtime/helpers/get\";\nimport _inherits from \"@babel/runtime/helpers/inherits\";\nimport _possibleConstructorReturn from \"@babel/runtime/helpers/possibleConstructorReturn\";\nimport _getPrototypeOf from \"@babel/runtime/helpers/getPrototypeOf\";\n\nfunction _createSuper(Derived) { var hasNativeReflectConstruct = _isNativeReflectConstruct(); return function _createSuperInternal() { var Super = _getPrototypeOf(Derived), result; if (hasNativeReflectConstruct) { var NewTarget = _getPrototypeOf(this).constructor; result = Reflect.construct(Super, arguments, NewTarget); } else { result = Super.apply(this, arguments); } return _possibleConstructorReturn(this, result); }; }\n\nfunction _isNativeReflectConstruct() { if (typeof Reflect === \"undefined\" || !Reflect.construct) return false; if (Reflect.construct.sham) return false; if (typeof Proxy === \"function\") return true; try { Boolean.prototype.valueOf.call(Reflect.construct(Boolean, [], function () {})); return true; } catch (e) { return false; } }\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport { greaterEqual, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from \"../backend/tfjs_backend\";\nimport { Layer } from \"../engine/topology\";\nimport { getExactlyOneTensor } from \"../utils/types_utils\";\nexport var GaussianNoise = function (_Layer) {\n  _inherits(GaussianNoise, _Layer);\n\n  var _super = _createSuper(GaussianNoise);\n\n  function GaussianNoise(args) {\n    var _this;\n\n    _classCallCheck(this, GaussianNoise);\n\n    _this = _super.call(this, args);\n    _this.supportsMasking = true;\n    _this.stddev = args.stddev;\n    return _this;\n  }\n\n  _createClass(GaussianNoise, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianNoise.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        stddev: this.stddev\n      };\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this2 = this;\n\n      return tidy(function () {\n        _this2.invokeCallHook(inputs, kwargs);\n\n        var input = getExactlyOneTensor(inputs);\n\n        var noised = function noised() {\n          return K.randomNormal(input.shape, 0, _this2.stddev).add(input);\n        };\n\n        var output = K.inTrainPhase(noised, function () {\n          return input;\n        }, kwargs['training'] || false);\n        return output;\n      });\n    }\n  }]);\n\n  return GaussianNoise;\n}(Layer);\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport var GaussianDropout = function (_Layer2) {\n  _inherits(GaussianDropout, _Layer2);\n\n  var _super2 = _createSuper(GaussianDropout);\n\n  function GaussianDropout(args) {\n    var _this3;\n\n    _classCallCheck(this, GaussianDropout);\n\n    _this3 = _super2.call(this, args);\n    _this3.supportsMasking = true;\n    _this3.rate = args.rate;\n    return _this3;\n  }\n\n  _createClass(GaussianDropout, [{\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(GaussianDropout.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        rate: this.rate\n      };\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this4 = this;\n\n      return tidy(function () {\n        _this4.invokeCallHook(inputs, kwargs);\n\n        var input = getExactlyOneTensor(inputs);\n\n        if (_this4.rate > 0 && _this4.rate < 1) {\n          var noised = function noised() {\n            var stddev = Math.sqrt(_this4.rate / (1 - _this4.rate));\n            return input.mul(K.randomNormal(input.shape, 1, stddev));\n          };\n\n          return K.inTrainPhase(noised, function () {\n            return input;\n          }, kwargs['training'] || false);\n        }\n\n        return input;\n      });\n    }\n  }]);\n\n  return GaussianDropout;\n}(Layer);\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\nexport var AlphaDropout = function (_Layer3) {\n  _inherits(AlphaDropout, _Layer3);\n\n  var _super3 = _createSuper(AlphaDropout);\n\n  function AlphaDropout(args) {\n    var _this5;\n\n    _classCallCheck(this, AlphaDropout);\n\n    _this5 = _super3.call(this, args);\n    _this5.supportsMasking = true;\n    _this5.rate = args.rate;\n    _this5.noiseShape = args.noiseShape;\n    return _this5;\n  }\n\n  _createClass(AlphaDropout, [{\n    key: \"_getNoiseShape\",\n    value: function _getNoiseShape(inputs) {\n      return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n  }, {\n    key: \"computeOutputShape\",\n    value: function computeOutputShape(inputShape) {\n      return inputShape;\n    }\n  }, {\n    key: \"getConfig\",\n    value: function getConfig() {\n      var baseConfig = _get(_getPrototypeOf(AlphaDropout.prototype), \"getConfig\", this).call(this);\n\n      var config = {\n        rate: this.rate\n      };\n\n      _extends(config, baseConfig);\n\n      return config;\n    }\n  }, {\n    key: \"call\",\n    value: function call(inputs, kwargs) {\n      var _this6 = this;\n\n      return tidy(function () {\n        if (_this6.rate < 1 && _this6.rate > 0) {\n          var noiseShape = _this6._getNoiseShape(inputs);\n\n          var droppedInputs = function droppedInputs() {\n            var input = getExactlyOneTensor(inputs);\n            var alpha = 1.6732632423543772848170429916717;\n            var scale = 1.0507009873554804934193349852946;\n            var alphaP = -alpha * scale;\n            var keptIdx = greaterEqual(randomUniform(noiseShape), _this6.rate);\n            keptIdx = K.cast(keptIdx, 'float32');\n            var a = Math.pow((1 - _this6.rate) * (1 + _this6.rate * Math.pow(alphaP, 2)), -0.5);\n            var b = -a * alphaP * _this6.rate;\n            var x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n            return x.mul(a).add(b);\n          };\n\n          return K.inTrainPhase(droppedInputs, function () {\n            return getExactlyOneTensor(inputs);\n          }, kwargs['training'] || false);\n        }\n\n        return inputs;\n      });\n    }\n  }]);\n\n  return AlphaDropout;\n}(Layer);\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);","map":{"version":3,"sources":["../../src/layers/noise.ts"],"names":[],"mappings":";;;;;;;;;;;;AAAA;;;;;;;;AAQG;AAMH,SAAQ,YAAR,EAAsB,aAAtB,EAAqC,aAArC,EAA4D,IAA5D,QAAuE,uBAAvE;AAEA,OAAO,KAAK,CAAZ;AACA,SAAQ,KAAR;AAGA,SAAQ,mBAAR;AAOA,WAAa,aAAb;EAAA;;EAAA;;EAKE,uBAAY,IAAZ,EAAmC;IAAA;;IAAA;;IACjC,0BAAM,IAAN;IACA,MAAK,eAAL,GAAuB,IAAvB;IACA,MAAK,MAAL,GAAc,IAAI,CAAC,MAAnB;IAHiC;EAIlC;;EATH;IAAA;IAAA,OAWE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAbH;IAAA;IAAA,OAeE,qBAAS;MACP,IAAM,UAAU,+EAAhB;;MACA,IAAM,MAAM,GAAG;QAAC,MAAM,EAAE,KAAK;MAAd,CAAf;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EApBH;IAAA;IAAA,OAsBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QACf,MAAI,CAAC,cAAL,CAAoB,MAApB,EAA4B,MAA5B;;QACA,IAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;;QACA,IAAM,MAAM,GAAG,SAAT,MAAS;UAAA,OACX,CAAC,CAAC,YAAF,CAAe,KAAK,CAAC,KAArB,EAA4B,CAA5B,EAA+B,MAAI,CAAC,MAApC,EAA4C,GAA5C,CAAgD,KAAhD,CADW;QAAA,CAAf;;QAEA,IAAM,MAAM,GACR,CAAC,CAAC,YAAF,CAAe,MAAf,EAAuB;UAAA,OAAM,KAAN;QAAA,CAAvB,EAAoC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CADJ;QAEA,OAAO,MAAP;MACD,CARU,CAAX;IASD;EAhCH;;EAAA;AAAA,EAAmC,KAAnC;AAES,aAAA,CAAA,SAAA,GAAY,eAAZ;AAgCT,aAAa,CAAC,aAAd,CAA4B,aAA5B;AAOA,WAAa,eAAb;EAAA;;EAAA;;EAKE,yBAAY,IAAZ,EAAqC;IAAA;;IAAA;;IACnC,4BAAM,IAAN;IACA,OAAK,eAAL,GAAuB,IAAvB;IACA,OAAK,IAAL,GAAY,IAAI,CAAC,IAAjB;IAHmC;EAIpC;;EATH;IAAA;IAAA,OAWE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAbH;IAAA;IAAA,OAeE,qBAAS;MACP,IAAM,UAAU,iFAAhB;;MACA,IAAM,MAAM,GAAG;QAAC,IAAI,EAAE,KAAK;MAAZ,CAAf;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EApBH;IAAA;IAAA,OAsBE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QACf,MAAI,CAAC,cAAL,CAAoB,MAApB,EAA4B,MAA5B;;QACA,IAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;;QACA,IAAI,MAAI,CAAC,IAAL,GAAY,CAAZ,IAAiB,MAAI,CAAC,IAAL,GAAY,CAAjC,EAAoC;UAClC,IAAM,MAAM,GAAG,SAAT,MAAS,GAAK;YAClB,IAAM,MAAM,GAAG,IAAI,CAAC,IAAL,CAAU,MAAI,CAAC,IAAL,IAAa,IAAI,MAAI,CAAC,IAAtB,CAAV,CAAf;YACA,OAAO,KAAK,CAAC,GAAN,CAAU,CAAC,CAAC,YAAF,CAAe,KAAK,CAAC,KAArB,EAA4B,CAA5B,EAA+B,MAA/B,CAAV,CAAP;UACD,CAHD;;UAIA,OAAO,CAAC,CAAC,YAAF,CAAe,MAAf,EAAuB;YAAA,OAAM,KAAN;UAAA,CAAvB,EAAoC,MAAM,CAAC,UAAD,CAAN,IAAsB,KAA1D,CAAP;QACD;;QACD,OAAO,KAAP;MACD,CAXU,CAAX;IAYD;EAnCH;;EAAA;AAAA,EAAqC,KAArC;AAES,eAAA,CAAA,SAAA,GAAY,iBAAZ;AAmCT,aAAa,CAAC,aAAd,CAA4B,eAA5B;AAyCA,WAAa,YAAb;EAAA;;EAAA;;EAME,sBAAY,IAAZ,EAAkC;IAAA;;IAAA;;IAChC,4BAAM,IAAN;IACA,OAAK,eAAL,GAAuB,IAAvB;IACA,OAAK,IAAL,GAAY,IAAI,CAAC,IAAjB;IACA,OAAK,UAAL,GAAkB,IAAI,CAAC,UAAvB;IAJgC;EAKjC;;EAXH;IAAA;IAAA,OAaE,wBAAe,MAAf,EAAsC;MACpC,OAAO,KAAK,UAAL,IAAmB,mBAAmB,CAAC,MAAD,CAAnB,CAA4B,KAAtD;IACD;EAfH;IAAA;IAAA,OAiBE,4BAAmB,UAAnB,EAA4C;MAC1C,OAAO,UAAP;IACD;EAnBH;IAAA;IAAA,OAqBE,qBAAS;MACP,IAAM,UAAU,8EAAhB;;MACA,IAAM,MAAM,GAAG;QAAC,IAAI,EAAE,KAAK;MAAZ,CAAf;;MACA,SAAc,MAAd,EAAsB,UAAtB;;MACA,OAAO,MAAP;IACD;EA1BH;IAAA;IAAA,OA4BE,cAAK,MAAL,EAA8B,MAA9B,EAA4C;MAAA;;MAC1C,OAAO,IAAI,CAAC,YAAK;QACf,IAAI,MAAI,CAAC,IAAL,GAAY,CAAZ,IAAiB,MAAI,CAAC,IAAL,GAAY,CAAjC,EAAoC;UAClC,IAAM,UAAU,GAAG,MAAI,CAAC,cAAL,CAAoB,MAApB,CAAnB;;UAEA,IAAM,aAAa,GAAG,SAAhB,aAAgB,GAAK;YACzB,IAAM,KAAK,GAAG,mBAAmB,CAAC,MAAD,CAAjC;YAEA,IAAM,KAAK,GAAG,iCAAd;YACA,IAAM,KAAK,GAAG,iCAAd;YAEA,IAAM,MAAM,GAAG,CAAC,KAAD,GAAS,KAAxB;YAEA,IAAI,OAAO,GAAG,YAAY,CAAC,aAAa,CAAC,UAAD,CAAd,EAA4B,MAAI,CAAC,IAAjC,CAA1B;YAEA,OAAO,GAAG,CAAC,CAAC,IAAF,CAAO,OAAP,EAAgB,SAAhB,CAAV;YAGA,IAAM,CAAC,YAAI,CAAC,IAAI,MAAI,CAAC,IAAV,KAAmB,IAAI,MAAI,CAAC,IAAL,YAAY,MAAZ,EAAsB,CAAtB,CAAvB,CAAJ,EAAwD,CAAC,GAAzD,CAAP;YACA,IAAM,CAAC,GAAG,CAAC,CAAD,GAAK,MAAL,GAAc,MAAI,CAAC,IAA7B;YAGA,IAAM,CAAC,GAAG,KAAK,CAAC,GAAN,CAAU,OAAV,EAAmB,GAAnB,CAAuB,OAAO,CAAC,GAAR,CAAY,CAAC,CAAb,EAAgB,GAAhB,CAAoB,MAApB,CAAvB,CAAV;YAEA,OAAO,CAAC,CAAC,GAAF,CAAM,CAAN,EAAS,GAAT,CAAa,CAAb,CAAP;UACD,CApBD;;UAqBA,OAAO,CAAC,CAAC,YAAF,CACH,aADG,EACY;YAAA,OAAM,mBAAmB,CAAC,MAAD,CAAzB;UAAA,CADZ,EAEH,MAAM,CAAC,UAAD,CAAN,IAAsB,KAFnB,CAAP;QAGD;;QACD,OAAO,MAAP;MACD,CA9BU,CAAX;IA+BD;EA5DH;;EAAA;AAAA,EAAkC,KAAlC;AAES,YAAA,CAAA,SAAA,GAAY,cAAZ;AA4DT,aAAa,CAAC,aAAd,CAA4B,YAA5B","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * TensorFlow.js Layers: Noise Layers.\n */\nimport { greaterEqual, randomUniform, serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from '../backend/tfjs_backend';\nimport { Layer } from '../engine/topology';\nimport { getExactlyOneTensor } from '../utils/types_utils';\nexport class GaussianNoise extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.stddev = args.stddev;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { stddev: this.stddev };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            const noised = () => K.randomNormal(input.shape, 0, this.stddev).add(input);\n            const output = K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            return output;\n        });\n    }\n}\n/** @nocollapse */\nGaussianNoise.className = 'GaussianNoise';\nserialization.registerClass(GaussianNoise);\nexport class GaussianDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            this.invokeCallHook(inputs, kwargs);\n            const input = getExactlyOneTensor(inputs);\n            if (this.rate > 0 && this.rate < 1) {\n                const noised = () => {\n                    const stddev = Math.sqrt(this.rate / (1 - this.rate));\n                    return input.mul(K.randomNormal(input.shape, 1, stddev));\n                };\n                return K.inTrainPhase(noised, () => input, kwargs['training'] || false);\n            }\n            return input;\n        });\n    }\n}\n/** @nocollapse */\nGaussianDropout.className = 'GaussianDropout';\nserialization.registerClass(GaussianDropout);\n/**\n * Applies Alpha Dropout to the input.\n *\n * As it is a regularization layer, it is only active at training time.\n *\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n * to their original values, in order to ensure the self-normalizing property\n * even after this dropout.\n * Alpha Dropout fits well to Scaled Exponential Linear Units\n * by randomly setting activations to the negative saturation value.\n *\n * Arguments:\n *   - `rate`: float, drop probability (as with `Dropout`).\n *     The multiplicative noise will have\n *     standard deviation `sqrt(rate / (1 - rate))`.\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\n *     shape for randomly generated keep/drop flags.\n *\n * Input shape:\n *   Arbitrary. Use the keyword argument `inputShape`\n *   (tuple of integers, does not include the samples axis)\n *   when using this layer as the first layer in a model.\n *\n * Output shape:\n *   Same shape as input.\n *\n * References:\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\n */\nexport class AlphaDropout extends Layer {\n    constructor(args) {\n        super(args);\n        this.supportsMasking = true;\n        this.rate = args.rate;\n        this.noiseShape = args.noiseShape;\n    }\n    _getNoiseShape(inputs) {\n        return this.noiseShape || getExactlyOneTensor(inputs).shape;\n    }\n    computeOutputShape(inputShape) {\n        return inputShape;\n    }\n    getConfig() {\n        const baseConfig = super.getConfig();\n        const config = { rate: this.rate };\n        Object.assign(config, baseConfig);\n        return config;\n    }\n    call(inputs, kwargs) {\n        return tidy(() => {\n            if (this.rate < 1 && this.rate > 0) {\n                const noiseShape = this._getNoiseShape(inputs);\n                const droppedInputs = () => {\n                    const input = getExactlyOneTensor(inputs);\n                    const alpha = 1.6732632423543772848170429916717;\n                    const scale = 1.0507009873554804934193349852946;\n                    const alphaP = -alpha * scale;\n                    let keptIdx = greaterEqual(randomUniform(noiseShape), this.rate);\n                    keptIdx = K.cast(keptIdx, 'float32'); // get default dtype.\n                    // Get affine transformation params.\n                    const a = ((1 - this.rate) * (1 + this.rate * alphaP ** 2)) ** -0.5;\n                    const b = -a * alphaP * this.rate;\n                    // Apply mask.\n                    const x = input.mul(keptIdx).add(keptIdx.add(-1).mul(alphaP));\n                    return x.mul(a).add(b);\n                };\n                return K.inTrainPhase(droppedInputs, () => getExactlyOneTensor(inputs), kwargs['training'] || false);\n            }\n            return inputs;\n        });\n    }\n}\n/** @nocollapse */\nAlphaDropout.className = 'AlphaDropout';\nserialization.registerClass(AlphaDropout);\n//# sourceMappingURL=noise.js.map"]},"metadata":{},"sourceType":"module"}