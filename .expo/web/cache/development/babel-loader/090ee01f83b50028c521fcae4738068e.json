{"ast":null,"code":"import _slicedToArray from \"@babel/runtime/helpers/slicedToArray\";\n\n/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from \"../gradients\";\nimport { convertToTensor } from \"../tensor_util_env\";\nimport { cast } from \"./cast\";\nimport { exp } from \"./exp\";\nimport { log } from \"./log\";\nimport { max } from \"./max\";\nimport { mul } from \"./mul\";\nimport { op } from \"./operation\";\nimport { sub } from \"./sub\";\nimport { sum } from \"./sum\";\n\nfunction logSoftmax_(logits) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n\n  if (axis === -1) {\n    axis = $logits.rank - 1;\n  }\n\n  if (axis !== $logits.rank - 1) {\n    throw Error('Log Softmax along a non-last dimension is not yet supported. ' + (\"Logits was rank \" + $logits.rank + \" and axis was \" + axis));\n  }\n\n  var customOp = customGrad(function (logits, save) {\n    var keepDims = true;\n    var xMax = max(logits, axis, true);\n    var shifted = sub(logits, xMax);\n    var value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n    save([value]);\n\n    var gradFunc = function gradFunc(dy, saved) {\n      var _saved = _slicedToArray(saved, 1),\n          value = _saved[0];\n\n      var keepDims = true;\n      var softmax = exp(value);\n      return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n    };\n\n    return {\n      value: value,\n      gradFunc: gradFunc\n    };\n  });\n  return customOp($logits);\n}\n\nexport var logSoftmax = op({\n  logSoftmax_: logSoftmax_\n});","map":{"version":3,"sources":["../../src/ops/log_softmax.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,UAAR;AAIA,SAAQ,eAAR;AAGA,SAAQ,IAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;AACA,SAAQ,EAAR;AACA,SAAQ,GAAR;AACA,SAAQ,GAAR;;AAuBA,SAAS,WAAT,CAAuC,MAAvC,EAAsE;EAAA,IAAT,IAAS,uEAAF,CAAC,CAAC;EACpE,IAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,YAAnB,CAA/B;;EAEA,IAAI,IAAI,KAAK,CAAC,CAAd,EAAiB;IACf,IAAI,GAAG,OAAO,CAAC,IAAR,GAAe,CAAtB;EACD;;EACD,IAAI,IAAI,KAAK,OAAO,CAAC,IAAR,GAAe,CAA5B,EAA+B;IAC7B,MAAM,KAAK,CACP,wFACmB,OAAO,CAAC,IAD3B,sBACgD,IADhD,CADO,CAAX;EAGD;;EAcD,IAAM,QAAQ,GAAG,UAAU,CAAC,UAAC,MAAD,EAAiB,IAAjB,EAAuC;IACjE,IAAM,QAAQ,GAAG,IAAjB;IACA,IAAM,IAAI,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,EAAe,IAAf,CAAhB;IACA,IAAM,OAAO,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAnB;IACA,IAAM,KAAK,GACP,GAAG,CAAC,IAAI,CAAC,OAAD,EAAU,SAAV,CAAL,EAA2B,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,OAAD,CAAJ,EAAe,IAAf,EAAqB,QAArB,CAAJ,CAA9B,CADP;IAEA,IAAI,CAAC,CAAC,KAAD,CAAD,CAAJ;;IAEA,IAAM,QAAQ,GAAG,SAAX,QAAW,CAAC,EAAD,EAAa,KAAb,EAAgC;MAC/C,4BAAgB,KAAhB;MAAA,IAAO,KAAP;;MACA,IAAM,QAAQ,GAAG,IAAjB;MACA,IAAM,OAAO,GAAG,GAAG,CAAC,KAAD,CAAnB;MACA,OAAO,GAAG,CAAC,EAAD,EAAK,GAAG,CAAC,GAAG,CAAC,EAAD,EAAK,IAAL,EAAW,QAAX,CAAJ,EAA0B,OAA1B,CAAR,CAAV;IACD,CALD;;IAMA,OAAO;MAAC,KAAK,EAAL,KAAD;MAAQ,QAAQ,EAAR;IAAR,CAAP;EACD,CAf0B,CAA3B;EAiBA,OAAO,QAAQ,CAAC,OAAD,CAAf;AAQD;;AAED,OAAO,IAAM,UAAU,GAAG,EAAE,CAAC;EAAC,WAAW,EAAX;AAAD,CAAD,CAArB","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2020 Google Inc. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { cast } from './cast';\nimport { exp } from './exp';\nimport { log } from './log';\nimport { max } from './max';\nimport { mul } from './mul';\nimport { op } from './operation';\nimport { sub } from './sub';\nimport { sum } from './sum';\n/**\n * Computes the log softmax.\n *\n * ```js\n * const a = tf.tensor1d([1, 2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * ```js\n * const a = tf.tensor2d([2, 4, 6, 1, 2, 3], [2, 3]);\n *\n * a.logSoftmax().print();  // or tf.logSoftmax(a)\n * ```\n *\n * @param logits The logits array.\n * @param axis The dimension softmax would be performed on. Defaults to `-1`\n *     which indicates the last dimension.\n *\n * @doc {heading: 'Operations', subheading: 'Normalization'}\n */\nfunction logSoftmax_(logits, axis = -1) {\n    const $logits = convertToTensor(logits, 'logits', 'logSoftmax');\n    if (axis === -1) {\n        axis = $logits.rank - 1;\n    }\n    if (axis !== $logits.rank - 1) {\n        throw Error('Log Softmax along a non-last dimension is not yet supported. ' +\n            `Logits was rank ${$logits.rank} and axis was ${axis}`);\n    }\n    // const forward: ForwardFunc<Tensor> = (backend, save) => {\n    //   const keepDims = true;\n    //   const xMax = max(logits, axis, true);\n    //   const shifted = sub(logits, xMax);\n    //   const value =\n    //       sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis,\n    //       keepDims)));\n    //   save([value]);\n    //   return value;\n    // };\n    // Use a custom gradient for numerical stability.\n    const customOp = customGrad((logits, save) => {\n        const keepDims = true;\n        const xMax = max(logits, axis, true);\n        const shifted = sub(logits, xMax);\n        const value = sub(cast(shifted, 'float32'), log(sum(exp(shifted), axis, keepDims)));\n        save([value]);\n        const gradFunc = (dy, saved) => {\n            const [value] = saved;\n            const keepDims = true;\n            const softmax = exp(value);\n            return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n        };\n        return { value, gradFunc };\n    });\n    return customOp($logits);\n    // TODO Use Engine.runKernel when CPU/WebGL/WASM backends implement this.\n    // const inputs: LogSoftmaxInputs = {logits: $logits};\n    // const attrs: LogSoftmaxAttrs = {axis};\n    // return ENGINE.runKernel(\n    //            LogSoftmax, inputs as {} as NamedTensorMap,\n    //            attrs as {} as NamedAttrMap);\n}\nexport const logSoftmax = op({ logSoftmax_ });\n//# sourceMappingURL=log_softmax.js.map"]},"metadata":{},"sourceType":"module"}