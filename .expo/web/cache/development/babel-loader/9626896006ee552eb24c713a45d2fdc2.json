{"ast":null,"code":"import _toConsumableArray from \"@babel/runtime/helpers/toConsumableArray\";\n\n/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from \"../common\";\nimport { NotImplementedError, ValueError } from \"../errors\";\nimport * as math_utils from \"../utils/math_utils\";\nimport { imageDataFormat } from \"./common\";\nvar backend = 'webgl';\nexport function setBackend(requestedBackend) {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\nexport function getBackend() {\n  return backend;\n}\nexport function isBackendSymbolic() {\n  return false;\n}\nexport function countParams(x) {\n  var shape = x.shape;\n\n  if (shape.length > 0) {\n    return shape.reduce(function (a, b) {\n      return a * b;\n    });\n  } else {\n    return 1;\n  }\n}\nexport function cast(x, dtype) {\n  return x.asType(dtype);\n}\nexport function expandDims(x) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var outShape = x.shape.slice();\n\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n\n  outShape.splice(axis, 0, 1);\n  return x.reshape(outShape);\n}\nexport function repeat(x, n) {\n  return tidy(function () {\n    if (x.shape.length !== 2) {\n      throw new ValueError(\"repeat() expects a rank-2 tensor, but received a \" + (\"rank-\" + x.shape.length + \" tensor.\"));\n    }\n\n    var y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\nexport function flatten(x) {\n  var newShape = [math_utils.arrayProd(x.shape)];\n  return x.reshape(newShape);\n}\nexport function batchFlatten(x) {\n  if (x.rank <= 1) {\n    throw new ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + x.rank + \".\");\n  }\n\n  var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return x.reshape(newShape);\n}\nexport function sliceAlongFirstAxis(array, start, size) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n\n      case 3:\n        return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n\n      case 4:\n        return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n\n      case 5:\n        return tfc.slice(array, [start, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]]);\n\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4], array.shape[5]]);\n\n      default:\n        throw new ValueError(\"sliceAlongFirstAxis() received an unsupported tensor rank: \" + (\"\" + array.rank));\n    }\n  });\n}\nexport function sliceAlongLastAxis(array, start, size) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n\n      case 3:\n        return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n\n      case 4:\n        return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + (\"\" + array.rank));\n    }\n  });\n}\nexport function sliceAlongAxis(array, start, size, axis) {\n  return tidy(function () {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + (\"\" + axis));\n        }\n\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + (\"\" + axis));\n        }\n\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n\n          case 2:\n            return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n\n          case 3:\n            return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n\n          default:\n            throw new ValueError(\"The axis is not within the rank of the tensor \" + (\"\" + axis));\n        }\n\n      default:\n        throw new ValueError(\"sliceAlongLastAxis() received an unsupported tensor rank: \" + (\"\" + array.rank));\n    }\n  });\n}\nexport function concatenate(tensors) {\n  var axis = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : -1;\n  var rank;\n\n  if (axis < 0) {\n    rank = tensors[0].rank;\n\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n\n  if (axis === tensors[0].rank) {\n    axis = -1;\n  }\n\n  return tfc.concat(tensors, axis);\n}\nexport function concatAlongFirstAxis(a, b) {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a, b]);\n\n    case 2:\n      return tfc.concat2d([a, b], 0);\n\n    case 3:\n      return tfc.concat3d([a, b], 0);\n\n    case 4:\n      return tfc.concat4d([a, b], 0);\n\n    default:\n      throw new ValueError(\"concatAlongFirstAxis() received an unsupported \" + (\"tensor rank: \" + a.rank));\n  }\n}\nexport function tile(x, n) {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n\n  if (x.rank !== n.length) {\n    throw new ValueError(\"The length of input n (\" + n.length + \") does not match \" + (\"the number of dimensions in input x (\" + x.rank + \")\"));\n  }\n\n  return tfc.tile(x, n);\n}\nexport function randomNormal(shape) {\n  var mean = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 0.0;\n  var stddev = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1.0;\n  var dtype = arguments.length > 3 ? arguments[3] : undefined;\n  var seed = arguments.length > 4 ? arguments[4] : undefined;\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\nexport function dot(a, b, activation, bias) {\n  if (a.rank < 2 || b.rank < 2) {\n    throw new NotImplementedError(\"dot requires both inputs to be rank >= 2\" + (\" but got x shape = \" + a.shape + \" and y shape = \" + b.shape));\n  }\n\n  if (b.rank >= 3) {\n    var xLastDim = a.shape.slice(-1)[0];\n    var ySecondLastDim = b.shape.slice(-2)[0];\n\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(\"If rank y >= 3, then the second last dim\" + (\" of y must equal the last dim of x but got x shape = \" + a.shape + \" and \") + (\" y shape = \" + b.shape));\n    }\n  }\n\n  if (a.rank === 2 && b.rank === 2) {\n    var transposeA = false;\n    var transposeB = false;\n    return tfc.fused.matMul({\n      a: a,\n      b: b,\n      transposeA: transposeA,\n      transposeB: transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation: activation\n    });\n  } else {\n    var aFirstDims = a.shape.slice();\n    var aLastDim = aFirstDims.pop();\n    a = a.reshape([-1, aLastDim]);\n    var bShape = b.shape.slice();\n    var bLastDim = bShape.pop();\n\n    var _ySecondLastDim = bShape.pop();\n\n    var yOtherDims = [].concat(_toConsumableArray(bShape), [bLastDim]);\n    var perm = Array.from({\n      length: b.rank\n    }, function (_, i) {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n\n      return i;\n    });\n    b = b.transpose(perm).reshape([_ySecondLastDim, -1]);\n    var outputShape = [].concat(_toConsumableArray(aFirstDims), _toConsumableArray(yOtherDims));\n    var _transposeA = false;\n    var _transposeB = false;\n    return tfc.fused.matMul({\n      a: a,\n      b: b,\n      transposeA: _transposeA,\n      transposeB: _transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation: activation\n    }).reshape(outputShape);\n  }\n}\nexport function sign(x) {\n  return tidy(function () {\n    var zerosLikeX = coreZerosLike(x);\n    var onesLikeX = coreOnesLike(x);\n    return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n  });\n}\nexport function oneHot(indices, numClasses) {\n  return tidy(function () {\n    if (indices.rank !== 1) {\n      throw new Error('Only 1D one-hot tensors are supported in the ' + 'deeplearn backend, at present.');\n    }\n\n    indices = indices.toInt();\n    return tfc.oneHot(indices, numClasses).toFloat();\n  });\n}\nexport function gather(reference, indices, axis) {\n  return tidy(function () {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = indices.toInt();\n    }\n\n    return tfc.gather(reference, indices, axis);\n  });\n}\nexport function square(x) {\n  return tfc.mul(x, x);\n}\nexport function pow(x, a) {\n  return tidy(function () {\n    if (typeof a === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(\"Non-int32 dtype (\" + a.dtype + \") is not supported by pow() yet\");\n    }\n\n    return tfc.pow(x, a);\n  });\n}\n\nfunction reshapeBias(xRank, bias, dataFormat) {\n  var biasShape = bias.shape;\n\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(\"Unexpected bias dimensions: \" + bias.rank + (\"; expected it to be 1 or \" + xRank));\n  }\n\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1, 1, 1]);\n      } else {\n        return bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1, 1]);\n      } else {\n        return bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1]);\n      } else {\n        return bias.reshape([1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n\n  throw new ValueError(\"Unsupported input rank by biasAdd: \" + bias.rank);\n}\n\nexport function biasAdd(x, bias, dataFormat) {\n  return tidy(function () {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n\n    checkDataFormat(dataFormat);\n    return x.add(reshapeBias(x.rank, bias, dataFormat));\n  });\n}\nexport function elu(x) {\n  var alpha = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : 1;\n\n  if (alpha !== 1) {\n    throw new NotImplementedError(\"Support for alpha values other than 1 (\" + alpha + \") is not implemented \" + \"yet.\");\n  }\n\n  return tfc.elu(x);\n}\nexport function softsign(x) {\n  return tidy(function () {\n    return tfc.div(x, tfc.abs(x).add(1));\n  });\n}\nexport function dropout(x, level, noiseShape, seed) {\n  return tidy(function () {\n    return tfc.dropout(x, level, noiseShape, seed);\n  });\n}\nexport function hardSigmoid(x) {\n  return tidy(function () {\n    var y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\nexport function inTrainPhase(x, alt) {\n  var training = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : false;\n  return training ? x() : alt();\n}","map":{"version":3,"sources":["../../src/backend/tfjs_backend.ts"],"names":[],"mappings":";;AAAA;;;;;;;;AAQG;AAMH,OAAO,KAAK,GAAZ,MAAqB,uBAArB;AACA,SAAQ,QAAQ,IAAI,YAApB,EAAkC,MAAlC,EAA4D,QAA5D,EAA8G,IAA9G,EAAoH,KAApH,EAA2H,SAAS,IAAI,aAAxI,QAA4J,uBAA5J;AACA,SAAQ,eAAR;AACA,SAAQ,mBAAR,EAA6B,UAA7B;AAGA,OAAO,KAAK,UAAZ;AAEA,SAAQ,eAAR;AAOA,IAAI,OAAO,GAAkB,OAA7B;AAEA,OAAM,SAAU,UAAV,CAAqB,gBAArB,EAAoD;EACxD,GAAG,CAAC,UAAJ,CAAe,gBAAf;EACA,OAAO,GAAG,gBAAV;AACD;AAED,OAAM,SAAU,UAAV,GAAoB;EACxB,OAAO,OAAP;AACD;AASD,OAAM,SAAU,iBAAV,GAA2B;EAC/B,OAAO,KAAP;AACD;AAOD,OAAM,SAAU,WAAV,CAAsB,CAAtB,EAAiC;EACrC,IAAM,KAAK,GAAG,CAAC,CAAC,KAAhB;;EACA,IAAI,KAAK,CAAC,MAAN,GAAe,CAAnB,EAAsB;IACpB,OAAO,KAAK,CAAC,MAAN,CAAa,UAAC,CAAD,EAAY,CAAZ;MAAA,OAA0B,CAAC,GAAG,CAA9B;IAAA,CAAb,CAAP;EACD,CAFD,MAEO;IAEL,OAAO,CAAP;EACD;AACF;AAQD,OAAM,SAAU,IAAV,CAAe,CAAf,EAA0B,KAA1B,EAA6C;EACjD,OAAO,CAAC,CAAC,MAAF,CAAS,KAAT,CAAP;AACD;AAQD,OAAM,SAAU,UAAV,CAAqB,CAArB,EAAyC;EAAA,IAAT,IAAS,uEAAF,CAAC,CAAC;EAC7C,IAAM,QAAQ,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAjB;;EACA,IAAI,IAAI,GAAG,CAAX,EAAc;IACZ,IAAI,GAAG,QAAQ,CAAC,MAAT,GAAkB,IAAlB,GAAyB,CAAhC;EACD;;EACD,QAAQ,CAAC,MAAT,CAAgB,IAAhB,EAAsB,CAAtB,EAAyB,CAAzB;EACA,OAAO,CAAC,CAAC,OAAF,CAAU,QAAV,CAAP;AACD;AAaD,OAAM,SAAU,MAAV,CAAiB,CAAjB,EAA4B,CAA5B,EAAqC;EACzC,OAAO,IAAI,CAAC,YAAK;IACf,IAAI,CAAC,CAAC,KAAF,CAAQ,MAAR,KAAmB,CAAvB,EAA0B;MACxB,MAAM,IAAI,UAAJ,CACF,iEACQ,CAAC,CAAC,KAAF,CAAQ,MADhB,cADE,CAAN;IAGD;;IACD,IAAM,CAAC,GAAG,UAAU,CAAC,CAAD,EAAI,CAAJ,CAApB;IACA,OAAO,IAAI,CAAC,CAAD,EAAI,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,CAAJ,CAAX;EACD,CARU,CAAX;AASD;AAOD,OAAM,SAAU,OAAV,CAAkB,CAAlB,EAA2B;EAC/B,IAAM,QAAQ,GAAG,CAAC,UAAU,CAAC,SAAX,CAAqB,CAAC,CAAC,KAAvB,CAAD,CAAjB;EACA,OAAO,CAAC,CAAC,OAAF,CAAU,QAAV,CAAP;AACD;AAUD,OAAM,SAAU,YAAV,CAAuB,CAAvB,EAAgC;EACpC,IAAI,CAAC,CAAC,IAAF,IAAU,CAAd,EAAiB;IACf,MAAM,IAAI,UAAJ,2DACsD,CAAC,CAAC,IADxD,OAAN;EAED;;EACD,IAAM,QAAQ,GAAG,CAAC,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAD,EAAa,UAAU,CAAC,SAAX,CAAqB,CAAC,CAAC,KAAvB,EAA8B,CAA9B,CAAb,CAAjB;EACA,OAAO,CAAC,CAAC,OAAF,CAAU,QAAV,CAAP;AACD;AAUD,OAAM,SAAU,mBAAV,CACF,KADE,EACa,KADb,EAC4B,IAD5B,EACwC;EAC5C,OAAO,IAAI,CAAC,YAAK;IACf,QAAQ,KAAK,CAAC,IAAd;MACE,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CAAY,KAAZ,EAA+B,KAA/B,EAAsC,IAAtC,CAAP;;MACF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,KAAD,EAAQ,CAAR,CADhB,EAC4B,CAAC,IAAD,EAAO,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAP,CAD5B,CAAP;;MAEF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,KAAD,EAAQ,CAAR,EAAW,CAAX,CADhB,EAEH,CAAC,IAAD,EAAO,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAP,EAAuB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvB,CAFG,CAAP;;MAGF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,CADhB,EAEH,CAAC,IAAD,EAAO,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAP,EAAuB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvB,EAAuC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;MAGF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,KAAJ,CAAU,KAAV,EAA6B,CAAC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,EAAiB,CAAjB,CAA7B,EAAkD,CACvD,IADuD,EACjD,KAAK,CAAC,KAAN,CAAY,CAAZ,CADiD,EACjC,KAAK,CAAC,KAAN,CAAY,CAAZ,CADiC,EACjB,KAAK,CAAC,KAAN,CAAY,CAAZ,CADiB,EACD,KAAK,CAAC,KAAN,CAAY,CAAZ,CADC,CAAlD,CAAP;;MAGF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,KAAJ,CAAU,KAAV,EAAiB,CAAC,KAAD,EAAQ,CAAR,EAAW,CAAX,EAAc,CAAd,EAAiB,CAAjB,EAAoB,CAApB,CAAjB,EAAyC,CAC9C,IAD8C,EACxC,KAAK,CAAC,KAAN,CAAY,CAAZ,CADwC,EACxB,KAAK,CAAC,KAAN,CAAY,CAAZ,CADwB,EACR,KAAK,CAAC,KAAN,CAAY,CAAZ,CADQ,EACQ,KAAK,CAAC,KAAN,CAAY,CAAZ,CADR,EAE9C,KAAK,CAAC,KAAN,CAAY,CAAZ,CAF8C,CAAzC,CAAP;;MAIF;QACE,MAAM,IAAI,UAAJ,CACF,sEACG,KAAK,CAAC,IADT,CADE,CAAN;IAxBJ;EA4BD,CA7BU,CAAX;AA8BD;AAUD,OAAM,SAAU,kBAAV,CACF,KADE,EACa,KADb,EAC4B,IAD5B,EACwC;EAC5C,OAAO,IAAI,CAAC,YAAK;IACf,QAAQ,KAAK,CAAC,IAAd;MACE,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CAAY,KAAZ,EAA+B,KAA/B,EAAsC,IAAtC,CAAP;;MACF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,KAAJ,CADhB,EAC4B,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,IAAjB,CAD5B,CAAP;;MAEF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAO,KAAP,CADhB,EAEH,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAjB,EAAiC,IAAjC,CAFG,CAAP;;MAGF,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,KAAV,CADhB,EAEH,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAjB,EAAiC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAjC,EAAiD,IAAjD,CAFG,CAAP;;MAGF;QACE,MAAM,IAAI,UAAJ,CACF,qEACG,KAAK,CAAC,IADT,CADE,CAAN;IAfJ;EAmBD,CApBU,CAAX;AAqBD;AAWD,OAAM,SAAU,cAAV,CACF,KADE,EACa,KADb,EAC4B,IAD5B,EAC0C,IAD1C,EACsD;EAC1D,OAAO,IAAI,CAAC,YAAK;IACf,QAAQ,KAAK,CAAC,IAAd;MACE,KAAK,CAAL;QACE,OAAO,GAAG,CAAC,OAAJ,CAAY,KAAZ,EAA+B,KAA/B,EAAsC,IAAtC,CAAP;;MACF,KAAK,CAAL;QACE,QAAQ,IAAR;UACE,KAAK,CAAL;YACE,OAAO,mBAAmB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAA1B;;UACF,KAAK,CAAL;YACE,OAAO,kBAAkB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAAzB;;UACF;YACE,MAAM,IAAI,UAAJ,CACF,yDACG,IADH,CADE,CAAN;QANJ;;MAUF,KAAK,CAAL;QACE,QAAQ,IAAR;UACE,KAAK,CAAL;YACE,OAAO,mBAAmB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAA1B;;UACF,KAAK,CAAL;YACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,KAAJ,EAAW,CAAX,CADhB,EAEH,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,IAAjB,EAAuB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvB,CAFG,CAAP;;UAGF,KAAK,CAAL;YACE,OAAO,kBAAkB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAAzB;;UACF;YACE,MAAM,IAAI,UAAJ,CACF,yDACG,IADH,CADE,CAAN;QAVJ;;MAcF,KAAK,CAAL;QACE,QAAQ,IAAR;UACE,KAAK,CAAL;YACE,OAAO,mBAAmB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAA1B;;UACF,KAAK,CAAL;YACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,KAAJ,EAAW,CAAX,EAAc,CAAd,CADhB,EAEH,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,IAAjB,EAAuB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvB,EAAuC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;UAGF,KAAK,CAAL;YACE,OAAO,GAAG,CAAC,OAAJ,CACH,KADG,EACgB,CAAC,CAAD,EAAI,CAAJ,EAAO,KAAP,EAAc,CAAd,CADhB,EAEH,CAAC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAD,EAAiB,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAjB,EAAiC,IAAjC,EAAuC,KAAK,CAAC,KAAN,CAAY,CAAZ,CAAvC,CAFG,CAAP;;UAGF,KAAK,CAAL;YACE,OAAO,kBAAkB,CAAC,KAAD,EAAQ,KAAR,EAAe,IAAf,CAAzB;;UACF;YACE,MAAM,IAAI,UAAJ,CACF,yDACG,IADH,CADE,CAAN;QAdJ;;MAkBF;QACE,MAAM,IAAI,UAAJ,CACF,qEACG,KAAK,CAAC,IADT,CADE,CAAN;IAjDJ;EAqDD,CAtDU,CAAX;AAuDD;AAQD,OAAM,SAAU,WAAV,CAAsB,OAAtB,EAAkD;EAAA,IAAT,IAAS,uEAAF,CAAC,CAAC;EACtD,IAAI,IAAJ;;EACA,IAAI,IAAI,GAAG,CAAX,EAAc;IACZ,IAAI,GAAG,OAAO,CAAC,CAAD,CAAP,CAAW,IAAlB;;IACA,IAAI,IAAI,KAAK,CAAb,EAAgB;MACd,IAAI,GAAG,IAAP;IACD,CAFD,MAEO;MACL,IAAI,GAAG,CAAP;IACD;EACF;;EACD,IAAI,IAAI,KAAK,OAAO,CAAC,CAAD,CAAP,CAAW,IAAxB,EAA8B;IAG5B,IAAI,GAAG,CAAC,CAAR;EACD;;EAED,OAAO,GAAG,CAAC,MAAJ,CAAW,OAAX,EAAoB,IAApB,CAAP;AACD;AASD,OAAM,SAAU,oBAAV,CAA+B,CAA/B,EAA0C,CAA1C,EAAmD;EACvD,QAAQ,CAAC,CAAC,IAAV;IACE,KAAK,CAAL;MACE,OAAO,GAAG,CAAC,QAAJ,CAAa,CAAC,CAAD,EAAgB,CAAhB,CAAb,CAAP;;IACF,KAAK,CAAL;MACE,OAAO,GAAG,CAAC,QAAJ,CAAa,CAAC,CAAD,EAAgB,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;IACF,KAAK,CAAL;MACE,OAAO,GAAG,CAAC,QAAJ,CAAa,CAAC,CAAD,EAAgB,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;IACF,KAAK,CAAL;MACE,OAAO,GAAG,CAAC,QAAJ,CAAa,CAAC,CAAD,EAAgB,CAAhB,CAAb,EAA6C,CAA7C,CAAP;;IACF;MACE,MAAM,IAAI,UAAJ,CACF,uEACgB,CAAC,CAAC,IADlB,CADE,CAAN;EAVJ;AAcD;AASD,OAAM,SAAU,IAAV,CAAe,CAAf,EAA0B,CAA1B,EAA4C;EAChD,IAAI,CAAC,KAAK,CAAC,OAAN,CAAc,CAAd,CAAL,EAAuB;IACrB,CAAC,GAAG,CAAC,CAAD,CAAJ;EACD;;EACD,IAAI,CAAC,CAAC,IAAF,KAAW,CAAC,CAAC,MAAjB,EAAyB;IACvB,MAAM,IAAI,UAAJ,CACF,4BAA0B,CAAC,CAAC,MAA5B,oEACwC,CAAC,CAAC,IAD1C,OADE,CAAN;EAGD;;EACD,OAAO,GAAG,CAAC,IAAJ,CAAS,CAAT,EAAY,CAAZ,CAAP;AACD;AAcD,OAAM,SAAU,YAAV,CACF,KADE,EAEW;EAAA,IADC,IACD,uEADQ,GACR;EAAA,IADa,MACb,uEADsB,GACtB;EAAA,IAD2B,KAC3B;EAAA,IAAb,IAAa;EACf,OAAO,GAAG,CAAC,YAAJ,CAAiB,KAAjB,EAAwB,IAAxB,EAA8B,MAA9B,EAAsC,KAAtC,EAA6C,IAA7C,CAAP;AACD;AAoBD,OAAM,SAAU,GAAV,CACF,CADE,EACS,CADT,EACoB,UADpB,EAEF,IAFE,EAEW;EACf,IAAK,CAAC,CAAC,IAAF,GAAS,CAAV,IAAiB,CAAC,CAAC,IAAF,GAAS,CAA9B,EAAkC;IAChC,MAAM,IAAI,mBAAJ,CACF,sEACsB,CAAC,CAAC,KADxB,uBAC+C,CAAC,CAAC,KADjD,CADE,CAAN;EAGD;;EACD,IAAI,CAAC,CAAC,IAAF,IAAU,CAAd,EAAiB;IACf,IAAM,QAAQ,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAC,CAAf,EAAkB,CAAlB,CAAjB;IACA,IAAM,cAAc,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,CAAc,CAAC,CAAf,EAAkB,CAAlB,CAAvB;;IACA,IAAI,QAAQ,KAAK,cAAjB,EAAiC;MAC/B,MAAM,IAAI,mBAAJ,CACF,wGAEI,CAAC,CAAC,KAFN,+BAGc,CAAC,CAAC,KAHhB,CADE,CAAN;IAKD;EACF;;EAED,IAAK,CAAC,CAAC,IAAF,KAAW,CAAZ,IAAmB,CAAC,CAAC,IAAF,KAAW,CAAlC,EAAsC;IACpC,IAAM,UAAU,GAAG,KAAnB;IACA,IAAM,UAAU,GAAG,KAAnB;IAIA,OAAO,GAAG,CAAC,KAAJ,CAAU,MAAV,CAAiB;MACtB,CAAC,EAAD,CADsB;MAEtB,CAAC,EAAE,CAFmB;MAGtB,UAAU,EAAV,UAHsB;MAItB,UAAU,EAAV,UAJsB;MAKtB,IAAI,EAAE,IAAI,GAAG,WAAW,CAAC,CAAC,CAAC,IAAH,EAAS,IAAT,EAAe,eAAe,EAA9B,CAAd,GAAkD,IALtC;MAMtB,UAAU,EAAV;IANsB,CAAjB,CAAP;EAQD,CAdD,MAcO;IAEL,IAAM,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAnB;IACA,IAAM,QAAQ,GAAG,UAAU,CAAC,GAAX,EAAjB;IACA,CAAC,GAAG,CAAC,CAAC,OAAF,CAAU,CAAC,CAAC,CAAF,EAAK,QAAL,CAAV,CAAJ;IAIA,IAAM,MAAM,GAAG,CAAC,CAAC,KAAF,CAAQ,KAAR,EAAf;IACA,IAAM,QAAQ,GAAG,MAAM,CAAC,GAAP,EAAjB;;IACA,IAAM,eAAc,GAAG,MAAM,CAAC,GAAP,EAAvB;;IACA,IAAM,UAAU,gCAAO,MAAP,IAAe,QAAf,EAAhB;IAGA,IAAM,IAAI,GAAG,KAAK,CAAC,IAAN,CAAW;MAAC,MAAM,EAAE,CAAC,CAAC;IAAX,CAAX,EAA6B,UAAC,CAAD,EAAI,CAAJ,EAAS;MACjD,IAAI,CAAC,KAAK,CAAV,EAAa;QACX,OAAO,CAAC,CAAC,IAAF,GAAS,CAAhB;MACD,CAFD,MAEO,IAAI,CAAC,IAAI,CAAC,CAAC,IAAF,GAAS,CAAlB,EAAqB;QAC1B,OAAO,CAAC,GAAG,CAAX;MACD;;MACD,OAAO,CAAP;IACD,CAPY,CAAb;IAQA,CAAC,GAAG,CAAC,CAAC,SAAF,CAAY,IAAZ,EAAkB,OAAlB,CAA0B,CAAC,eAAD,EAAiB,CAAC,CAAlB,CAA1B,CAAJ;IAGA,IAAM,WAAW,gCAAO,UAAP,sBAAsB,UAAtB,EAAjB;IACA,IAAM,WAAU,GAAG,KAAnB;IACA,IAAM,WAAU,GAAG,KAAnB;IACA,OAAO,GAAG,CAAC,KAAJ,CACF,MADE,CACK;MACN,CAAC,EAAD,CADM;MAEN,CAAC,EAAD,CAFM;MAGN,UAAU,EAAV,WAHM;MAIN,UAAU,EAAV,WAJM;MAKN,IAAI,EAAE,IAAI,GAAG,WAAW,CAAC,CAAC,CAAC,IAAH,EAAS,IAAT,EAAe,eAAe,EAA9B,CAAd,GAAkD,IALtD;MAMN,UAAU,EAAV;IANM,CADL,EASF,OATE,CASM,WATN,CAAP;EAUD;AACF;AAYD,OAAM,SAAU,IAAV,CAAe,CAAf,EAAwB;EAE5B,OAAO,IAAI,CAAC,YAAK;IACf,IAAM,UAAU,GAAG,aAAa,CAAC,CAAD,CAAhC;IACA,IAAM,SAAS,GAAG,YAAY,CAAC,CAAD,CAA9B;IACA,OAAO,KAAK,CACR,GAAG,CAAC,KAAJ,CAAU,CAAV,EAAa,UAAb,CADQ,EACkB,UADlB,EAER,KAAK,CACD,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,aAAa,CAAC,CAAD,CAA5B,CADC,EACiC,SADjC,EAED,GAAG,CAAC,GAAJ,CAAQ,CAAC,CAAT,EAAY,SAAZ,CAFC,CAFG,CAAZ;EAKD,CARU,CAAX;AASD;AAUD,OAAM,SAAU,MAAV,CAAiB,OAAjB,EAAkC,UAAlC,EAAoD;EACxD,OAAO,IAAI,CAAC,YAAK;IACf,IAAI,OAAO,CAAC,IAAR,KAAiB,CAArB,EAAwB;MACtB,MAAM,IAAI,KAAJ,CACF,kDACA,gCAFE,CAAN;IAGD;;IACD,OAAO,GAAG,OAAO,CAAC,KAAR,EAAV;IACA,OAAO,GAAG,CAAC,MAAJ,CAAW,OAAX,EAAgC,UAAhC,EAA4C,OAA5C,EAAP;EACD,CARU,CAAX;AASD;AAWD,OAAM,SAAU,MAAV,CACF,SADE,EACiB,OADjB,EAC6C,IAD7C,EAC0D;EAC9D,OAAO,IAAI,CAAC,YAAK;IACf,IAAI,KAAK,CAAC,OAAN,CAAc,OAAd,CAAJ,EAA4B;MAC1B,OAAO,GAAG,QAAQ,CAAC,OAAD,EAAU,OAAV,CAAlB;IACD,CAFD,MAEO;MACL,OAAO,GAAG,OAAO,CAAC,KAAR,EAAV;IACD;;IACD,OAAO,GAAG,CAAC,MAAJ,CAAW,SAAX,EAAsB,OAAtB,EAA+B,IAA/B,CAAP;EACD,CAPU,CAAX;AAQD;AAOD,OAAM,SAAU,MAAV,CAAiB,CAAjB,EAA0B;EAC9B,OAAO,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,CAAX,CAAP;AACD;AAcD,OAAM,SAAU,GAAV,CAAc,CAAd,EAAyB,CAAzB,EAAyC;EAC7C,OAAO,IAAI,CAAC,YAAK;IACf,IAAI,OAAQ,CAAR,KAAe,QAAnB,EAA6B;MAC3B,CAAC,GAAG,MAAM,CAAC,IAAI,CAAC,KAAL,CAAW,CAAX,CAAD,EAAgB,OAAhB,CAAV;IACD;;IACD,IAAI,CAAC,CAAC,KAAF,KAAY,OAAhB,EAAyB;MACvB,MAAM,IAAI,mBAAJ,uBACkB,CAAC,CAAC,KADpB,qCAAN;IAED;;IACD,OAAO,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,CAAX,CAAP;EACD,CATU,CAAX;AAUD;;AAKD,SAAS,WAAT,CAAqB,KAArB,EAAoC,IAApC,EAAkD,UAAlD,EAAoE;EAClE,IAAM,SAAS,GAAG,IAAI,CAAC,KAAvB;;EAEA,IAAI,IAAI,CAAC,IAAL,KAAc,CAAd,IAAmB,IAAI,CAAC,IAAL,KAAc,KAArC,EAA4C;IAC1C,MAAM,IAAI,UAAJ,CACF,iCAA+B,IAAI,CAAC,IAApC,kCAC4B,KAD5B,CADE,CAAN;EAGD;;EAED,IAAI,KAAK,KAAK,CAAd,EAAiB;IACf,IAAI,UAAU,KAAK,eAAnB,EAAoC;MAClC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,EAAqB,CAArB,EAAwB,CAAxB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CACH,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,SAAS,CAAC,CAAD,CAA3B,EAAgC,SAAS,CAAC,CAAD,CAAzC,EAA8C,SAAS,CAAC,CAAD,CAAvD,CADG,CAAP;MAED;IACF,CAPD,MAOO,IAAI,UAAU,KAAK,cAAnB,EAAmC;MACxC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,CAAV,EAAa,SAAS,CAAC,CAAD,CAAtB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,MAAJ,CAAW,SAAX,CAAb,CAAP;MACD;IACF;EACF,CAfD,MAeO,IAAI,KAAK,KAAK,CAAd,EAAiB;IACtB,IAAI,UAAU,KAAK,eAAnB,EAAoC;MAClC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,EAAqB,CAArB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,SAAS,CAAC,CAAD,CAA3B,EAAgC,SAAS,CAAC,CAAD,CAAzC,CAAb,CAAP;MACD;IACF,CAND,MAMO,IAAI,UAAU,KAAK,cAAnB,EAAmC;MACxC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,SAAS,CAAC,CAAD,CAAnB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,MAAJ,CAAW,SAAX,CAAb,CAAP;MACD;IACF;EACF,CAdM,MAcA,IAAI,KAAK,KAAK,CAAd,EAAiB;IACtB,IAAI,UAAU,KAAK,eAAnB,EAAoC;MAClC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,CAAlB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,SAAS,CAAC,CAAD,CAAb,EAAkB,SAAS,CAAC,CAAD,CAA3B,CAAb,CAAP;MACD;IACF,CAND,MAMO,IAAI,UAAU,KAAK,cAAnB,EAAmC;MACxC,IAAI,SAAS,CAAC,MAAV,KAAqB,CAAzB,EAA4B;QAC1B,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,SAAS,CAAC,CAAD,CAAhB,CAAb,CAAP;MACD,CAFD,MAEO;QACL,OAAO,IAAI,CAAC,OAAL,CAAa,CAAC,CAAD,EAAI,MAAJ,CAAW,SAAX,CAAb,CAAP;MACD;IACF;EACF,CAdM,MAcA,IAAI,KAAK,GAAG,CAAZ,EAAe;IACpB,OAAO,IAAP;EACD;;EACD,MAAM,IAAI,UAAJ,yCAAqD,IAAI,CAAC,IAA1D,CAAN;AACD;;AAYD,OAAM,SAAU,OAAV,CACF,CADE,EACS,IADT,EACuB,UADvB,EAC8C;EAClD,OAAO,IAAI,CAAC,YAAK;IACf,IAAI,UAAU,IAAI,IAAlB,EAAwB;MACtB,UAAU,GAAG,eAAe,EAA5B;IACD;;IACD,eAAe,CAAC,UAAD,CAAf;IAEA,OAAO,CAAC,CAAC,GAAF,CAAM,WAAW,CAAC,CAAC,CAAC,IAAH,EAAS,IAAT,EAAe,UAAf,CAAjB,CAAP;EACD,CAPU,CAAX;AAQD;AAQD,OAAM,SAAU,GAAV,CAAc,CAAd,EAAkC;EAAA,IAAT,KAAS,uEAAD,CAAC;;EAEtC,IAAI,KAAK,KAAK,CAAd,EAAiB;IACf,MAAM,IAAI,mBAAJ,CACF,4CAA0C,KAA1C,mCADE,CAAN;EAGD;;EACD,OAAO,GAAG,CAAC,GAAJ,CAAQ,CAAR,CAAP;AACD;AAUD,OAAM,SAAU,QAAV,CAAmB,CAAnB,EAA4B;EAChC,OAAO,IAAI,CAAC;IAAA,OAAM,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAG,CAAC,GAAJ,CAAQ,CAAR,EAAW,GAAX,CAAe,CAAf,CAAX,CAAN;EAAA,CAAD,CAAX;AACD;AAYD,OAAM,SAAU,OAAV,CACF,CADE,EACS,KADT,EACwB,UADxB,EAC+C,IAD/C,EAC4D;EAChE,OAAO,IAAI,CAAC;IAAA,OAAM,GAAG,CAAC,OAAJ,CAAY,CAAZ,EAAe,KAAf,EAAsB,UAAtB,EAAkC,IAAlC,CAAN;EAAA,CAAD,CAAX;AACD;AAWD,OAAM,SAAU,WAAV,CAAsB,CAAtB,EAA+B;EACnC,OAAO,IAAI,CAAC,YAAK;IACf,IAAM,CAAC,GAAG,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,GAAG,CAAC,GAAJ,CAAQ,EAAR,EAAY,CAAZ,CAAZ,CAAV;IACA,OAAO,GAAG,CAAC,WAAJ,CAAgB,CAAhB,EAAmB,CAAnB,EAAsB,CAAtB,CAAP;EACD,CAHU,CAAX;AAID;AAeD,OAAM,SAAU,YAAV,CAA0B,CAA1B,EAAsC,GAAtC,EAAoE;EAAA,IAAhB,QAAgB,uEAAL,KAAK;EACxE,OAAO,QAAQ,GAAG,CAAC,EAAJ,GAAS,GAAG,EAA3B;AACD","sourceRoot":"","sourcesContent":["/**\n * @license\n * Copyright 2018 Google LLC\n *\n * Use of this source code is governed by an MIT-style\n * license that can be found in the LICENSE file or at\n * https://opensource.org/licenses/MIT.\n * =============================================================================\n */\n/**\n * deeplearn.js backend.\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from '../common';\nimport { NotImplementedError, ValueError } from '../errors';\nimport * as math_utils from '../utils/math_utils';\nimport { imageDataFormat } from './common';\n// tslint:enable\n/* Setting and getting backend from deeplearn.js. */\n// Default deeplearn.js backend is WebGL (GPU).\nlet backend = 'webgl';\nexport function setBackend(requestedBackend) {\n    tfc.setBackend(requestedBackend);\n    backend = requestedBackend;\n}\nexport function getBackend() {\n    return backend;\n}\n/**\n * Indicates whether the backend is operating symbolically.\n *\n * This function will be used to determine how to interpret user code. If\n * it returns true, calls to the backend construct a symbolic graph; if\n * it returns false, calls to the backend execute immediately.\n */\nexport function isBackendSymbolic() {\n    return false;\n}\n/**\n * Get the number of elements in a Tensor.\n * @param x The Tensor.\n * @return Number of elements in `x`.\n */\nexport function countParams(x) {\n    const shape = x.shape;\n    if (shape.length > 0) {\n        return shape.reduce((a, b) => a * b);\n    }\n    else {\n        // Scalar.\n        return 1;\n    }\n}\n/**\n * Casts a tensor to a different dtype and returns it.\n * @param x Input tensor.\n * @param dtype String: 'float32'|'int32'|'bool'.\n * @returns Tensor of the specified `dtype`.\n */\nexport function cast(x, dtype) {\n    return x.asType(dtype);\n}\n/**\n * Adds a 1-sized dimension at index \"axis\".\n * @param x Input tensor.\n * @param axis Position where to add the new axis.\n * @returns Result of the dimension expansion.\n */\nexport function expandDims(x, axis = -1) {\n    const outShape = x.shape.slice();\n    if (axis < 0) {\n        axis = outShape.length + axis + 1;\n    }\n    outShape.splice(axis, 0, 1);\n    return x.reshape(outShape);\n}\n/**\n * Repeats a 2D tensor.\n *\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\n * will have shape `[samples, 2, dim]`.\n *\n * @param x Input tensor.\n * @param n Integer, number of times to repeat.\n * @returns The result of the repeat operation.\n * @throws ValueError: If input tensor is not 2D.\n */\nexport function repeat(x, n) {\n    return tidy(() => {\n        if (x.shape.length !== 2) {\n            throw new ValueError(`repeat() expects a rank-2 tensor, but received a ` +\n                `rank-${x.shape.length} tensor.`);\n        }\n        const y = expandDims(x, 1);\n        return tile(y, [1, n, 1]);\n    });\n}\n/**\n * Flatten a Tensor into 1D.\n * @param x Input tensor.\n * @return The result of the flattening `x`.\n */\nexport function flatten(x) {\n    const newShape = [math_utils.arrayProd(x.shape)];\n    return x.reshape(newShape);\n}\n/**\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\n * In other words, it flattens each data samples of a batch.\n *\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\n *   or higher.\n * @return The result of the flattening.\n */\nexport function batchFlatten(x) {\n    if (x.rank <= 1) {\n        throw new ValueError(`batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n    }\n    const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n    return x.reshape(newShape);\n}\n/**\n * Do slicing along the first axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the first axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongFirstAxis(array, start, size) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n            case 3:\n                return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n            case 4:\n                return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n            case 5:\n                return tfc.slice(array, [start, 0, 0, 0, 0], [\n                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]\n                ]);\n            case 6:\n                return tfc.slice(array, [start, 0, 0, 0, 0, 0], [\n                    size, array.shape[1], array.shape[2], array.shape[3], array.shape[4],\n                    array.shape[5]\n                ]);\n            default:\n                throw new ValueError(`sliceAlongFirstAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Do slicing along the last axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size size of the slice along the last axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongLastAxis(array, start, size) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n            case 3:\n                return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n            case 4:\n                return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n            default:\n                throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Do slicing along the sepcified axis.\n * @param array input `tf.Tensor`.\n * @param start starting index, inclusive.\n * @param size of the slice along the chosen axis.\n * @param choose an axis.\n * @returns result of the slicing.\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function sliceAlongAxis(array, start, size, axis) {\n    return tidy(() => {\n        switch (array.rank) {\n            case 1:\n                return tfc.slice1d(array, start, size);\n            case 2:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            case 3:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n                    case 3:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            case 4:\n                switch (axis) {\n                    case 1:\n                        return sliceAlongFirstAxis(array, start, size);\n                    case 2:\n                        return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n                    case 3:\n                        return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n                    case 4:\n                        return sliceAlongLastAxis(array, start, size);\n                    default:\n                        throw new ValueError(`The axis is not within the rank of the tensor ` +\n                            `${axis}`);\n                }\n            default:\n                throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` +\n                    `${array.rank}`);\n        }\n    });\n}\n/**\n * Concatenates a list of tensors alongside the specified axis.\n * @param tensors `Array` of tensors to concatenate.\n * @param axis Concatenation axis.\n * @returns The result of the concatenation.\n */\nexport function concatenate(tensors, axis = -1) {\n    let rank;\n    if (axis < 0) {\n        rank = tensors[0].rank;\n        if (rank !== 0) {\n            axis = rank;\n        }\n        else {\n            axis = 0;\n        }\n    }\n    if (axis === tensors[0].rank) {\n        // Porting Note: This is necessary because tfc.concat() requires axis to be\n        //   in the interval [-rank, rank).\n        axis = -1;\n    }\n    // Porting Note: Sparse concat is not supported yet.\n    return tfc.concat(tensors, axis);\n}\n/**\n * Concatenate two arrays along the first dimension.\n * @param a The 1st `tf.Tensor` to concatenate.\n * @param b The 2nd `tf.Tensor` to concatenate.\n * @returns Result of the concatenation.\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\n */\nexport function concatAlongFirstAxis(a, b) {\n    switch (a.rank) {\n        case 1:\n            return tfc.concat1d([a, b]);\n        case 2:\n            return tfc.concat2d([a, b], 0);\n        case 3:\n            return tfc.concat3d([a, b], 0);\n        case 4:\n            return tfc.concat4d([a, b], 0);\n        default:\n            throw new ValueError(`concatAlongFirstAxis() received an unsupported ` +\n                `tensor rank: ${a.rank}`);\n    }\n}\n/**\n * Creates a tensor by tiling `x` by `n`.\n * @param x A tensor.\n * @param n An Array of integers or a single integer. If an Array, the length\n *   must be the same as the number of dimensions in `x`. If a single integer,\n *   it will be treated as an Array of length 1.\n */\nexport function tile(x, n) {\n    if (!Array.isArray(n)) {\n        n = [n];\n    }\n    if (x.rank !== n.length) {\n        throw new ValueError(`The length of input n (${n.length}) does not match ` +\n            `the number of dimensions in input x (${x.rank})`);\n    }\n    return tfc.tile(x, n);\n}\n/* Creation of random tensors. */\n/**\n * Get a tensor with normal distribution of values.\n *\n * @param shape Shape of the tensor.\n * @param mean mean value of the normal distribution.\n * @param stddev standard deviation of the normal distribution.\n * @param dtype\n * @param seed\n * @return The normal tensor.\n */\nexport function randomNormal(shape, mean = 0.0, stddev = 1.0, dtype, seed) {\n    return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n/* Linear Algebra */\n/**\n * Multiply two tensors and returns the result as a tensor.\n *\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\n * For tensors of higher ranks, it follows the Theano behavior,\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\n *\n * For N dimensions it is a sum product over the last axis of x and the\n * second-to-last of y:\n *\n * @param a A tensor of at least rank 2.\n * @param b A tensor of at least rank 2.\n * @param activation (optional) A string identifying the activation\n *   function.\n * @return Result of the dot operation.\n */\nexport function dot(a, b, activation, bias) {\n    if ((a.rank < 2) || (b.rank < 2)) {\n        throw new NotImplementedError(`dot requires both inputs to be rank >= 2` +\n            ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n    }\n    if (b.rank >= 3) {\n        const xLastDim = a.shape.slice(-1)[0];\n        const ySecondLastDim = b.shape.slice(-2)[0];\n        if (xLastDim !== ySecondLastDim) {\n            throw new NotImplementedError(`If rank y >= 3, then the second last dim` +\n                ` of y must equal the last dim of x but got x shape = ${a.shape} and ` +\n                ` y shape = ${b.shape}`);\n        }\n    }\n    // Handle basic 2D x 2D case.\n    if ((a.rank === 2) && (b.rank === 2)) {\n        const transposeA = false;\n        const transposeB = false;\n        // tfc.fused.matMul only fuses certain activation functions. Unsupported\n        // activation functions are treated as 'linear' activations, which is\n        // equivalent to a no-op.\n        return tfc.fused.matMul({\n            a,\n            b: b,\n            transposeA,\n            transposeB,\n            bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n            activation\n        });\n    }\n    else {\n        // Reshape x into the analogous 2D Tensor.\n        const aFirstDims = a.shape.slice(); // Holds all but the last dim of x.\n        const aLastDim = aFirstDims.pop();\n        a = a.reshape([-1, aLastDim]);\n        // Reshape y into the analogous 2D Tensor, and keep track of the\n        // required dimensions to reproduce the output shape.\n        const bShape = b.shape.slice();\n        const bLastDim = bShape.pop();\n        const ySecondLastDim = bShape.pop();\n        const yOtherDims = [...bShape, bLastDim];\n        // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n        // where r is the rank of y.\n        const perm = Array.from({ length: b.rank }, (_, i) => {\n            if (i === 0) {\n                return b.rank - 2;\n            }\n            else if (i <= b.rank - 2) {\n                return i - 1;\n            }\n            return i;\n        });\n        b = b.transpose(perm).reshape([ySecondLastDim, -1]);\n        // Multiply x and y as 2D Tensors, and then reshape back to original.\n        const outputShape = [...aFirstDims, ...yOtherDims];\n        const transposeA = false;\n        const transposeB = false;\n        return tfc.fused\n            .matMul({\n            a,\n            b,\n            transposeA,\n            transposeB,\n            bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n            activation\n        })\n            .reshape(outputShape);\n    }\n}\n/**\n * Compute the sign Tensor of an input Tensor.\n *\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\n *\n * @param x Input `tf.Tensor`.\n * @return The sign `tf.Tensor`.\n */\nexport function sign(x) {\n    // TODO(cais): Move to the core.\n    return tidy(() => {\n        const zerosLikeX = coreZerosLike(x);\n        const onesLikeX = coreOnesLike(x);\n        return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n    });\n}\n/**\n * Computes the one-hot representation of an integer tensor.\n * @param indices nD integer tensor of shape\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\n * @param numClasses Integer, number of classes to consider.\n * @returns (n + 1)D one hot representation of the input\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\n */\nexport function oneHot(indices, numClasses) {\n    return tidy(() => {\n        if (indices.rank !== 1) {\n            throw new Error('Only 1D one-hot tensors are supported in the ' +\n                'deeplearn backend, at present.');\n        }\n        indices = indices.toInt();\n        return tfc.oneHot(indices, numClasses).toFloat();\n    });\n}\n/* Elementary math functions. */\n/**\n * Retrieves the elements of indices `indices` in the tensor `reference`.\n * @param reference A tensor.\n * @param indices An integer tensor of indices or an `Array` of integers.\n * @param axis Axis along which to perform the gather operation.\n * @returns The result of the gathering as a tensor.\n */\nexport function gather(reference, indices, axis) {\n    return tidy(() => {\n        if (Array.isArray(indices)) {\n            indices = tensor1d(indices, 'int32');\n        }\n        else {\n            indices = indices.toInt();\n        }\n        return tfc.gather(reference, indices, axis);\n    });\n}\n/**\n * Element-wise square.\n * @param x Input tensor.\n * @return element-wise x^2\n */\nexport function square(x) {\n    return tfc.mul(x, x);\n}\n/**\n * Element-wise exponentiation.\n *\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\n *\n * @param x The base tensor.\n * @param a The exponent, tensor or number. If a number, it is rounded to the\n *   nearest integer and converted to a tensor.\n * @returns A tensor of the same shape as `x`.\n */\nexport function pow(x, a) {\n    return tidy(() => {\n        if (typeof (a) === 'number') {\n            a = scalar(Math.round(a), 'int32');\n        }\n        if (a.dtype !== 'int32') {\n            throw new NotImplementedError(`Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n        }\n        return tfc.pow(x, a);\n    });\n}\n/**\n * Reshapes bias tensor according to rank of x.\n */\nfunction reshapeBias(xRank, bias, dataFormat) {\n    const biasShape = bias.shape;\n    if (bias.rank !== 1 && bias.rank !== xRank) {\n        throw new ValueError(`Unexpected bias dimensions: ${bias.rank}` +\n            `; expected it to be 1 or ${xRank}`);\n    }\n    if (xRank === 5) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1, 1, 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, 1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank === 4) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1, 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank === 3) {\n        if (dataFormat === 'channelsFirst') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, biasShape[0], 1]);\n            }\n            else {\n                return bias.reshape([1, biasShape[1], biasShape[0]]);\n            }\n        }\n        else if (dataFormat === 'channelsLast') {\n            if (biasShape.length === 1) {\n                return bias.reshape([1, 1, biasShape[0]]);\n            }\n            else {\n                return bias.reshape([1].concat(biasShape));\n            }\n        }\n    }\n    else if (xRank < 3) {\n        return bias;\n    }\n    throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n/* Neural-network operations. */\n/**\n * Add a bias to a tensor.\n *\n * @param x The tensor to add the bias to.\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\n * @return Result of the bias adding.\n * @throws ValueError: If the rank of `bias` is incorrect.\n */\nexport function biasAdd(x, bias, dataFormat) {\n    return tidy(() => {\n        if (dataFormat == null) {\n            dataFormat = imageDataFormat();\n        }\n        checkDataFormat(dataFormat);\n        return x.add(reshapeBias(x.rank, bias, dataFormat));\n    });\n}\n/**\n * Exponential linear unit (ELU).\n * @param x A tensor or variable to compute the activation function for.\n * @param alpha: A scalar, a scaling factor for the negative section.\n * @return Output of the ELU operation.\n */\nexport function elu(x, alpha = 1) {\n    // TODO(cais): Add support for alpha values other than 1.\n    if (alpha !== 1) {\n        throw new NotImplementedError(`Support for alpha values other than 1 (${alpha}) is not implemented ` +\n            `yet.`);\n    }\n    return tfc.elu(x);\n}\n/**\n * Softsign of a tensor.\n *\n * Defined as x / (abs(x) + 1), element-wise.\n *\n * @param x: Input.\n * @returns Output.\n */\nexport function softsign(x) {\n    return tidy(() => tfc.div(x, tfc.abs(x).add(1)));\n}\n/**\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\n *\n * @param x input tensor.\n * @param level fraction of the entries in the tensor that will be set to 0.\n * @param noiseShape shape of randomly generated keep/drop flags, must be\n *   broadcastable to the shape of `x`. Optional.\n * @param seed random seed to ensure determinism. Optional.\n * @returns Result of the dropout operation.\n */\nexport function dropout(x, level, noiseShape, seed) {\n    return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n/**\n * Element-wise, segment-wise linear approximation of sigmoid.\n *\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\n *\n * @param x Input tensor.\n * @returns Output tensor.\n */\nexport function hardSigmoid(x) {\n    return tidy(() => {\n        const y = tfc.add(.5, tfc.mul(.2, x));\n        return tfc.clipByValue(y, 0, 1);\n    });\n}\n/**\n * Invoke `x` in the training phase, and `alt` otherwise.\n *\n * Porting Note: We do not create placeholder tensors for the `training`\n * boolean flag here, because there is no such thing in the TF.js imperative\n * backend.\n *\n * @param x The function to invoke iff `training` is `true`.\n * @param alt The function to invoke iff `training` is `false`.\n * @param training Boolean flag for whether training phase is active.\n * @returns The return value of `x()` if `training` is `true`, or the return\n *   value of `alt()` if `training` is `false`.\n */\nexport function inTrainPhase(x, alt, training = false) {\n    return training ? x() : alt();\n}\n//# sourceMappingURL=tfjs_backend.js.map"]},"metadata":{},"sourceType":"module"}